[
  {
    "objectID": "notebooks/nlp/limpeza_de_texto.html",
    "href": "notebooks/nlp/limpeza_de_texto.html",
    "title": "Limpeza de texto",
    "section": "",
    "text": "A limpeza de texto, também chamado de pré-processamento de texto é uma etapa essencial no processamento de linguagem natural (NLP) que visa preparar e limpar os dados textuais para análise por algoritmos de aprendizado de máquina. Envolve uma série de etapas para transformar o texto bruto em um formato mais adequado para análise, incluindo:\n\nConfigurando Dados\n\nimport pandas as pd\n\ndf = pd.read_csv('datas/tccs.csv')\n\ndf = df.drop('Unnamed: 0', axis=1)\n\ndf.head()\n\n\n\n\n\n\n\n\ntitulo\ncurso\n\n\n\n\n0\nAvaliação microscópica dos fragmentos ósseos o...\nZZZ MEST\n\n\n1\nEfeito da Terapia Fotodinâmica sobre Bactérias...\nZZZ MEST\n\n\n2\nAvaliação longitudinal do impacto da Saúde Buc...\nZZZ MEST\n\n\n3\nSOLANUM PANICULATUM LINN E MIMOSA TENUIFLORA (...\nZZZ MEST\n\n\n4\nCondições de saúde bucal e capacidade funciona...\nZZZ MEST\n\n\n\n\n\n\n\n\n\nTécnicas de limpeza textual\nTransformação de caracteres para tipo ASCII\n    Exemplo: \n    \n        Entrada: \"Olá, mundo!\"\n\n        Saída: \"Ola, mundo!\"\n\nimport unicodedata\n\ndef accented_characters(text):\n\n    normalized_text = unicodedata.normalize('NFKD', text)\n\n    ascii_text = normalized_text.encode('ASCII', 'ignore')\n\n    return ascii_text.decode()\n\n# Testando função!\nprint(f'\\nEntrada: {df[\"titulo\"].iloc[0]}\\n\\nSaída:\\t {accented_characters(df[\"titulo\"].iloc[0])}')\n\n\nEntrada: Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos de osteotomia e de irrigação em aloenxertos irradiados e congelados de coelhos\n\nSaída:   Avaliacao microscopica dos fragmentos osseos obtidos por diferentes metodos de osteotomia e de irrigacao em aloenxertos irradiados e congelados de coelhos\n\n\nRemoção de números e pontuações\n    Exemplo: \n    \n        Entrada: \"Hello, 123 world!\"\n\n        Saída: \"Hello world\"\n\nimport re\n\ndef numbers_and_punctuation(text):\n\n    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n\n    return text\n\n# Testando função!\nprint(f'\\nEntrada: {df[\"titulo\"].iloc[0]}\\n\\nSaída:\\t {numbers_and_punctuation(df[\"titulo\"].iloc[0])}')\n\n\nEntrada: Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos de osteotomia e de irrigação em aloenxertos irradiados e congelados de coelhos\n\nSaída:   Avalia  o microsc pica dos fragmentos  sseos obtidos por diferentes m todos de osteotomia e de irriga  o em aloenxertos irradiados e congelados de coelhos\n\n\nRemoção de espaços excessivos do texto\n    Exemplo: \n    \n        Entrada: \"Hello,    world!\"\n\n        Saída: \"Hello, world!\"\n\ndef excessive_spaces(text):\n\n    text = re.sub(r'\\s+', ' ', text)\n\n    return text\n\n# Testando função!\nprint(f'\\nEntrada: {df[\"titulo\"].iloc[0]}\\n\\nSaída:\\t {excessive_spaces(df[\"titulo\"].iloc[0])}')\n\n\nEntrada: Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos de osteotomia e de irrigação em aloenxertos irradiados e congelados de coelhos\n\nSaída:   Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos de osteotomia e de irrigação em aloenxertos irradiados e congelados de coelhos\n\n\nRemoção de palavras de n caracteres\n    Exemplo (n = 3): \n\n        Entrada: \"Hi, world!\"\n\n        Saída: \"world!\"\n\ndef n_words(text, n):\n\n    text = ' '.join([word for word in text.split() if len(word) &gt; n])\n\n    return text\n\n# Testando função!\nprint(f'\\nEntrada: {df[\"titulo\"].iloc[0]}\\n\\nSaída:\\t {n_words(df[\"titulo\"].iloc[0], 2)}')\n\n\nEntrada: Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos de osteotomia e de irrigação em aloenxertos irradiados e congelados de coelhos\n\nSaída:   Avaliação microscópica dos fragmentos ósseos obtidos por diferentes métodos osteotomia irrigação aloenxertos irradiados congelados coelhos\n\n\n\nFunção geral de limpeza textual\n\ndef all_clear(text, ac = 0, np = 0, es = 0, nw = 0, n = 3):\n\n    if ac == 1:\n        text = accented_characters(text)\n\n    if np == 1:\n        text = numbers_and_punctuation(text)\n\n    if es == 1:\n        text = excessive_spaces(text)\n\n    if nw == 1:\n        text = n_words(text, n)\n\n    return text\n\nprint(f'Entrada:\\n{df['titulo']}\\n')\nprint(f'Saída:\\n{df['titulo'].astype(str).apply(all_clear, ac=1, np=1, es=1, nw=1, n=3)}')\n\nEntrada:\n0        Avaliação microscópica dos fragmentos ósseos o...\n1        Efeito da Terapia Fotodinâmica sobre Bactérias...\n2        Avaliação longitudinal do impacto da Saúde Buc...\n3        SOLANUM PANICULATUM LINN E MIMOSA TENUIFLORA (...\n4        Condições de saúde bucal e capacidade funciona...\n                               ...                        \n75479    INOVAÇÃO DOS MINISTÉRIOS PÚBLICOS ESTADUAIS BR...\n75480    PLANO DE NEGÓCIOS PARA ABERTURA DE FILIAL EM S...\n75481    EMPODERAMENTO DA MULHER LÍDER:DESAFIOS EMERGEN...\n75482    ANÁLISE DOS DESAFIOS NA EXECUÇÃO DO PLANEJAMEN...\n75483    Inovação gerencial e práticas de gestão de pes...\nName: titulo, Length: 75484, dtype: object\n\nSaída:\n0        Avaliacao microscopica fragmentos osseos obtid...\n1        Efeito Terapia Fotodinamica sobre Bacterias Or...\n2        Avaliacao longitudinal impacto Saude Bucal Est...\n3        SOLANUM PANICULATUM LINN MIMOSA TENUIFLORA WIL...\n4        Condicoes saude bucal capacidade funcional idosos\n                               ...                        \n75479    INOVACAO MINISTERIOS PUBLICOS ESTADUAIS BRASIL...\n75480    PLANO NEGOCIOS PARA ABERTURA FILIAL MIGUEL GOS...\n75481    EMPODERAMENTO MULHER LIDER DESAFIOS EMERGENTES...\n75482    ANALISE DESAFIOS EXECUCAO PLANEJAMENTO ESTRATE...\n75483    Inovacao gerencial praticas gestao pessoas con...\nName: titulo, Length: 75484, dtype: object",
    "crumbs": [
      "4. Processamento de Linguagem Natural",
      "4.1 Limpeza de Texto"
    ]
  },
  {
    "objectID": "notebooks/nlp/vetorizacao_de_texto.html",
    "href": "notebooks/nlp/vetorizacao_de_texto.html",
    "title": "Vetorização",
    "section": "",
    "text": "A vetorização para processamento de linguagem natural (NLP) é o processo de converter texto em representações numéricas que podem ser entendidas pelos algoritmos de aprendizado de máquina. A vetorização envolve atribuir a cada palavra ou token do texto um vetor numérico, onde cada dimensão do vetor representa alguma característica da palavra. Esses vetores são então usados como entrada para os modelos de ML, permitindo que eles aprendam padrões nos dados textuais. As técnicas comuns de vetorização incluem a criação de um vocabulário único de palavras, a codificação one-hot encoding e a representação TF-IDF.\n\nConfigurando Dados\n\nimport pandas as pd\n\ndf = pd.read_csv('datas/tccs.csv')\n\ndf = df.drop('Unnamed: 0', axis=1)\n\ndf.head()\n\n\n\n\n\n\n\n\ntitulo\ncurso\n\n\n\n\n0\nAvaliação microscópica dos fragmentos ósseos o...\nZZZ MEST\n\n\n1\nEfeito da Terapia Fotodinâmica sobre Bactérias...\nZZZ MEST\n\n\n2\nAvaliação longitudinal do impacto da Saúde Buc...\nZZZ MEST\n\n\n3\nSOLANUM PANICULATUM LINN E MIMOSA TENUIFLORA (...\nZZZ MEST\n\n\n4\nCondições de saúde bucal e capacidade funciona...\nZZZ MEST\n\n\n\n\n\n\n\n\n\nAplicação de Vetorização\n\nimport nltk\n\n# Baixando stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Define as stopwords específicas do idioma português.\nstopwords_nltk = stopwords.words('portuguese')\n\n# Cria um vetorizador TF-IDF com configurações distintas.\nvectorizer = TfidfVectorizer(stop_words=stopwords_nltk, # Remove as stopwords definidas anteriormente.\n                             max_features=2000, # Limita o número máximo de features a 2000.\n                             ngram_range=(1, 2), # Considera unigramas e bigramas.\n                             min_df=5, # Ignora termos que aparecem em menos de 5 documentos.\n                             max_df=0.8, # Ignora termos que aparecem em mais de 80% dos documentos.\n                             lowercase=True # Converte o texto para minúsculas.\n                             )\n\n# Vetoriza as frases das linhas da coluna titulo\ntfidf_matrix = vectorizer.fit_transform(df['titulo'].astype(str))\n\n# Observando vetorização\ntfidf_matrix.toarray()\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nN-grams\nUma forma de demonstrar a eficácia da vetorização textual é na obtenção dos n-grams mais frequentes do conjunto de dados de texto vetorizado.\nArgs:\n    X_train (np.ndarray): Os dados de texto vetorizados.\n    vectorizer (TfidfVectorizer): O vetorizador usado para transformar os dados de texto.\n    top_n (int, opcional): O número de top n-grams a serem retornados. O padrão é 10.\n\nReturns:\n    np.ndarray: Um array dos n-grams mais frequentes.\n\nimport numpy as np\n\ndef top_ngrams(X_train: np.ndarray, vectorizer: TfidfVectorizer, top_n: int = 10) -&gt; np.ndarray:\n\n    # Soma todas as colunas para obter a frequência total de cada n-grama\n    frequencias_total_ngram = np.sum(X_train, axis=0)\n\n    # Ordena os n-grams por sua frequência total\n    indices_ngrams_ordenados = np.argsort(frequencias_total_ngram)[::-1]\n\n    # Obtém os índices dos top n-grams mais frequentes\n    indices_top_ngrams = indices_ngrams_ordenados[:top_n]\n\n    # Obtém os nomes dos n-grams correspondentes aos índices mais frequentes\n    nomes_ngrams = np.array(vectorizer.get_feature_names_out())\n\n    return nomes_ngrams[indices_top_ngrams]\n\n# Usa a função para obter os top 10 n-grams dos dados de treinamento\ntop_ngrams = top_ngrams(tfidf_matrix.toarray(), vectorizer, top_n=10)\n\n# Mostrando resultado\nprint(f\"Aqui estão as 10 palavras mais usadas nos TCC's: {', '.join(top_ngrams)}.\")\n\nAqui estão as 10 palavras mais usadas nos TCC's: análise, rn, estudo, saúde, sobre, natal, avaliação, educação, ensino, rio.",
    "crumbs": [
      "4. Processamento de Linguagem Natural",
      "4.2 Vetorização de Texto"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html",
    "href": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html",
    "title": "Detecção e Exclusão de Linhas Duplicadas",
    "section": "",
    "text": "Detectar dados duplicados em um DataFrame (ou outro conjunto de dados) é importante para garantir a qualidade e integridade dos dados. Dados duplicados podem surgir de diversas formas, como erros no processo de coleta, integração de múltiplas fontes de dados ou falhas de importação.",
    "crumbs": [
      "1. Pré-processamento",
      "1.2 Detecção e Exclusão de Dados Duplicados"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#configurando-dados",
    "href": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#configurando-dados",
    "title": "Detecção e Exclusão de Linhas Duplicadas",
    "section": "Configurando dados",
    "text": "Configurando dados\n\n# Importando biblioteca pandas\nimport pandas as pd\n\n# Importando a função minmax_scale da biblioteca scikit-learn\nfrom sklearn.preprocessing import minmax_scale\n\n# Lendo arquivo CSV e transformando em um DataFrame\ndf = pd.read_csv('datas/diabetes.csv')\n\n# Varrendo colunas do DataFrame\nfor column in df:\n\n    # Substituindo valores NAN por zero para fins de demonstração\n    df[column] = df[column].fillna(0)\n\n    # Dimensiona valores para mesma escala (entre 0 e 1) para fins de visualização de gráfico\n    df[column] = minmax_scale(df[column])\n\n# Duplicando linhas para fins de demonstração\nlinhas_duplicadas = df.iloc[:10]  # Seleciona as dez primeiras linhas\ndf = pd.concat([df, linhas_duplicadas], ignore_index=True)\n\n# Exibindo 10 primeiras linhas do DataFrame\ndf.head(10)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n0\n0.352941\n0.743719\n0.590164\n0.353535\n0.000000\n0.500745\n0.269183\n0.617284\n\n\n1\n0.058824\n0.427136\n0.540984\n0.292929\n0.000000\n0.396423\n0.150672\n0.382716\n\n\n2\n0.470588\n0.919598\n0.524590\n0.000000\n0.000000\n0.347243\n0.288505\n0.395062\n\n\n3\n0.058824\n0.447236\n0.540984\n0.232323\n0.111111\n0.418778\n0.071665\n0.259259\n\n\n4\n0.000000\n0.688442\n0.327869\n0.353535\n0.198582\n0.642325\n0.982395\n0.000000\n\n\n5\n0.294118\n0.582915\n0.606557\n0.000000\n0.000000\n0.381520\n0.086264\n0.370370\n\n\n6\n0.176471\n0.391960\n0.409836\n0.323232\n0.104019\n0.461997\n0.106445\n0.320988\n\n\n7\n0.588235\n0.577889\n0.000000\n0.000000\n0.000000\n0.526080\n0.057495\n0.358025\n\n\n8\n0.117647\n0.989950\n0.573770\n0.454545\n0.641844\n0.454545\n0.067800\n0.654321\n\n\n9\n0.470588\n0.628141\n0.786885\n0.000000\n0.000000\n0.000000\n0.099575\n0.666667",
    "crumbs": [
      "1. Pré-processamento",
      "1.2 Detecção e Exclusão de Dados Duplicados"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#detecção-de-dados-duplicados",
    "href": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#detecção-de-dados-duplicados",
    "title": "Detecção e Exclusão de Linhas Duplicadas",
    "section": "Detecção de dados Duplicados",
    "text": "Detecção de dados Duplicados\n\n# Buscando linhas duplicadas\nduplicacoes = df.duplicated()\n\n# Exibindo linhas duplicadas\ndf[duplicacoes].head(10)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n768\n0.352941\n0.743719\n0.590164\n0.353535\n0.000000\n0.500745\n0.269183\n0.617284\n\n\n769\n0.058824\n0.427136\n0.540984\n0.292929\n0.000000\n0.396423\n0.150672\n0.382716\n\n\n770\n0.470588\n0.919598\n0.524590\n0.000000\n0.000000\n0.347243\n0.288505\n0.395062\n\n\n771\n0.058824\n0.447236\n0.540984\n0.232323\n0.111111\n0.418778\n0.071665\n0.259259\n\n\n772\n0.000000\n0.688442\n0.327869\n0.353535\n0.198582\n0.642325\n0.982395\n0.000000\n\n\n773\n0.294118\n0.582915\n0.606557\n0.000000\n0.000000\n0.381520\n0.086264\n0.370370\n\n\n774\n0.176471\n0.391960\n0.409836\n0.323232\n0.104019\n0.461997\n0.106445\n0.320988\n\n\n775\n0.588235\n0.577889\n0.000000\n0.000000\n0.000000\n0.526080\n0.057495\n0.358025\n\n\n776\n0.117647\n0.989950\n0.573770\n0.454545\n0.641844\n0.454545\n0.067800\n0.654321\n\n\n777\n0.470588\n0.628141\n0.786885\n0.000000\n0.000000\n0.000000\n0.099575\n0.666667",
    "crumbs": [
      "1. Pré-processamento",
      "1.2 Detecção e Exclusão de Dados Duplicados"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#exclusão-de-dados-duplicados",
    "href": "notebooks/preprocessing/deteccao_e_exclusao_de_dados_duplicados.html#exclusão-de-dados-duplicados",
    "title": "Detecção e Exclusão de Linhas Duplicadas",
    "section": "Exclusão de dados Duplicados",
    "text": "Exclusão de dados Duplicados\n\n# Removendo duplicatas e mantendo apenas a primeira ocorrência\ndf = df.drop_duplicates()\n\n# Buscando novamente linhas duplicadas\nduplicacoes = df.duplicated()\n\n# Exibindo linhas duplicadas\ndf[duplicacoes].head(10)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage",
    "crumbs": [
      "1. Pré-processamento",
      "1.2 Detecção e Exclusão de Dados Duplicados"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html",
    "title": "Substituição por Medidas Centrais",
    "section": "",
    "text": "Afim de reduzir a influência dos valores extremos (outliers) em um conjunto de dados, é possível transformá-los em uma medida central como a média ou a mediana. Essa técnica é uma forma simples de lidar com os valores extremos, mas é importante considerar o contexto dos dados e os possíveis efeitos colaterais dessa abordagem, como a distorção da distribuição dos dados.",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#configuração-dos-dados",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#configuração-dos-dados",
    "title": "Substituição por Medidas Centrais",
    "section": "Configuração dos dados",
    "text": "Configuração dos dados\n\n# Importando biblioteca pandas\nimport pandas as pd\n\n# Importando a função minmax_scale da biblioteca scikit-learn\nfrom sklearn.preprocessing import minmax_scale\n\n# Lendo arquivo CSV e transformando em um DataFrame\ndf = pd.read_csv('datas/diabetes.csv')\n\n# Varrendo colunas do DataFrame\nfor column in df:\n\n    # Substituindo valores NAN por zero para fins de demonstração\n    df[column] = df[column].fillna(0)\n\n    # Dimensiona valores para mesma escala (entre 0 e 1) para fins de visualização de gráfico\n    df[column] = minmax_scale(df[column])\n\n# Exibindo 5 primeiras linhas do DataFrame\ndf.head(5)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n0\n0.352941\n0.743719\n0.590164\n0.353535\n0.000000\n0.500745\n0.269183\n0.617284\n\n\n1\n0.058824\n0.427136\n0.540984\n0.292929\n0.000000\n0.396423\n0.150672\n0.382716\n\n\n2\n0.470588\n0.919598\n0.524590\n0.000000\n0.000000\n0.347243\n0.288505\n0.395062\n\n\n3\n0.058824\n0.447236\n0.540984\n0.232323\n0.111111\n0.418778\n0.071665\n0.259259\n\n\n4\n0.000000\n0.688442\n0.327869\n0.353535\n0.198582\n0.642325\n0.982395\n0.000000\n\n\n\n\n\n\n\n\n# Importando biblioteca matplotlib\nimport matplotlib.pyplot as plt\n\n# Exibindo disposição dos dados em um gráfico boxplot\nboxplot = plt.boxplot(df)\nplt.show()",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#função-de-substituição-por-medidas-centrais",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#função-de-substituição-por-medidas-centrais",
    "title": "Substituição por Medidas Centrais",
    "section": "Função de Substituição por Medidas Centrais",
    "text": "Função de Substituição por Medidas Centrais\n\n# Função para substituir dados outliers por mediana ou média\ndef substituir_mc(dataframe, coluna, medida):\n\n    # Calculando o primeiro quartil (Q1) da coluna\n    Q1 = dataframe[coluna].quantile(0.25)\n    \n    # Calculando o terceiro quartil (Q3) da coluna\n    Q3 = dataframe[coluna].quantile(0.75)\n\n    # Calculando a amplitude interquartil (IQR)\n    IQR = Q3 - Q1\n\n    # Identificando os outliers\n    # (modificando 1.5 para 0.5 da fórmula, para diminuir a quantidade de outliers pós-processamento)\n    outliers = dataframe[(dataframe[coluna] &lt; (Q1 - 0.5 * IQR)) | (dataframe[coluna] &gt; (Q3 + 0.5 * IQR))]\n\n    # Verificando se a medida é 'media'\n    if medida == 'media':\n        \n        # Substituindo os valores outliers pela média da coluna\n        dataframe.loc[outliers.index, coluna] = dataframe[coluna].mean()\n\n    # Verificando se a medida é 'mediana'\n    elif medida == 'mediana':\n        \n        # Substituindo os valores outliers pela mediana da coluna\n        dataframe.loc[outliers.index, coluna] = dataframe[coluna].median()\n\n    # Retornando a coluna modificada do DataFrame\n    return dataframe[coluna]",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#substituindo-outliers-por-média",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#substituindo-outliers-por-média",
    "title": "Substituição por Medidas Centrais",
    "section": "Substituindo Outliers por Média",
    "text": "Substituindo Outliers por Média\n\n# Configurando novo DataFrame para media\ndf_media = df\n\n# Varrendo colunas do DataFrame\nfor column in df_media:\n\n    # Substituindo outliers por média\n    df_media[column] = substituir_mc(df_media, column, 'media')\n\n# Exibindo disposição dos dados em um gráfico boxplot\nboxplot = plt.boxplot(df_media)\nplt.show()",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#substituindo-outliers-por-mediana",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#substituindo-outliers-por-mediana",
    "title": "Substituição por Medidas Centrais",
    "section": "Substituindo Outliers por Mediana",
    "text": "Substituindo Outliers por Mediana\n\n# Configurando novo DataFrame para mediana\ndf_mediana = df\n\n# Varrendo colunas do DataFrame\nfor column in df_mediana:\n\n    # Substituindo outliers por média\n    df_mediana[column] = substituir_mc(df_mediana, column, 'media')\n\n# Exibindo disposição dos dados em um gráfico boxplot\nboxplot = plt.boxplot(df_mediana)\nplt.show()",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#considerações-finais",
    "href": "notebooks/preprocessing/substituicao_por_medidas_centrais.html#considerações-finais",
    "title": "Substituição por Medidas Centrais",
    "section": "Considerações Finais",
    "text": "Considerações Finais\nÉ importante notar, que ao utilizar a técnica, novos outliers são gerados independente da medida central utilizada, e isso se dá porque uma nova disposição de dados é gerada. Contudo, os outliers de fato são suavizados.",
    "crumbs": [
      "1. Pré-processamento",
      "1.4 Substituição de Outliers por Medidas Centrais"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "",
    "text": "São vários os métodos de imputação usados para substituir valores ausentes em conjuntos de dados por valores estimados. Eles ajudam a melhorar a qualidade dos dados, permitindo que eles sejam usados em análises estatísticas ou no treinamento de modelos de aprendizado de máquina, que muitas vezes não lidam bem com valores ausentes.\nPor que usar imputação?",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#configurando-dados",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#configurando-dados",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "Configurando dados",
    "text": "Configurando dados\n\n# Importando biblioteca pandas\nimport pandas as pd\n\n# Importando biblioteca numpy\nimport numpy as np\n\n# Importando a função minmax_scale da biblioteca scikit-learn\nfrom sklearn.preprocessing import minmax_scale\n\n# Função para medir acurácia\ndef err(df, df_):\n    # Calculando o MAE\n    mae = (df - df_).abs().mean().mean()\n    # Calculando MSE\n    mse = ((df - df_) ** 2).mean().mean()\n    # Calculando Correlação\n    correlation = df.corrwith(df_).mean()\n    # Exibindo resultados\n    print(f'\\nAcurácia para os dados usados:\\n\\nMAE: {mae:.4f}\\nMSE: {mse:.4f}\\nCor: {correlation:.4f}')\n\n# Lendo arquivo CSV e transformando em um DataFrame\ndf = pd.read_csv('datas/diabetes.csv')\n\n# Varrendo colunas do DataFrame\nfor column in df:\n\n    # Dimensiona valores para mesma escala (entre 0 e 1) para fins de visualização de gráfico\n    df[column] = minmax_scale(df[column])\n\n# Excluindo valores nulos existentes\ndf = df.dropna()\n\n# Declarando novo dataframe que terão dados nulos forjados\ndf_nan = df.copy()\n\n# Selecionando índices aleatórios para substituir por NaN\nmissing_indices = np.random.choice(\n    df_nan.size,  # Número total de elementos no DataFrame\n    int(df_nan.size * 0.1),  # Número de elementos que serão substituídos por NaN (10%)\n    replace=False\n)\n\n# Substituindo os valores nos índices escolhidos por NaN\ndf_nan_1d = df_nan.values.flatten()  # Transforma o DataFrame em um array 1D\ndf_nan_1d[missing_indices] = np.nan  # Insere NaN nos índices selecionados\ndf_nan[:] = df_nan_1d.reshape(df_nan.shape)  # Reshape para o formato original\n\n# Exibindo contagem de valores nulos por coluna\nprint(\"\\nNulos por coluna:\\n\")\nprint(df_nan.isnull().sum())\n\n\nNulos por coluna:\n\npreg    80\nplas    76\npres    71\nskin    70\ninsu    76\nmass    94\npedi    70\nage     71\ndtype: int64",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-por-constante",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-por-constante",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "Substituição por constante:",
    "text": "Substituição por constante:\n\nPreenche valores ausentes com um valor fixo, como 0 ou “Desconhecido” (em variáveis categóricas).\nÚtil quando um valor específico tem um significado claro.\n\n\n# Declarando novo dataframe que terão dados nulos transformados em constante\ndf_cons = df_nan.copy()\n\n# Substituindo os valores NaN por 0\ndf_cons = df_cons.fillna(0)\n\nerr(df, df_cons)\n\n\nAcurácia para os dados usados:\n\nMAE: 0.0322\nMSE: 0.0157\nCor: 0.7975",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-pela-médiamedianamoda",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-pela-médiamedianamoda",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "Substituição pela média/mediana/moda:",
    "text": "Substituição pela média/mediana/moda:\n\nPara variáveis numéricas, usa:\n\nMédia: mean\nMediana: median\n\nPara variáveis categóricas, usa:\n\nModa (valor mais frequente).\n\nSimples de implementar, mas pode distorcer a variabilidade dos dados.\n\n\nfrom sklearn.impute import SimpleImputer\n\n# Declarando novo dataframe que terão dados nulos transformados em media\ndf_mean = df_nan.copy()\n\nfor column in df_mean.columns:\n\n    # Definindo estratégia de imputação\n    imputer = SimpleImputer(strategy='mean')  # Estratégia: 'mean', 'median', 'most_frequent'\n\n    # Aplicando imputação nos dados faltantes\n    df_mean[column] = imputer.fit_transform(df_mean[[column]])\n\nerr(df, df_mean)\n\n\nAcurácia para os dados usados:\n\nMAE: 0.0116\nMSE: 0.0022\nCor: 0.9561",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-usando-o-k-nearest-neighbors-knn",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#substituição-usando-o-k-nearest-neighbors-knn",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "Substituição usando o K-Nearest Neighbors (KNN):",
    "text": "Substituição usando o K-Nearest Neighbors (KNN):\n\nSubstitui valores ausentes com base na média/mediana/moda de k-vizinhos mais próximos.\nCaptura melhor as relações entre variáveis.\n\n\nfrom sklearn.impute import KNNImputer\n\n# Declarando novo dataframe que terá os dados nulos transformados\ndf_knn = df_nan.copy()\n\n# Definindo imputação para k vizinhos (5)\nimputer = KNNImputer(n_neighbors=5)\n\n# Aplicando imputação no DataFrame inteiro, pois o imputador espera uma matriz bidimensional\ndf_knn.iloc[:, :] = imputer.fit_transform(df_knn)\n\nerr(df, df_knn)\n\n\nAcurácia para os dados usados:\n\nMAE: 0.0099\nMSE: 0.0019\nCor: 0.9624",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#outros-métodos-de-substituição",
    "href": "notebooks/preprocessing/tratamento_de_valores_ausentes.html#outros-métodos-de-substituição",
    "title": "Tratamento de Valores Ausentes com Imputação",
    "section": "Outros métodos de substituição",
    "text": "Outros métodos de substituição\n\nPreenchimento pelo último ou próximo valor conhecido (forward/backward fill):\n\nPreenche com o último valor não ausente encontrado (ou o próximo, no caso do backward fill).\nComum em séries temporais.\n\nRegressão:\n\nUsa regressão para prever valores ausentes com base em outras variáveis.\nExemplo: Prever valores ausentes de altura com base em idade e peso.\n\nInterpolação:\n\nEstima valores ausentes com base em técnicas matemáticas, como Linear, Polinomial, Splines.\n\nModelos de Aprendizado de Máquina:\n\nMétodos como árvores de decisão ou redes neurais podem imputar valores com maior precisão ao capturar relações complexas nos dados.\n\nMúltipla Imputação:\n\nGera várias estimativas para os valores ausentes e combina os resultados para refletir a incerteza da imputação.\n\n\nCada método possui suas próprias especificidades e custo computacional. A efetividade de uso de cada um se dará a partir do tipo de dado que está sendo tratado, não havendo um que se destaque sobre outro para toda e qualquer situação.",
    "crumbs": [
      "1. Pré-processamento",
      "1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/analises_de_agrupamentos.html",
    "href": "notebooks/unsupervised/analises_de_agrupamentos.html",
    "title": "Técnicas de análise de agrupamento de dados",
    "section": "",
    "text": "A análise de agrupamentos é uma técnica exploratória que visa identificar estruturas intrínsecas nos dados não rotulados. Essas técnicas são usadas para avaliar a qualidade dos agrupamentos e ajudar na interpretação dos resultados, auxiliando na tomada de decisões sobre o número ideal de clusters e na validação dos agrupamentos obtidos.\n\nConfiguração de dados\n\n# Importando biblioteca pandas\nimport pandas as pd\n\n# Importando classe MinMaxScaler da biblioteca scikit-learn\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Importando dados\ndf = pd.read_csv('dados/banana_quality.csv')\n\n# Excluindo coluna qualitativa\ndf = df.drop('Quality', axis=1)\n\nfor coluna in df.columns:\n    \n    # Calculando mediana\n    mediana_coluna = df[coluna].median()\n    \n    # Substitui os valores nulos pela mediana da coluna usando loc\n    df.loc[df[coluna].isnull(), coluna] = mediana_coluna\n\n    # Normalizando dados\n    df[coluna] = MinMaxScaler().fit_transform(df[[coluna]])\n\n# Visualizando dados\ndf.head(3)\n\n\n\n\n\n\n\n\nSize\nWeight\nSweetness\nSoftness\nHarvestTime\nRipeness\nAcidity\n\n\n\n\n0\n0.380309\n0.626747\n0.680712\n0.360975\n0.567312\n0.671933\n0.543416\n\n\n1\n0.349951\n0.628093\n0.485275\n0.293682\n0.481689\n0.646850\n0.545720\n\n\n2\n0.478460\n0.699448\n0.572694\n0.283811\n0.499358\n0.716580\n0.617337\n\n\n\n\n\n\n\n\n\nDavies-Bouldin Index\nO Índice de Davies-Bouldin mede a similaridade média entre cada cluster e seu cluster mais próximo, avaliando a compactação e separação dos clusters. Quanto menor o índice, melhor a separação entre os clusters.\n\n# Importando a classe KMeans do módulo cluster da biblioteca scikit-learn\nfrom sklearn.cluster import KMeans\n\n# Importando classe metrics da biblioteca scikit-learn\nfrom sklearn import metrics\n\n# Importando matplotlib\nimport matplotlib.pyplot as plt\n\n# Calculando Davies-Bouldin para diferentes valores de k\nvalores_de_db = []\nfor k in range(2, 21):    \n    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    indice_db = metrics.davies_bouldin_score(df, km.fit_predict(df))\n    valores_de_db.append(metrics.davies_bouldin_score(df, km.fit_predict(df)))\n    if k == 2:\n        x_menor = 2\n        y_menor = indice_db\n    if indice_db &lt; y_menor:\n        x_menor = k\n        y_menor = indice_db\n\n# Plotando gráfico de Davies-Bouldin\nplt.figure(figsize=(8, 4))\nplt.plot(range(2, 21), valores_de_db, marker='o')\nplt.title('Davies-Bouldin')\nplt.xlabel('Grupos')\nplt.ylabel('Índice')\nplt.grid(True)\nplt.plot(x_menor, y_menor, 'ro')\nplt.show()\n\n\n\n\n\n\n\n\nA conclusão do uso do método de Davies-Bouldin, é que o número ideal de agrupamentos é 5.\n\n\nSilhouette Score\nCalcula o quão similar um objeto é ao seu próprio cluster (coesão) em comparação com outros clusters (separação). O score varia de -1 a 1, onde valores próximos de 1 indicam que o objeto está bem ajustado ao seu cluster.\n\n# Importando a classe silhouette_score do módulo metrics da biblioteca scikit-learn\nfrom sklearn.metrics import silhouette_score\n\n# Calculando silhouette para diferentes valores de k\nvalores_de_si = []\nfor k in range(2, 21):    \n    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    indice_si = silhouette_score(df, km.fit_predict(df), metric='euclidean')\n    valores_de_si.append(indice_si)\n    if k == 2:\n        x_menor = 2\n        y_menor = indice_si\n    if ((indice_si - 1)**2)**(1/2) &lt; ((y_menor - 1)**2)**(1/2):\n        x_menor = k\n        y_menor = indice_si\n\n# Plotando gráfico de Davies-Bouldin\nplt.figure(figsize=(8, 4))\nplt.plot(range(2, 21), valores_de_si, marker='o')\nplt.title('Silhouette')\nplt.xlabel('Grupos')\nplt.ylabel('Índice')\nplt.grid(True)\nplt.plot(x_menor, y_menor, 'ro')\nplt.show()\n\n\n\n\n\n\n\n\nA conclusão do uso do método Silhouette, é que o número ideal de agrupamentos é 4.\n\n\nMétodo Elbow (do Cotovelo)\nAjuda a determinar o número ideal de clusters em um conjunto de dados, identificando o ponto de inflexão onde a adição de mais clusters não melhora significativamente a coesão intra-cluster.\n\n# Calculando a inércia para diferentes valores de k\ninercias = []\nfor k in range(2, 21):\n    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    km.fit(df)\n    inercias.append(km.inertia_)\n\n# Plotando o gráfico de Elbow\nplt.figure(figsize=(8, 4))\nplt.plot(range(2, 21), inercias, marker='o')\nplt.title('Método de Elbow')\nplt.xlabel('Número de Clusters')\nplt.ylabel('Inércia')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nA conclusão do uso do método Elbow, é que o número ideal de agrupamentos está entre 5 e 7.\n\n\nConsiderações Finais\nA escolha do uso do método irá depender de uma série de fatores, que podem ser avaliados através de testes estatísticos ou de acurácia, visto que muitas vezes eles podem apresentar resultados contraditórios.",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.2 Análises de Agrupamentos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html",
    "title": "Agrupamento de dados",
    "section": "",
    "text": "O agrupamento de dados, conhecido como clustering em inglês, é uma técnica de análise de dados utilizada para agrupar um conjunto de objetos de forma que objetos no mesmo grupo (ou cluster) sejam mais semelhantes entre si do que com objetos de outros grupos. O objetivo principal é segmentar os dados em grupos significativos e interpretáveis, sem a necessidade de rótulos prévios para os grupos. O clustering é amplamente utilizado em diversas áreas, como mineração de dados, aprendizado de máquina, reconhecimento de padrões, entre outros, para identificar padrões e estruturas nos dados não rotulados.",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-k-means",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-k-means",
    "title": "Agrupamento de dados",
    "section": "Agrupamento K-Means",
    "text": "Agrupamento K-Means\nUsado para agrupar dados não rotulados em clusters com base em suas características. O K-Means visa encontrar grupos nos dados, com um número de grupos (ou clusters) especificado. Sua utilidade está em explorar padrões em conjuntos de dados.\nO funcionamento básico do algoritmo K-Means é o seguinte:\n\nInicializa os centróides dos clusters de forma aleatória.\nAtribui cada ponto de dados ao centróide mais próximo, formando clusters.\nCalcula os novos centróides de cada cluster, com base nos pontos atribuídos a eles.\nRepete os passos 2 e 3 até que os centróides não mudem significativamente ou até que um número máximo de iterações seja atingido.\n\n\n# Importando a classe KMeans do módulo cluster da biblioteca scikit-learn\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(\n        n_clusters=2, # número de clusters desejado (neste caso, 2)\n        init='k-means++', # inicialização dos centróides ('k-means++' para inicialização inteligente)\n        max_iter=300, # máximo de iterações para cada execução do algoritmo (padrão: 300)\n        n_init=10, # vezes que o algoritmo será executado com diferentes centróides iniciais (padrão: 10)\n        random_state=0) # semente para a geração de números aleatórios (reproducibilidade dos resultados)\n\n# Aplicando o algoritmo KMeans aos dados do DataFrame\nkm.fit(df)\n\n# Importando a biblioteca matplotlib para visualização de dados\nimport matplotlib.pyplot as plt\n\n# Importando a classe Axes3D do módulo mpl_toolkits.mplot3d para plotagem em 3D\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Plotando gráficos de agrupamentos\nfig = plt.figure(figsize=(8, 3))\nax1 = fig.add_subplot(121)  # Subplot 2D\nax2 = fig.add_subplot(122, projection='3d')  # Subplot 3D\nax1.scatter(df.iloc[:, 0], df.iloc[:, 1], c=km.labels_, cmap='viridis')\nax1.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='red', s=300)\nax1.set_title('Agrupamento KM (2D)')\nax2.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=km.labels_, cmap='viridis')\nax2.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], km.cluster_centers_[:, 2], c='red', s=300)\nax2.set_title('Agrupamento KM (3D)')\nplt.show()",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-expectation-maximization",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-expectation-maximization",
    "title": "Agrupamento de dados",
    "section": "Agrupamento Expectation-Maximization",
    "text": "Agrupamento Expectation-Maximization\nO Expectation-Maximization (EM) é usado para modelar dados não rotulados e é especialmente útil quando os dados possuem distribuições ocultas. O objetivo do EM é encontrar os parâmetros de um modelo de mistura de distribuições probabilísticas que melhor descrevem os dados.\nO funcionamento básico do algoritmo Expectation-Maximization é o seguinte:\n\nInicializa os parâmetros do modelo de mistura de forma aleatória ou usando alguma heurística.\nPasso de Expectation (E-step): Calcula a probabilidade de cada ponto de dados pertencer a cada componente da mistura (responsabilidades).\nPasso de Maximization (M-step): Atualiza os parâmetros do modelo para maximizar a probabilidade conjunta dos dados e das 4. responsabilidades calculadas no passo anterior.\nRepete os passos 2 e 3 até que os parâmetros converjam ou até que um critério de parada seja atingido.\n\n\n# Importando a classe GaussianMixture do módulo misture da biblioteca scikit-learn\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(\n                n_components=2, # número de clusters desejado (neste caso, 2)\n                covariance_type='spherical') # Tipo de covariância (nesse caso, esférica)\n\n# Aplicando o algoritmo GMM aos dados do DataFrame\ngmm.fit(df)\n\n# Plotando gráficos de agrupamentos\nfig = plt.figure(figsize=(8, 3))\nax1 = fig.add_subplot(121)  # Subplot 2D\nax2 = fig.add_subplot(122, projection='3d')  # Subplot 3D\nax1.scatter(df.iloc[:, 0], df.iloc[:, 1], c=gmm.predict(df), cmap='viridis')\nax1.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', s=300)\nax1.set_title('Agrupamento EM (2D)')\nax2.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=gmm.predict(df), cmap='viridis')\nax2.scatter(gmm.means_[:, 0], gmm.means_[:, 1], gmm.means_[:, 2], c='red', s=300)\nax2.set_title('Agrupamento EM (3D)')\nplt.show()\n\n\n\n\n\n\n\n\nAlém do spherical, os outros tipos de covariância são:\n\n‘tied’: Neste caso, todas as componentes do modelo compartilham a mesma matriz de covariância, que é uma matriz completa e simétrica. Isso significa que todas as variáveis têm a mesma covariância entre si.\n‘diag’: Neste caso, cada componente tem sua própria matriz diagonal de covariância, o que significa que as variáveis são consideradas independentes umas das outras, exceto pela sua variância. Ou seja, a covariância entre diferentes dimensões é zero.\n‘full’: Neste caso, cada componente do modelo tem sua própria matriz completa e simétrica de covariância. Isso significa que não há restrições nas covariâncias entre as diferentes variáveis, permitindo correlações arbitrárias entre elas.\n\nEm alguns casos, a escolha deles podem afetar significativamente o agrupamento dos dados.",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-hierarquico-aglomerativo",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-hierarquico-aglomerativo",
    "title": "Agrupamento de dados",
    "section": "Agrupamento Hierarquico Aglomerativo",
    "text": "Agrupamento Hierarquico Aglomerativo\nO Agrupamento Hierárquico Aglomerativo é um algoritmo de agrupamento hierárquico que constrói uma árvore de clusters. Ele começa com cada ponto de dados como um cluster separado e, em seguida, mescla os clusters mais próximos com base em uma métrica de distância até restar apenas um cluster.\nA ideia básica do algoritmo é a seguinte:\n\nComece com cada ponto de dados como um cluster separado.\nMescle os dois clusters mais próximos em um único cluster.\nAtualize a matriz de distância para refletir as novas distâncias entre clusters.\nRepita os passos 2 e 3 até que reste apenas um cluster.\n\nO Agrupamento Hierárquico Aglomerativo é útil para explorar relacionamentos hierárquicos nos dados e pode ser visualizado como um dendrograma, que mostra como os clusters são mesclados em cada etapa.\n\n# Importando a classe AgglomerativeClustering do módulo cluster da biblioteca scikit-learn\nfrom sklearn.cluster import AgglomerativeClustering\n\nha = AgglomerativeClustering(\n                n_clusters=2, # número de clusters desejado (neste caso, 2)\n                linkage='complete') # método de ligação para calcular a distância entre os clusters\n\n# Aplicando o algoritmo HA aos dados do DataFrame\nha.fit(df)\n\n# Plotando gráficos de agrupamentos\nfig = plt.figure(figsize=(8, 3))\nax1 = fig.add_subplot(121)  # Subplot 2D\nax2 = fig.add_subplot(122, projection='3d')  # Subplot 3D\nax1.scatter(df.iloc[:, 0], df.iloc[:, 1], c=ha.labels_, cmap='viridis')\nax1.set_title('Agrupamento HA (2D)')\nax2.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=ha.labels_, cmap='viridis')\nax2.set_title('Agrupamento HA (3D)')\nplt.show()\n\n\n\n\n\n\n\n\nO parâmetro linkage no algoritmo de Agrupamento Hierárquico Aglomerativo define a estratégia utilizada para calcular a distância entre os clusters durante a mesclagem. Existem diferentes métodos de ligação (linkage) disponíveis:\n\n‘complete’: Calcula a distância máxima entre os pontos de dois clusters. A distância entre dois clusters é a distância máxima entre seus pontos.\n‘single’: Calcula a distância mínima entre os pontos de dois clusters. A distância entre dois clusters é a distância mínima entre seus pontos.\n‘average’: Calcula a média das distâncias entre os pontos de dois clusters. A distância entre dois clusters é a média das distâncias entre seus pontos.\n‘ward’: Minimiza a variância dos clusters sendo mesclados. Essa abordagem tende a formar clusters de tamanhos relativamente iguais.",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-dbscan",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html#agrupamento-dbscan",
    "title": "Agrupamento de dados",
    "section": "Agrupamento DBSCAN",
    "text": "Agrupamento DBSCAN\nO DBSCAN (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de agrupamento baseado em densidade que é capaz de identificar clusters de formatos e tamanhos arbitrários em um conjunto de dados. Ele é especialmente útil quando os clusters têm densidades diferentes, pois ele define clusters como regiões de alta densidade separadas por regiões de baixa densidade.\nO funcionamento básico do algoritmo DBSCAN é o seguinte:\n\nCada ponto de dados é classificado como um ponto central, um ponto de borda ou um ponto de ruído.\nPara cada ponto central, o algoritmo forma um cluster ao seu redor, incluindo todos os pontos alcançáveis por densidade a partir desse ponto.\nPontos de borda são aqueles que estão dentro da vizinhança de densidade de um ponto central, mas não são pontos centrais eles próprios. Eles são atribuídos ao cluster do ponto central mais próximo em sua vizinhança.\nPontos de ruído são aqueles que não são pontos centrais nem pontos de borda e são descartados ou considerados outliers.\n\nO DBSCAN é útil para encontrar clusters em conjuntos de dados onde os clusters têm diferentes densidades ou quando há presença de ruído nos dados. Ele não requer a especificação do número de clusters a priori, o que o torna uma escolha popular para muitos problemas de agrupamento. Além disso, o DBSCAN é capaz de lidar com clusters de forma arbitrária, não se limitando a formas geométricas específicas.\n\nfrom sklearn.cluster import DBSCAN\n\ndbs = DBSCAN(\n        eps=0.35, # raio máximo entre dois pontos para serem considerados vizinhos\n        min_samples=5) # número mínimo de pontos em uma vizinhança para formar um cluster\n\n# Aplicando o algoritmo DBSCAN aos dados do DataFrame\ndbs.fit(df)\n\n# Plotando gráficos de agrupamentos\nfig = plt.figure(figsize=(8, 3))\nax1 = fig.add_subplot(121)  # Subplot 2D\nax2 = fig.add_subplot(122, projection='3d')  # Subplot 3D\nax1.scatter(df.iloc[:, 0], df.iloc[:, 1], c=dbs.labels_, cmap='viridis')\nax1.set_title('Agrupamento DS (2D)')\nax2.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=dbs.labels_, cmap='viridis')\nax2.set_title('Agrupamento DS (3D)')\nplt.show()",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/unsupervised/agrupamento_de_dados.html#considerações-finais",
    "href": "notebooks/unsupervised/agrupamento_de_dados.html#considerações-finais",
    "title": "Agrupamento de dados",
    "section": "Considerações Finais",
    "text": "Considerações Finais\nComo pode ser visto nos gráficos 2D e 3D, após o agrupamento, os dados tendem a se concentrar em volta dos centróides que são dois, visto que correspondem ao número de clusters especificado.",
    "crumbs": [
      "3. Aprendizado Não Supervisionado",
      "3.1 Agrupamento de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/studies/dont_overfit_ii/dont_overfit_ii.html",
    "href": "notebooks/studies/dont_overfit_ii/dont_overfit_ii.html",
    "title": "Configuração de Dados",
    "section": "",
    "text": "import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsample_submission = pd.read_csv('C:\\\\Users\\\\Estrela\\\\Desktop\\\\\\Mestrado\\\\Treino\\\\dont-overfit-ii\\\\sample_submission.csv')\n\ntest_1 = pd.read_csv('C:\\\\Users\\\\Estrela\\\\Desktop\\\\\\Mestrado\\\\Treino\\\\dont-overfit-ii\\\\test_1.csv')\ntest_2 = pd.read_csv('C:\\\\Users\\\\Estrela\\\\Desktop\\\\\\Mestrado\\\\Treino\\\\dont-overfit-ii\\\\test_2.csv')\n\ntest = pd.concat([test_1, test_2])\n\ntrain = pd.read_csv('C:\\\\Users\\\\Estrela\\\\Desktop\\\\Mestrado\\\\Treino\\\\dont-overfit-ii\\\\train.csv')\n\n\nAnalise de Dados\n\ntrain.head(5)\n\n\n\n\n\n\n\n\nid\ntarget\n0\n1\n2\n3\n4\n5\n6\n7\n...\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n\n\n\n0\n0\n1.0\n-1.067\n-1.114\n-0.616\n0.376\n1.090\n0.467\n-0.422\n0.460\n...\n0.220\n-0.339\n0.254\n-0.179\n0.352\n0.125\n0.347\n0.436\n0.958\n-0.824\n\n\n1\n1\n0.0\n-0.831\n0.271\n1.716\n1.096\n1.731\n-0.197\n1.904\n-0.265\n...\n-0.765\n-0.735\n-1.158\n2.554\n0.856\n-1.506\n0.462\n-0.029\n-1.932\n-0.343\n\n\n2\n2\n0.0\n0.099\n1.390\n-0.732\n-1.065\n0.005\n-0.081\n-1.450\n0.317\n...\n-1.311\n0.799\n-1.001\n1.544\n0.575\n-0.309\n-0.339\n-0.148\n-0.646\n0.725\n\n\n3\n3\n1.0\n-0.989\n-0.916\n-1.343\n0.145\n0.543\n0.636\n1.127\n0.189\n...\n-1.370\n1.093\n0.596\n-0.589\n-0.649\n-0.163\n-0.958\n-1.081\n0.805\n3.401\n\n\n4\n4\n0.0\n0.811\n-1.509\n0.522\n-0.360\n-0.220\n-0.959\n0.334\n-0.566\n...\n-0.178\n0.718\n-1.017\n1.249\n-0.596\n-0.445\n1.751\n1.442\n-0.393\n-0.643\n\n\n\n\n5 rows × 302 columns\n\n\n\n\n# Quantificação do target\n\ntrain['target'].value_counts()\n\n0.0    183\n1.0     67\nName: target, dtype: int64\n\n\n\n# Quantidade de nulos\n\ntrain.isna().any().any()\n\nFalse\n\n\n\n# Testando quais colunas estão mais correlacionadas com o resultado, para separá-las de forma decrescente\n# e treiná-las, evitando assim overfitting\n\ntrain.corr()['target'].sort_values(ascending=False)\n\ntarget    1.000000\n127       0.337540\n18        0.206452\n241       0.173879\n3         0.153317\n            ...   \n126      -0.167064\n16       -0.179796\n135      -0.179960\n59       -0.203166\n176      -0.217100\nName: target, Length: 302, dtype: float64\n\n\n\n# 14 primeiras colunas com maiores correlação ao 'target'\n\ntrain.corr().nlargest(15, 'target')['target'].index\n\nIndex(['target', '127', '18', '241', '3', '66', '93', '260', '213', '167',\n       '175', '261', '278', '211', '151'],\n      dtype='object')\n\n\n\nf , ax = plt.subplots(figsize = (14,12))\n\nsns.heatmap(np.corrcoef(train[train.corr().nlargest(15, 'target')['target'].index].values.T), \n            vmax=.8, \n            linewidths=0.01, \n            square=True, \n            annot=True, \n            cmap='viridis',\n            linecolor=\"white\",\n            xticklabels = train.corr().nlargest(15, 'target')['target'].index.values,\n            annot_kws = {'size':12},\n            yticklabels = train.corr().nlargest(15, 'target')['target'].index.values)\n\n\n\n\n\n\n\n\n\n\nTeste de Acurácia para Treinamentos\n\n# Separando melhores colunas em novos data frames\n\ndf_train_x = train[list(train.corr().nlargest(15, 'target')['target'].index)[1:]]\ndf_train_y = train['target']\n\ndf_test_x  = test[list(train.corr().nlargest(15, 'target')['target'].index)[1:]]\n\n\n# Normalizando dados\n\n'''\nO objetivo do MinMaxScaler é normalizar ou redimensionar as características  (ou variáveis) \ncontínuas para um intervalo específico, como 0 e 1. Isso é feito para melhorar o desempenho \ndos algoritmos de aprendizado de máquina, que muitas vezes funcionam melhor quando as carac\nterísticas estão na mesma escala.\n'''\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Dados de exemplo\ndata = df_train_x.columns\n\n# Criar um objeto MinMaxScaler\nscaler = MinMaxScaler()\n\n# Ajustar e transformar os dados\ndata_scaled = scaler.fit_transform(df_train_x)\n\n# Transformando dados normalizados em data frame\n\nscaled_df_train_x = pd.DataFrame(data_scaled, columns=data)\n\nscaled_df_train_x.head(5)\n\n\n\n\n\n\n\n\n127\n18\n241\n3\n66\n93\n260\n213\n167\n175\n261\n278\n211\n151\n\n\n\n\n0\n0.795947\n0.421700\n0.550931\n0.596574\n0.589857\n0.922965\n0.501684\n0.637804\n0.410788\n0.619153\n0.658881\n0.561672\n0.366523\n0.619055\n\n\n1\n0.751492\n0.263822\n0.639913\n0.727770\n0.459745\n0.836144\n0.585419\n0.417918\n0.210433\n0.232599\n0.225508\n0.427318\n0.227919\n0.514652\n\n\n2\n0.551367\n0.583094\n0.648236\n0.334001\n0.261336\n0.502603\n0.257942\n0.265599\n0.333136\n0.638497\n0.338262\n0.628247\n0.217830\n0.102642\n\n\n3\n0.677034\n0.351230\n0.223345\n0.554483\n0.461595\n0.793879\n0.495242\n0.319486\n0.357834\n0.160774\n0.663922\n0.811113\n0.441118\n0.643875\n\n\n4\n0.420829\n0.566155\n0.249306\n0.462464\n0.618730\n0.555278\n0.535354\n0.465799\n0.295594\n0.392738\n0.394892\n0.489936\n0.724115\n0.516093\n\n\n\n\n\n\n\n\n# Preparando dados para testar precisão do treinamento\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_df_train_x, df_train_y, test_size=0.2, random_state=42)\n\n\n# Definindo diferentes métodos de treinamento de máquina para escolher o melhor\n\nimport xgboost as xgb\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Definindo biblioteca para calcular acurácia dos dados de treino\n\nfrom sklearn.metrics import accuracy_score\n\n# Definindo função de treinamento\n\ndef treinamento_de_modelos(modelo, x_t, y_t, x_tt, y_tt, resultado):\n    \n    inicio = time.time()\n    \n    # Treinando\n    modelo.fit(x_t, y_t)\n    \n    # Prevendo\n    y_pred = modelo.predict(x_tt)\n    \n    fim = time.time()\n    \n    # Resultados\n    print(f'Modelo de treino: {str(modelo)}')\n    \n    if y_tt is not None:    \n        print(f'Acurácia: {accuracy_score(y_tt, y_pred)}')\n    \n    print(f'Tempo de Processamento: {fim - inicio:.4f}s')\n    \n    if resultado == 1:\n        return y_pred\n\n\n# Definindo modelos de ML\n\nmodels = [xgb.XGBClassifier(),\n          LGBMClassifier(verbose=-1),\n          RandomForestClassifier(),\n          IsolationForest(n_jobs=-1),\n          LogisticRegression(), \n          KNeighborsClassifier(), \n          SVC()]\n\n# Treinando modelos\n\nfor model in models:\n    \n    # Treinando modelos\n       \n    treinamento_de_modelos(model, x_train, y_train, x_test, y_test, 0)\n    \n    if str(model) != 'SVC()':\n        print('')\n\nModelo de treino: XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\nAcurácia: 0.82\nTempo de Processamento: 0.0370s\n\nModelo de treino: LGBMClassifier(verbose=-1)\nAcurácia: 0.84\nTempo de Processamento: 0.1200s\n\nModelo de treino: RandomForestClassifier()\nAcurácia: 0.84\nTempo de Processamento: 0.2310s\n\nModelo de treino: IsolationForest(n_jobs=-1)\nAcurácia: 0.18\nTempo de Processamento: 0.1710s\n\nModelo de treino: LogisticRegression()\nAcurácia: 0.84\nTempo de Processamento: 0.0070s\n\nModelo de treino: KNeighborsClassifier()\nAcurácia: 0.72\nTempo de Processamento: 0.0050s\n\nModelo de treino: SVC()\nAcurácia: 0.8\nTempo de Processamento: 0.0060s\n\n\n\n\nTreinamento de Máquina\n\n'''\nComo o modelo de regressão logística foi o que obteve melhor desempenho, será ele o usado p\nara treinar os dados.\n'''\n\n# Treinando\nresultados = treinamento_de_modelos(LogisticRegression(), df_train_x, df_train_y, df_test_x, None, 1)\n\n# Criando DataFrame\ndf_resultados = pd.DataFrame({'id': test['id'], 'target': resultados})\n\n# Mudando index\ndf_resultados.set_index('id', inplace=True)\n\nModelo de treino: LogisticRegression()\nTempo de Processamento: 0.0070s\n\n\n\ndf_resultados\n\n\n\n\n\n\n\n\ntarget\n\n\nid\n\n\n\n\n\n250\n0.0\n\n\n251\n0.0\n\n\n252\n0.0\n\n\n253\n1.0\n\n\n254\n1.0\n\n\n...\n...\n\n\n19995\n0.0\n\n\n19996\n0.0\n\n\n19997\n0.0\n\n\n19998\n0.0\n\n\n19999\n0.0\n\n\n\n\n19750 rows × 1 columns\n\n\n\n\n\nExportação de Resultados\n\ndf_resultados.to_csv('C:\\\\Users\\\\Estrela\\\\Desktop\\\\doii.csv', header=True)",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.2 Dont Overfit II"
    ]
  },
  {
    "objectID": "notebooks/studies/toxic_tweets/final_project.html",
    "href": "notebooks/studies/toxic_tweets/final_project.html",
    "title": "Projeto Final",
    "section": "",
    "text": "Professor: Elias Jacob\nAluno: Alexandre Estrela de L. Nobrega\nConjunto de Dados: ToLD-Br\n\n\nResumo do Conjunto de Dados\nToLD-Br é o maior conjunto de dados para tweets tóxicos em português brasileiro, desenvolvido a partir de 42 anotadores selecionados a partir de um grupo de 129 voluntários. Os anotadores foram selecionados visando criar um grupo plural em termos de demografia (etnia, orientação sexual, idade, gênero). Cada tweet foi rotulado por três anotadores em 6 categorias possíveis: LGBTQ+fobia, Xenofobia, Obsceno, Insulto, Misoginia e Racismo.\nDado os rótulos, foi possível definir um dataset com texto do tweet (string) seguido por uma classe binária “tóxica” com valores 0 ou 1. O valor de 0 representa um texto não tóxico, e 1 representa um comportamento tóxico.\n\nBaixar dados.\nFonte dos dados.\n\n\nConfigurando Dados\n\nimport pandas as pd\n\n# Importando Dados\ndf = pd.read_csv('datas/ptbr_train_1annotator.csv')\n#df_test = pd.read_csv('datas/ptbr_test_1annotator.csv')\n#df_validation = pd.read_csv('datas/ptbr_validation_1annotator.csv')\n\n# Visualizando Dados\ndf.head()\n\n\n\n\n\n\n\n\ntext\ntoxic\n\n\n\n\n0\nrt @user olha quem chegouuuuu, nossos queridin...\n0\n\n\n1\nveio umas teorias muito loucas na minha cabeça...\n1\n\n\n2\n@user @user 😂😂😂😂mais nao tinha falado ontem qu...\n0\n\n\n3\nrt @user quer ser filha da puta logo comigo qu...\n1\n\n\n4\nvai besta 😂😂😂😂 casquei com a ultima foto\n1\n\n\n\n\n\n\n\n\n\nFunção de Limpeza de Dados\n\n# Funções para limpeza de dados\n\nimport unicodedata\n\ndef accented_characters(text):\n\n    normalized_text = unicodedata.normalize('NFKD', text)\n\n    ascii_text = normalized_text.encode('ASCII', 'ignore')\n\n    return ascii_text.decode()\n\nimport re\n\ndef numbers_and_punctuation(text):\n\n    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n\n    return text\n\ndef excessive_spaces(text):\n\n    text = re.sub(r'\\s+', ' ', text)\n\n    return text\n\ndef n_words(text, n):\n\n    text = ' '.join([word for word in text.split() if len(word) &gt; n])\n\n    return text\n\ndef all_clear(text, ac = 0, np = 0, es = 0, nw = 0, n = 1):\n\n    if ac == 1:\n        text = accented_characters(text)\n\n    if np == 1:\n        text = numbers_and_punctuation(text)\n\n    if es == 1:\n        text = excessive_spaces(text)\n\n    if nw == 1:\n        text = n_words(text, n)\n\n    return text\n\n\n# Aplicando limpeza de dados\ndf['text'] = df['text'].astype(str).apply(all_clear, ac=1, np=1, es=1, nw=1, n=1)\n\n# Visualizando Dados\ndf.head()\n\n\n\n\n\n\n\n\ntext\ntoxic\n\n\n\n\n0\nrt user olha quem chegouuuuu nossos queridinho...\n0\n\n\n1\nveio umas teorias muito loucas na minha cabeca...\n1\n\n\n2\nuser user mais nao tinha falado ontem que nao ...\n0\n\n\n3\nrt user quer ser filha da puta logo comigo que...\n1\n\n\n4\nvai besta casquei com ultima foto\n1\n\n\n\n\n\n\n\n\n\nAnálise de Palavras mais Citadas\nÉ sempre bom ter uma noção das palavras que mais aparecem nas frases dos usuários, para assim receber possíveis direcionamentos na criação das funções de rotulagem.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport numpy as np\n\nstopwords_nltk = stopwords.words('portuguese')\n\n# Carregar as stopwords em português\nvectorizer = TfidfVectorizer(stop_words=stopwords_nltk, max_features=2000, ngram_range=(1, 2), min_df=5, max_df=0.8, lowercase=True)\n\n# Criar um vetor TF-IDF\nvectorizer.fit(df.text) # Ajuste apenas com os dados de treinamento.\n\n# Transformar os textos em vetores\nX_train = vectorizer.transform(df.text)\ny_train = df.toxic\n\ndef get_top_ngrams(X_train: np.ndarray, vectorizer: TfidfVectorizer, top_n: int = 30) -&gt; np.ndarray:\n    \"\"\"\n    Obter os n-grams mais frequentes dos dados textuais vetorizados.\n\n    Args:\n        X_treino (np.ndarray): Os dados textuais vetorizados.\n        vectorizer (TfidfVectorizer): O vetor utilizado para transformar os dados textuais.\n        top_n (int, optional): O número de top n-grams a retornar. Padrão é 30.\n\n    Returns:\n        np.ndarray: Um array dos n-grams mais frequentes.\n    \"\"\"\n    # Somar todas as colunas para obter a frequência total de cada n-gram\n    total_ngram_frequencies = np.sum(X_train, axis=0)\n\n    # Ordenar os n-grams pela sua frequência total\n    sorted_ngrams_indices = np.argsort(total_ngram_frequencies)[::-1]\n\n    # Obter os índices dos top n n-grams mais frequentes\n    top_ngrams_indices = sorted_ngrams_indices[:top_n]\n\n    # Obter os nomes dos n-grams correspondentes aos top n índices\n    ngram_names = np.array(vectorizer.get_feature_names_out())\n\n    return ngram_names[top_ngrams_indices]\n\n# Usar a função para obter os top 30 n-grams dos dados de treinamento\ntop_ngrams = get_top_ngrams(X_train, vectorizer, top_n=30)\n\nprint('Palavras mais citadas:\\n')\n\nfor words in top_ngrams:\n    for i, word in enumerate(words):\n        print(word, end=', ')\n        if i % 5 == 0 and i != 0:\n            print()\n\n        if i == 10:\n            break\n\nPalavras mais citadas:\n\ngt gt, honesto, proximidade, voces nao, salles, ano passado, \naqui nao, primeira impressao, voce vai, correr, pequena, \n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Flatten the list of lists into a single list of words\nall_words = [word for words in top_ngrams for word in words]\n\n# Join all words into a single string\ntext = ' '.join(all_words)\n\n# Generate the word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Display the word cloud using matplotlib\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeparando DataFrame entre Treino, Teste e Desenvolvimento\n\nimport math\n\nfrom sklearn.model_selection import train_test_split\n\n# Criando backup do DataFrame original:\ndf_label = df.copy()\n\nz = 1.96 # Valor Z para nível de confiança de 95%\np = 0.5 # Proporção esperada de avaliações positivas. Assumimos 50%, que é o pior cenário.\ne = 0.05 # Margem de erro\n\n# Calculando tamanho da amostra e arredondando\ntamanho_amostra = math.ceil((z**2 * p * (1-p)) / e**2)\n\nprint(f\"\\nTamanho da amostra: {tamanho_amostra}.\\n\")\n\n# Embaralhando ordem do DataFrame\ndf_label = df_label.sample(frac=1.0, random_state=1)\n\n# Criando novo DataFrame para treino com primeira metade do DataFrame embaralhado\ndf_train = df_label[tamanho_amostra * 2:]\n\n# Criando DataFrame para treino final com primeira metade do DataFrame embaralhado\ndf_train_final = df_label[tamanho_amostra * 2:]\n\n# Excluindo rótulos do DataFrame de Treino\ndf_train = df_train.drop('toxic', axis=1)\n\n# Criando novo DataFrame para teste com segunda metade do DataFrame embaralhado\ndf_develop_test = df_label[:tamanho_amostra * 2]\n\n# Dividindo aleatoriamente DataFrames entre teste e desenvolvimento\ndf_test, df_develop = train_test_split(df_develop_test, \n                                       test_size=0.5, \n                                       random_state=1, \n                                       stratify=df_develop_test.toxic)\n\nprint(f\"Tamanho do conjunto de dados original: {len(df_label)}.\")\nprint(f\"Tamanho do conjunto de treinamento: {len(df_train)}.\")\nprint(f\"Tamanho do conjunto de teste: {len(df_test)}.\")\nprint(f\"Tamanho do conjunto de desenvolvimento: {len(df_develop)}.\")\n\n\nTamanho da amostra: 385.\n\nTamanho do conjunto de dados original: 16800.\nTamanho do conjunto de treinamento: 16030.\nTamanho do conjunto de teste: 385.\nTamanho do conjunto de desenvolvimento: 385.\n\n\n\n\nLabeling Functions (Funções de Rotulagem | LF) para Palavras\nAs funções de rotulagem de palavras para o presente dataset, servirão para definir com base nas palavras da frase do usuário, se seu comentário foi tóxico ou não.\n\n( 1 ) Tóxico: Quando alguma palavra ofensiva e/ou tóxica está presente na frase.\n( 0 ) Não tóxico: Quando alguma palavra de elogio e/ou gentil está presente na frase.\n( -1 ) Abstenção: Quando as palavras tóxicas ou não tóxicas não estiverem presentes na frase.\n\n\nimport re\n\nfrom snorkel.labeling import labeling_function\nfrom snorkel.labeling import PandasLFApplier\n\n# Definindo um dicionário com os padrões regex\nregex_patterns = {\n    \"e_amor\": r'amor',\n    \"o_boiola\": r'boiola',\n    \"e_bom dia\": r'bom dia',\n    \"e_boa tarde\": r'boa tarde',\n    \"o_buceta\": r'buceta',\n    \"o_bunda\": r'bunda',\n    \"e_carai\": r'carai',\n    \"o_caralho\": r'caralho',\n    \"o_cu\": r'cu',\n    \"e_feliz\": r'feliz',\n    \"o_f[uo]de\": r'f[uo]de',\n    \"e_incrivel\": r'incrivel',\n    \"e_lind[oa]\": r'lind[oa]',\n    \"e_maravilhos[oa]\": r'maravilhos[oa]',\n    \"o_merd\": r'merd',\n    \"e_mulherao\": r'mulherao',\n    \"o_pal\": r'pal',\n    \"o_pica\": r'pica',\n    \"o_pinto\": r'pinto',\n    \"o_piranha\": r'piranha',\n    \"o_podre\": r'podre',\n    \"o_porra\": r'porra',\n    \"o_pqp\": r'pqp',\n    \"o_put\": r'put',\n    \"o_putinh\": r'putinh',\n    \"o_ridicul\": r'ridicul',\n    \"o_sapatao\": r'sapatao',\n    \"o_tnc\": r'tnc',\n    \"o_vaca\": r'vaca',\n    \"o_vagabund\": r'vagabund',\n    \"o_viado\": r'viado',\n    \"e_vsf\": r'vsf',\n    \"o_vtnc\": r'vtnc'\n}\n\n# Função que define funções que definem as Labeling Funcions\ndef create_labeling_function(index, pattern, label_positive, label_negative):\n\n    regex = re.compile(pattern, re.IGNORECASE)\n\n    @labeling_function(name=f'lf_r_{index}')\n    def lf(x):\n        return label_positive if regex.search(x.text) else label_negative\n    \n    return lf\n\n# Definindo Funções de Labeling Functions\nlfs = []\n\nfor i, item in regex_patterns.items():\n\n    if i[0] == 'o':\n        lfs.append(create_labeling_function(i, item, 1, -1))\n\n    else:\n        lfs.append(create_labeling_function(i, item, 0, -1))\n\napplier = PandasLFApplier(lfs=lfs)\n\nNota-se que a esmagadora maioria das das palavras definidas nas LF’s não estão presentes no gráfico da nuvem de palavras. Porém é importante ter em mente que a escolha das palavras que irão compor elas não está unicamente associada a quantidade de vezes que elas aparecem nos textos, pois também é relevante escolher palavras com alto grau de certeza de que caso elas componham a frase, será obtido um rótulo correto.\n\nAnalisando Influência das Funções de Rotulagem\n\nj: Índice da função de rotulagem\nPolarity: Se a LF se refere a tóxidade e/ou não tóxidade.\nCoverage: Proporção de vezes que uma função de rotulagem forneceu rotulos para instâncias (amostras de dados).\nOverlaps: Proporção de vezes que uma função de rotulagem forneceu rótulos para instâncias que foram rotuladas por pelo menos outra função de rotulagem de mesma polaridade.\nConflicts: Proporção de vezes que uma função de rotulagem atribuiu rótulos de polaridade diferente dos rótulos atribuídos por outras funções de rotulagem para as mesmas instâncias.\n\n\nfrom snorkel.labeling import LFAnalysis\n\n# Aplica as funções de rotulagem (lfs) no conjunto de dados de treinamento (df_label_train_tain)\n# L_train é uma matriz onde cada linha representa um exemplo e cada coluna representa a saída de uma função de rotulagem\nL_train = applier.apply(df=df_train)\n\nprint(f'\\nTotal Coverge: {LFAnalysis(L=L_train, lfs=lfs).label_coverage()}.')\n\n# Cria uma instância de LFAnalysis para analisar o desempenho das funções de rotulagem\nLFAnalysis(L=L_train, lfs=lfs).lf_summary()\n\n100%|██████████| 16030/16030 [00:04&lt;00:00, 3826.20it/s]\n\n\n\nTotal Coverge: 0.536930754834685.\n\n\n\n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\n\n\n\n\nlf_r_e_amor\n0\n[0]\n0.022021\n0.012227\n0.010792\n\n\nlf_r_o_boiola\n1\n[1]\n0.001934\n0.000374\n0.000250\n\n\nlf_r_e_bom dia\n2\n[0]\n0.002246\n0.001310\n0.001061\n\n\nlf_r_e_boa tarde\n3\n[0]\n0.000873\n0.000437\n0.000250\n\n\nlf_r_o_buceta\n4\n[1]\n0.003493\n0.001622\n0.000187\n\n\nlf_r_o_bunda\n5\n[1]\n0.004616\n0.003431\n0.000686\n\n\nlf_r_e_carai\n6\n[0]\n0.016594\n0.003369\n0.002870\n\n\nlf_r_o_caralho\n7\n[1]\n0.079538\n0.025639\n0.007548\n\n\nlf_r_o_cu\n8\n[1]\n0.117467\n0.042296\n0.009108\n\n\nlf_r_e_feliz\n9\n[0]\n0.012976\n0.005864\n0.005053\n\n\nlf_r_o_f[uo]de\n10\n[1]\n0.022208\n0.009482\n0.002059\n\n\nlf_r_e_incrivel\n11\n[0]\n0.003556\n0.001809\n0.001310\n\n\nlf_r_e_lind[oa]\n12\n[0]\n0.021460\n0.012601\n0.010917\n\n\nlf_r_e_maravilhos[oa]\n13\n[0]\n0.003431\n0.001934\n0.001560\n\n\nlf_r_o_merd\n14\n[1]\n0.025203\n0.019588\n0.002371\n\n\nlf_r_e_mulherao\n15\n[0]\n0.000936\n0.000936\n0.000873\n\n\nlf_r_o_pal\n16\n[1]\n0.015721\n0.007361\n0.001372\n\n\nlf_r_o_pica\n17\n[1]\n0.004492\n0.001123\n0.000312\n\n\nlf_r_o_pinto\n18\n[1]\n0.002932\n0.000437\n0.000062\n\n\nlf_r_o_piranha\n19\n[1]\n0.004492\n0.001372\n0.000374\n\n\nlf_r_o_podre\n20\n[1]\n0.001061\n0.000561\n0.000125\n\n\nlf_r_o_porra\n21\n[1]\n0.093637\n0.025390\n0.007611\n\n\nlf_r_o_pqp\n22\n[1]\n0.080599\n0.016032\n0.005864\n\n\nlf_r_o_put\n23\n[1]\n0.075483\n0.028759\n0.004991\n\n\nlf_r_o_putinh\n24\n[1]\n0.003119\n0.003119\n0.000062\n\n\nlf_r_o_ridicul\n25\n[1]\n0.008921\n0.008921\n0.000312\n\n\nlf_r_o_sapatao\n26\n[1]\n0.003369\n0.000873\n0.000312\n\n\nlf_r_o_tnc\n27\n[1]\n0.010044\n0.005614\n0.000499\n\n\nlf_r_o_vaca\n28\n[1]\n0.003182\n0.000499\n0.000125\n\n\nlf_r_o_vagabund\n29\n[1]\n0.007735\n0.003493\n0.000561\n\n\nlf_r_o_viado\n30\n[1]\n0.006925\n0.001684\n0.000499\n\n\nlf_r_e_vsf\n31\n[0]\n0.012165\n0.003930\n0.003119\n\n\nlf_r_o_vtnc\n32\n[1]\n0.004616\n0.004616\n0.000374\n\n\n\n\n\n\n\nA tabela mostra que as LF’s conseguiram cobrir aproximadamente 54% de todo o dataset, com algumas palavras que estiveram presentes na nuvem de palavras (porra e cu) ganhando destaque por seu alto Coverage, e baixo Conflict.\n\n\nAnalisando o Acerto das Funções de Rotulagem\n\n# Aplica as funções de rotulagem (lfs) no conjunto de dados de desenvolvimento (df_label_train_develop)\n# L_dev é uma matriz onde cada linha representa um exemplo e cada coluna representa a saída de uma função de rotulagem\nL_dev = applier.apply(df=df_develop)\n\n# Cria uma instância de LFAnalysis para analisar o desempenho das funções de rotulagem, com acertos e erros\nLFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=df_develop.toxic.values)\n\n100%|██████████| 385/385 [00:00&lt;00:00, 3457.98it/s]\n\n\n\n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\nCorrect\nIncorrect\nEmp. Acc.\n\n\n\n\nlf_r_e_amor\n0\n[0]\n0.031169\n0.018182\n0.018182\n9\n3\n0.750000\n\n\nlf_r_o_boiola\n1\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_e_bom dia\n2\n[0]\n0.005195\n0.002597\n0.002597\n2\n0\n1.000000\n\n\nlf_r_e_boa tarde\n3\n[0]\n0.005195\n0.005195\n0.002597\n1\n1\n0.500000\n\n\nlf_r_o_buceta\n4\n[1]\n0.010390\n0.002597\n0.000000\n4\n0\n1.000000\n\n\nlf_r_o_bunda\n5\n[1]\n0.007792\n0.005195\n0.000000\n3\n0\n1.000000\n\n\nlf_r_e_carai\n6\n[0]\n0.010390\n0.002597\n0.002597\n3\n1\n0.750000\n\n\nlf_r_o_caralho\n7\n[1]\n0.103896\n0.031169\n0.005195\n27\n13\n0.675000\n\n\nlf_r_o_cu\n8\n[1]\n0.083117\n0.025974\n0.005195\n21\n11\n0.656250\n\n\nlf_r_e_feliz\n9\n[0]\n0.018182\n0.007792\n0.005195\n5\n2\n0.714286\n\n\nlf_r_o_f[uo]de\n10\n[1]\n0.033766\n0.012987\n0.002597\n10\n3\n0.769231\n\n\nlf_r_e_incrivel\n11\n[0]\n0.005195\n0.002597\n0.002597\n1\n1\n0.500000\n\n\nlf_r_e_lind[oa]\n12\n[0]\n0.018182\n0.005195\n0.005195\n4\n3\n0.571429\n\n\nlf_r_e_maravilhos[oa]\n13\n[0]\n0.005195\n0.000000\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_merd\n14\n[1]\n0.023377\n0.018182\n0.005195\n8\n1\n0.888889\n\n\nlf_r_e_mulherao\n15\n[0]\n0.002597\n0.002597\n0.002597\n1\n0\n1.000000\n\n\nlf_r_o_pal\n16\n[1]\n0.010390\n0.010390\n0.005195\n3\n1\n0.750000\n\n\nlf_r_o_pica\n17\n[1]\n0.002597\n0.000000\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_pinto\n18\n[1]\n0.007792\n0.000000\n0.000000\n2\n1\n0.666667\n\n\nlf_r_o_piranha\n19\n[1]\n0.002597\n0.000000\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_podre\n20\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_porra\n21\n[1]\n0.106494\n0.044156\n0.015584\n29\n12\n0.707317\n\n\nlf_r_o_pqp\n22\n[1]\n0.090909\n0.015584\n0.007792\n21\n14\n0.600000\n\n\nlf_r_o_put\n23\n[1]\n0.088312\n0.041558\n0.005195\n25\n9\n0.735294\n\n\nlf_r_o_putinh\n24\n[1]\n0.007792\n0.007792\n0.000000\n3\n0\n1.000000\n\n\nlf_r_o_ridicul\n25\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_sapatao\n26\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_tnc\n27\n[1]\n0.015584\n0.007792\n0.000000\n4\n2\n0.666667\n\n\nlf_r_o_vaca\n28\n[1]\n0.002597\n0.000000\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_vagabund\n29\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_viado\n30\n[1]\n0.005195\n0.000000\n0.000000\n2\n0\n1.000000\n\n\nlf_r_e_vsf\n31\n[0]\n0.007792\n0.002597\n0.002597\n2\n1\n0.666667\n\n\nlf_r_o_vtnc\n32\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\n\n\n\n\n\nComo pode ser visto na tabela, todas as funções de rotulagem obtiveram um grau de acertabilidade, ao comparar com os rótulos originais, igual ou maior que 50% (com a esmagadora maioria sendo maior).\n\n\nAnalisando Conflito entre Funções de Rotulagem\n\ndef compute_conflict_matrices(L_train, lfs):\n\n    # Função para criar e processar a matriz de conflitos normalizada\n    def process_conflict_matrix(L, lfs, normalize=False, column_name='conflict'):\n        lf_analysis = LFAnalysis(L=L, lfs=lfs)\n        conflict_matrix = lf_analysis.lf_conflicts(normalize_by_overlaps=normalize)\n        conflict_matrix = pd.DataFrame([conflict_matrix], columns=[lf.name for lf in lfs]).T\n        conflict_matrix.columns = [column_name]\n        conflict_matrix = conflict_matrix.sort_values(by=column_name, ascending=False)\n        return conflict_matrix\n\n    # Calcular a matriz de conflitos normalizada\n    normalized_conflict_matrix = process_conflict_matrix(L_train, lfs, normalize=True, column_name='normalized_conflict')\n\n    # Calcular a matriz de conflitos não normalizada\n    conflict_matrix = process_conflict_matrix(L_train, lfs, normalize=False, column_name='conflict')\n\n    # Concatenar as duas matrizes de conflitos\n    final_cm = pd.concat([conflict_matrix, normalized_conflict_matrix], axis=1)\n\n    return final_cm\n\n# Calcular a matriz de conflitos final\ncompute_conflict_matrices(L_train, lfs)\n\n\n\n\n\n\n\n\nconflict\nnormalized_conflict\n\n\n\n\nlf_r_e_lind[oa]\n0.010917\n0.866337\n\n\nlf_r_e_amor\n0.010792\n0.882653\n\n\nlf_r_o_cu\n0.009108\n0.215339\n\n\nlf_r_o_porra\n0.007611\n0.299754\n\n\nlf_r_o_caralho\n0.007548\n0.294404\n\n\nlf_r_o_pqp\n0.005864\n0.365759\n\n\nlf_r_e_feliz\n0.005053\n0.861702\n\n\nlf_r_o_put\n0.004991\n0.173536\n\n\nlf_r_e_vsf\n0.003119\n0.793651\n\n\nlf_r_e_carai\n0.002870\n0.851852\n\n\nlf_r_o_merd\n0.002371\n0.121019\n\n\nlf_r_o_f[uo]de\n0.002059\n0.217105\n\n\nlf_r_e_maravilhos[oa]\n0.001560\n0.806452\n\n\nlf_r_o_pal\n0.001372\n0.186441\n\n\nlf_r_e_incrivel\n0.001310\n0.724138\n\n\nlf_r_e_bom dia\n0.001061\n0.809524\n\n\nlf_r_e_mulherao\n0.000873\n0.933333\n\n\nlf_r_o_bunda\n0.000686\n0.200000\n\n\nlf_r_o_vagabund\n0.000561\n0.160714\n\n\nlf_r_o_viado\n0.000499\n0.296296\n\n\nlf_r_o_tnc\n0.000499\n0.088889\n\n\nlf_r_o_vtnc\n0.000374\n0.081081\n\n\nlf_r_o_piranha\n0.000374\n0.272727\n\n\nlf_r_o_ridicul\n0.000312\n0.034965\n\n\nlf_r_o_sapatao\n0.000312\n0.357143\n\n\nlf_r_o_pica\n0.000312\n0.277778\n\n\nlf_r_o_boiola\n0.000250\n0.666667\n\n\nlf_r_e_boa tarde\n0.000250\n0.571429\n\n\nlf_r_o_buceta\n0.000187\n0.115385\n\n\nlf_r_o_podre\n0.000125\n0.222222\n\n\nlf_r_o_vaca\n0.000125\n0.250000\n\n\nlf_r_o_putinh\n0.000062\n0.020000\n\n\nlf_r_o_pinto\n0.000062\n0.142857\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef conflict_matrix(lfs, L):\n\n    \"\"\"\n    Esta função calcula a matriz de conflitos para um conjunto de funções de rotulagem e um conjunto de dados rotulados.\n\n    Args:\n    - lfs: Uma lista de funções de rotulagem.\n    - L: Um array numpy de forma (num_examples, num_lfs) contendo os rótulos atribuídos por cada função de rotulagem a cada exemplo.\n\n    Retorna:\n    - Um DataFrame do pandas contendo a matriz de conflitos.\n    \"\"\"\n\n    # Inicializa uma matriz de zeros com forma (num_lfs, num_lfs)\n    conflict_matrix = np.zeros((len(lfs), len(lfs)))\n\n    # Calcula o conflito entre cada par de funções de rotulagem\n    for i in range(len(lfs)):\n        for j in range(len(lfs)):\n            # Calcula o conflito entre as funções de rotulagem i e j\n            conflict_matrix[i, j] = ((L[:, i] != L[:, j]) & (L[:, i] != -1) & (L[:, j] != -1)).mean()\n\n    # Converte a matriz de conflitos para um DataFrame do pandas com os nomes das funções de rotulagem como rótulos das linhas e colunas\n    conflict_matrix = pd.DataFrame(conflict_matrix, columns=[lf.name for lf in lfs], index=[lf.name for lf in lfs])\n    return conflict_matrix\n\ndef matrix_graph(lfs, L_train):\n\n    # Plotar a matriz de conflitos usando seaborn\n    plt.figure(figsize=(7, 4))\n    sns.heatmap(conflict_matrix(lfs, L_train), annot=False, cmap=\"coolwarm\", cbar=True, annot_kws={\"fontsize\": 6})\n    plt.title(\"Matriz de Conflitos entre Funções de Rotulagem\\n\")\n    plt.xticks(rotation=45, fontsize=6)\n    plt.yticks(rotation=45, fontsize=6)\n    plt.show()\n\nmatrix_graph(lfs, L_train)\n\n\n\n\n\n\n\n\nO gráfico da matriz de conflito demonstra que de forma geral é bem baixo o índice de conflitos, com apenas alguns pontos apresentando uma maior diferença do padrão.\n\n\n\nLabeling Functions para Palavras com uso de Transformer\nO uso de um transformer como LF, será a aplicação de um modelo pré-treinado (a partir de um dataset com características semelhantes) para a rotulação das instâncias. O autor do transformer utilizado no presente carderno o define como um classificador de sequência de modelo multilíngue Distil-Bert treinado com base no conjunto de dados JIGSAW Toxic Comment Classification Challenge.\n\nO autor do transformer explica que o modelo foi treinado em um subconjunto aleatório do conjunto de dados told-br (1/3 do tamanho original). Ele deixa claro que o principal objetivo é fornecer um pequeno modelo que possa ser usado para classificar tweets em português brasileiro de forma binária (‘tóxico’ ou ‘não tóxico’).\nO conjunto de dados que o transformer utilizou curiosamente foi o mesmo que está sendo usado no presente estudo. Será interessante ver como ele desempenhará apenas compondo parte das funções de rotulagem.\n\nFonte do modelo do Transformer.\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\n# Define o caminho do modelo (um caminho local ou um identificador de modelo de huggingface.co/models)\nmodel_path = \"inctdd/told_br_binary_sm\"\n\n# Carrega o tokenizador e o modelo\ntokenizer_hf = AutoTokenizer.from_pretrained(model_path)\nmodel_hf = AutoModelForSequenceClassification.from_pretrained(model_path)\n\n# Cria um classificador de sentimento que une o tokenizador e o modelo como um pipeline\ntoxic_classifier = pipeline(\"text-classification\", model=model_hf, tokenizer=tokenizer_hf, device=1 if torch.cuda.is_available() else -1, verbose=False)\n\n# Imprime o dispositivo em que o modelo está. Cuda significa GPU, caso contrário, CPU. GPU é mais rápido.\nprint(f'- Modelo em que o dispositivo está: {model_hf.device}.\\n')\n\nfor row in df_train.sample(10, random_state=1).itertuples():\n    print(f'Text: {row.text}')\n    print(f'Label: {toxic_classifier(row.text)[0]['label']}')\n    print(f'Score: {toxic_classifier(row.text)[0]['score']}\\n')\n\n- Modelo em que o dispositivo está: cpu.\n\nText: estou juntando dinheiro no cofre focoooo vou encher essa porra toda\nLabel: 0\nScore: 0.630939781665802\n\nText: user Depois de encher rabo com dinheiro da lei Rouanet se aproveitar do dinheiro dos pagadores de imposto tem cara de pau de vir publico defender militantes disfarcados de jornalistas cujo unico objetivo soltar Jumento de nove dedos\nLabel: 1\nScore: 0.9713601469993591\n\nText: user de cair cu da bunda\nLabel: 1\nScore: 0.9717990159988403\n\nText: user bah entao tu imagina com vou ficar quase tao linda quanto tu\nLabel: 0\nScore: 0.9809370636940002\n\nText: montando meu primeiro curriculo para estagio la no hospital so consigo pensar beleza nome ja foi agora que que eu escrevo\nLabel: 0\nScore: 0.9876489043235779\n\nText: boa tarde so pra quem ta com fome de cu\nLabel: 1\nScore: 0.9587632417678833\n\nText: to ligada no pqp\nLabel: 1\nScore: 0.5966371893882751\n\nText: hoje eu acordei com uma vontade de morrer me olhei no espelho nao aguentei nao suporto nem me ver eu sou assim um lixo burro fracassado acho que eu vou me matar\nLabel: 1\nScore: 0.8548468351364136\n\nText: rt user me matem de uma vez porraaaaa ahhahaha eu toda quando vi foto kkkkk https co mwl ssaw\nLabel: 0\nScore: 0.633847713470459\n\nText: user que tem ver um com os outros\nLabel: 0\nScore: 0.974162757396698\n\n\n\nCriando Labeling Function para Transformer\n\nfrom snorkel.preprocess import preprocessor\n\n# Define um pré-processador usando a função de decorador do Snorkel\n@preprocessor(memoize=True)\ndef hf_toxic(x):\n\n    \"\"\"\n    Esta função é um pré-processador que usa um classificador de toxidade pré-treinado para adicionar\n    uma etiqueta de previsão e um score aos dados de entrada.\n\n    Args:\n    - x: Um objeto contendo os dados de entrada, que deve ter um atributo 'text' contendo o texto a ser classificado.\n\n    Retorna:\n    - O mesmo objeto de entrada 'x', mas com atributos adicionais 'pred_label' e 'score' adicionados com base nas previsões do modelo.\n    \"\"\"\n\n    # Define argumentos para o tokenizador, garantindo que o texto seja preenchido e truncado para um comprimento máximo de 512 tokens\n    tokenizer_kwargs = {'padding': True, 'truncation': True, 'max_length': 512}\n\n    # Obtém as pontuações de sentimento do classificador de sentimento pré-treinado\n    # O [0] no final é usado para obter o primeiro (e único) resultado do classificador, pois a saída é uma lista de previsões\n    scores = toxic_classifier(x.text, **tokenizer_kwargs)[0]\n\n    # Adiciona a etiqueta prevista ('label') e a pontuação ('score') aos atributos do objeto 'x'\n    x.pred_label = scores.get('label')\n    x.score = scores.get('score')\n\n    # Retorna o objeto 'x' modificado\n    return x\n\n@labeling_function(pre=[hf_toxic])\ndef lf_hf_toxic(x):\n\n    \"\"\"\n    Esta função de rotulagem atribui um rótulo baseado no sentimento previsto por um classificador de sentimento pré-treinado.\n\n    Args:\n    - x: Um objeto de entrada que deve ter atributos 'pred_label' e 'score' adicionados pelo pré-processador 'hf_toxic'.\n\n    Retorna:\n    - ( 1) se a pontuação do sentimento for maior ou igual a 0.85 e o rótulo previsto por 'toxic'.\n    - ( 0) se a pontuação do sentimento for maior ou igual a 0.85 e o rótulo previsto for 'not_toxic'.\n    - (-1) caso contrário.\n    \"\"\"\n\n    # Verifica se a pontuação do sentimento é maior ou igual a 0.85\n    if x.score &gt;= 0.85:\n\n        # Se o rótulo previsto for 'Positive', retorna YES\n        if x.pred_label == '1':\n            return 1\n        \n        # Se o rótulo previsto for 'Negative', retorna NO\n        elif x.pred_label == '0':\n            return 0\n    \n    # Retorna ABSTAIN se a pontuação for menor que 0.85 ou o rótulo não for 'toxic' nem 'not_toxic'\n    return -1\n\nlfs.append(lf_hf_toxic)\n\nAplicando as Funções de Rotulagem antigas com a nova do Transformer\n\napplier = PandasLFApplier(lfs=lfs)\n\nL_train = applier.apply(df=df_train)\n\nL_dev = applier.apply(df=df_develop)\n\nprint(f'\\nTotal Coverge: {LFAnalysis(L=L_dev, lfs=lfs).label_coverage()}.')\n\nLFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=df_develop.toxic.values)\n\n100%|██████████| 16030/16030 [31:06&lt;00:00,  8.59it/s] \n100%|██████████| 385/385 [00:42&lt;00:00,  9.01it/s]\n\n\n\nTotal Coverge: 0.8649350649350649.\n\n\n\n\n\n\n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\nCorrect\nIncorrect\nEmp. Acc.\n\n\n\n\nlf_r_e_amor\n0\n[0]\n0.031169\n0.031169\n0.018182\n9\n3\n0.750000\n\n\nlf_r_o_boiola\n1\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_e_bom dia\n2\n[0]\n0.005195\n0.005195\n0.002597\n2\n0\n1.000000\n\n\nlf_r_e_boa tarde\n3\n[0]\n0.005195\n0.005195\n0.002597\n1\n1\n0.500000\n\n\nlf_r_o_buceta\n4\n[1]\n0.010390\n0.010390\n0.000000\n4\n0\n1.000000\n\n\nlf_r_o_bunda\n5\n[1]\n0.007792\n0.007792\n0.000000\n3\n0\n1.000000\n\n\nlf_r_e_carai\n6\n[0]\n0.010390\n0.002597\n0.002597\n3\n1\n0.750000\n\n\nlf_r_o_caralho\n7\n[1]\n0.103896\n0.044156\n0.010390\n27\n13\n0.675000\n\n\nlf_r_o_cu\n8\n[1]\n0.083117\n0.072727\n0.028571\n21\n11\n0.656250\n\n\nlf_r_e_feliz\n9\n[0]\n0.018182\n0.018182\n0.005195\n5\n2\n0.714286\n\n\nlf_r_o_f[uo]de\n10\n[1]\n0.033766\n0.028571\n0.002597\n10\n3\n0.769231\n\n\nlf_r_e_incrivel\n11\n[0]\n0.005195\n0.002597\n0.002597\n1\n1\n0.500000\n\n\nlf_r_e_lind[oa]\n12\n[0]\n0.018182\n0.018182\n0.007792\n4\n3\n0.571429\n\n\nlf_r_e_maravilhos[oa]\n13\n[0]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_merd\n14\n[1]\n0.023377\n0.020779\n0.005195\n8\n1\n0.888889\n\n\nlf_r_e_mulherao\n15\n[0]\n0.002597\n0.002597\n0.002597\n1\n0\n1.000000\n\n\nlf_r_o_pal\n16\n[1]\n0.010390\n0.010390\n0.005195\n3\n1\n0.750000\n\n\nlf_r_o_pica\n17\n[1]\n0.002597\n0.000000\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_pinto\n18\n[1]\n0.007792\n0.007792\n0.007792\n2\n1\n0.666667\n\n\nlf_r_o_piranha\n19\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_podre\n20\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_porra\n21\n[1]\n0.106494\n0.054545\n0.025974\n29\n12\n0.707317\n\n\nlf_r_o_pqp\n22\n[1]\n0.090909\n0.023377\n0.012987\n21\n14\n0.600000\n\n\nlf_r_o_put\n23\n[1]\n0.088312\n0.064935\n0.010390\n25\n9\n0.735294\n\n\nlf_r_o_putinh\n24\n[1]\n0.007792\n0.007792\n0.000000\n3\n0\n1.000000\n\n\nlf_r_o_ridicul\n25\n[1]\n0.005195\n0.005195\n0.005195\n2\n0\n1.000000\n\n\nlf_r_o_sapatao\n26\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_tnc\n27\n[1]\n0.015584\n0.010390\n0.002597\n4\n2\n0.666667\n\n\nlf_r_o_vaca\n28\n[1]\n0.002597\n0.002597\n0.002597\n1\n0\n1.000000\n\n\nlf_r_o_vagabund\n29\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_viado\n30\n[1]\n0.005195\n0.002597\n0.000000\n2\n0\n1.000000\n\n\nlf_r_e_vsf\n31\n[0]\n0.007792\n0.005195\n0.002597\n2\n1\n0.666667\n\n\nlf_r_o_vtnc\n32\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_hf_toxic\n33\n[0, 1]\n0.550649\n0.254545\n0.080519\n195\n17\n0.919811\n\n\n\n\n\n\n\nO transformer aumentou a cobertura de rotulação das instâncias de aproximadamente 54% para 86%, um aumento significativo de aproximadamente 32%. Porém, é importante perceber que o transformer atingiu um alto índice de conflito.\nAnalisando a nova Matriz de Conflito\n\nmatrix_graph(lfs, L_train)\n\n\n\n\n\n\n\n\nO índice de acertabilidade do transformer como visto na tabela foi extremamente alto, contudo, mesmo assim apresentou erros, e liderou o conflito com as outras funções de rotulagem, principalmente com a palavra cu que é a mais citada das palavras escolhidas para compor as LF’s.\n\n\nLabeling Functions de Modelo Treinado a partir de dados rotulados por as LF’s das Palavras e do Transformer\n\nA lógica que se segue é treinar um modelo de aprendizado de máquinas a partir das LF’s criadas anteriormente, e criar uma LF com ele.\n\nDefinindo Função para treinamento de diferentes modelos\n\nfrom typing import List, Tuple\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, accuracy_score, classification_report, matthews_corrcoef, confusion_matrix\nimport time\n\ndef train_and_evaluate_classification_models(X: pd.DataFrame, y: pd.Series) -&gt; Tuple[pd.DataFrame, List]:\n\n    \"\"\"\n    Train multiple classification models and evaluate their performance.\n\n    Args:\n        X (pd.DataFrame): The training data.\n        y (pd.Series): The training labels.\n\n    Returns:\n        Tuple[pd.DataFrame, List]: A DataFrame with the performance metrics of each model and a list of classification reports.\n    \"\"\"\n\n    random_state = 314\n    \n    # Define the models to be trained\n    models = []\n    models.append(('Calibrated-LSVC', CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'))))\n    models.append(('LR', LogisticRegression(random_state=314, n_jobs=-1, class_weight='balanced')))\n    models.append(('RF', RandomForestClassifier(random_state=314, n_jobs=-1, class_weight='balanced')))\n    models.append(('XGB', XGBClassifier(random_state=314, n_jobs=-1, class_weight='balanced', verbosity=0)))\n    models.append(('MLP', MLPClassifier(random_state=314)))\n    models.append(('NB', MultinomialNB()))\n    models.append(('LSVC', LinearSVC(random_state=314, class_weight='balanced')))\n    models.append(('KNN', KNeighborsClassifier(n_jobs=-1)))\n    models.append(('DT', DecisionTreeClassifier(random_state=314, class_weight='balanced')))\n    models.append(('ExtraTrees', ExtraTreesClassifier(random_state=314, n_jobs=-1, class_weight='balanced')))\n    \n    \n    performance_results = []\n    classification_reports = []\n    \n    cross_validation = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n\n    for model_name, model in models:\n        start_time = time.time()\n\n        try:\n            predicted_labels = cross_val_predict(estimator=model, X=X, y=y, cv=cross_validation,\n                                     method=\"predict\", n_jobs=2)\n        except Exception as e:\n            print(f'Error {model_name} - {e}')\n            continue \n\n        f1 = f1_score(y, predicted_labels, average='micro')\n        balanced_accuracy = balanced_accuracy_score(y, predicted_labels)\n        accuracy = accuracy_score(y, predicted_labels)\n        classification_report_str = classification_report(y, predicted_labels)\n        matthews_corr_coeff = matthews_corrcoef(y, predicted_labels)\n        confusion_mat = confusion_matrix(y, predicted_labels)\n        classification_reports.append([model_name, classification_report_str, confusion_mat])\n\n        elapsed_time = time.time() - start_time\n        performance_results.append([model_name, f1, balanced_accuracy, accuracy, matthews_corr_coeff, elapsed_time, confusion_mat, classification_report_str])\n\n        '''\n        print(f'Model: {model_name} - F1: {f1:.4f} - Balanced Accuracy: {balanced_accuracy:.4f} - Accuracy: {accuracy:.4f} - Matthews Correlation Coefficient: {matthews_corr_coeff:.4f} - Elapsed time: {elapsed_time:.2f}s')\n        print(classification_report_str)\n        print(confusion_mat)\n        print('*' * 20, '\\n')\n        '''\n\n    results_df = pd.DataFrame(performance_results, columns=['Model', 'F1', 'Balanced Accuracy', 'Accuracy', 'Matthews Correlation Coefficient', 'Elapsed Time', 'Confusion Matrix', 'Classification Report'])\n    results_df['Confusion Matrix'] = results_df['Confusion Matrix'].apply(lambda x: str(x))\n\n    return results_df, classification_reports\n\nDesempenho de diferentes modelos treinados a partir de Funções de Rotulagens que misturam a identificação de palavras com o Transformer\n\ntfidf_vec_ssl = TfidfVectorizer(ngram_range=(1,2), strip_accents='unicode', lowercase=True, max_features=2000, min_df=3)\n\nX_dev = tfidf_vec_ssl.fit_transform(df_develop.text)\nX_dev = X_dev.toarray()\n\ny_dev = df_develop.toxic.values\n\nprint('Shape:', X_dev.shape, '\\n')\n\ndf_results, creports = train_and_evaluate_classification_models(X_dev, y_dev) \n\ndf_results.sort_values(by='Matthews Correlation Coefficient', ascending=False)\n\nShape: (385, 374) \n\n\n\n\n\n\n\n\n\n\nModel\nF1\nBalanced Accuracy\nAccuracy\nMatthews Correlation Coefficient\nElapsed Time\nConfusion Matrix\nClassification Report\n\n\n\n\n6\nLSVC\n0.644156\n0.636821\n0.644156\n0.273009\n0.169900\n[[152 70]\\n [ 67 96]]\nprecision recall f1-score ...\n\n\n8\nDT\n0.631169\n0.621483\n0.631169\n0.243378\n0.252122\n[[152 70]\\n [ 72 91]]\nprecision recall f1-score ...\n\n\n3\nXGB\n0.633766\n0.615583\n0.633766\n0.237437\n1.407682\n[[163 59]\\n [ 82 81]]\nprecision recall f1-score ...\n\n\n2\nRF\n0.638961\n0.606229\n0.638961\n0.236414\n1.497457\n[[182 40]\\n [ 99 64]]\nprecision recall f1-score ...\n\n\n1\nLR\n0.623377\n0.613096\n0.623377\n0.226776\n0.240224\n[[151 71]\\n [ 74 89]]\nprecision recall f1-score ...\n\n\n5\nNB\n0.633766\n0.600094\n0.633766\n0.224138\n0.150079\n[[182 40]\\n [101 62]]\nprecision recall f1-score ...\n\n\n9\nExtraTrees\n0.618182\n0.594733\n0.618182\n0.198714\n1.247638\n[[166 56]\\n [ 91 72]]\nprecision recall f1-score ...\n\n\n4\nMLP\n0.584416\n0.567084\n0.584416\n0.136814\n5.690514\n[[151 71]\\n [ 89 74]]\nprecision recall f1-score ...\n\n\n0\nCalibrated-LSVC\n0.584416\n0.536920\n0.584416\n0.094075\n1.764345\n[[188 34]\\n [126 37]]\nprecision recall f1-score ...\n\n\n7\nKNN\n0.584416\n0.529583\n0.584416\n0.084848\n0.179999\n[[197 25]\\n [135 28]]\nprecision recall f1-score ...\n\n\n\n\n\n\n\nDesempenho de diferentes modelos treinados a partir de apenas o Transformer\n\n# Usando um modelo de transformador para gerar embeddings e depois usar um classificador em cima disso\n\nfrom sentence_transformers import SentenceTransformer\nfrom IPython.display import clear_output\n\nPATH_LM = 'inctdd/told_br_binary_sm'\nmodel = SentenceTransformer(PATH_LM)\n\nX_dev_transformer = model.encode(df_develop.text.values, batch_size=64, show_progress_bar=True, convert_to_tensor=False)\n\ndf_results, creports = train_and_evaluate_classification_models(X_dev_transformer, y_dev) \n\nclear_output(wait=True)\n\nprint(model.encode('teste').shape)\n\nprint(X_dev_transformer.shape, '\\n')\n\ndf_results.sort_values(by='Matthews Correlation Coefficient', ascending=False)\n\n(768,)\n(385, 768) \n\n\n\n\n\n\n\n\n\n\nModel\nF1\nBalanced Accuracy\nAccuracy\nMatthews Correlation Coefficient\nElapsed Time\nConfusion Matrix\nClassification Report\n\n\n\n\n8\nExtraTrees\n0.828571\n0.823633\n0.828571\n0.648361\n1.180001\n[[190 32]\\n [ 34 129]]\nprecision recall f1-score ...\n\n\n2\nRF\n0.815584\n0.809111\n0.815584\n0.620974\n1.955261\n[[189 33]\\n [ 38 125]]\nprecision recall f1-score ...\n\n\n3\nXGB\n0.807792\n0.801539\n0.807792\n0.605189\n10.979467\n[[187 35]\\n [ 39 124]]\nprecision recall f1-score ...\n\n\n6\nKNN\n0.800000\n0.796413\n0.800000\n0.591455\n0.175097\n[[182 40]\\n [ 37 126]]\nprecision recall f1-score ...\n\n\n4\nMLP\n0.750649\n0.745468\n0.750649\n0.490164\n5.801871\n[[173 49]\\n [ 47 116]]\nprecision recall f1-score ...\n\n\n1\nLR\n0.748052\n0.744031\n0.748052\n0.486251\n0.609632\n[[171 51]\\n [ 46 117]]\nprecision recall f1-score ...\n\n\n7\nDT\n0.740260\n0.731567\n0.740260\n0.465651\n1.544923\n[[175 47]\\n [ 53 110]]\nprecision recall f1-score ...\n\n\n0\nCalibrated-LSVC\n0.709091\n0.693127\n0.709091\n0.396131\n9.157617\n[[177 45]\\n [ 67 96]]\nprecision recall f1-score ...\n\n\n5\nLSVC\n0.688312\n0.680001\n0.688312\n0.360610\n3.773913\n[[163 59]\\n [ 61 102]]\nprecision recall f1-score ...\n\n\n\n\n\n\n\nOs modelos treinados apenas pelo transformer obtiveram melhor desempenho do que os treinados com as funções de rotulagem que uniram a identificação de palavras mais o transformer. Contudo, por mais que tenha tido um desempenho inferior, para o bem do trabalho continuaremos usando as funções de rotulagem, para gerar um resultado com a partir de formas mais variadas de rotulação.\nCriando Função de Rotulagem para melhor modelo obtido a partir do treinamento dos diversos modelos para o DataFrame de desenvolvimento\n\nmodel_ssl = CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'), cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=314))\nmodel_ssl = model_ssl.fit(X_dev, y_dev)\n\n# Define uma função de pré-processamento que adiciona um rótulo previsto e uma pontuação a um exemplo dado usando um classificador empilhado\n@preprocessor(memoize=True)\ndef train_model_predict(x):\n    \"\"\"\n    Esta função é um pré-processador que usa um modelo LinearSVC pré-treinado para adicionar\n    uma etiqueta de previsão e um score aos dados de entrada.\n\n    Args:\n    - x: Um objeto contendo os dados de entrada, que deve ter um atributo 'text' contendo o texto a ser classificado.\n\n    Retorna:\n    - O mesmo objeto de entrada 'x', mas com atributos adicionais 'pred_label' e 'score' adicionados com base nas previsões do modelo.\n    \"\"\"\n    # Transformar o texto usando o vetorizador TF-IDF\n    X_input = tfidf_vec_ssl.transform([x.text])\n    \n    # Obter as previsões do modelo\n    pred_label = model_ssl.predict(X_input)\n    pred_score = model_ssl.predict_proba(X_input).max(axis=1)\n    \n    # Adicionar a etiqueta prevista ('pred_label') e a pontuação ('score') aos atributos do objeto 'x'\n    x.pred_label = int(pred_label[0])\n    x.score = float(pred_score[0])\n    \n    return x\n\n@labeling_function(pre=[train_model_predict])\ndef lf_train_model(x):\n    \"\"\"\n    Esta função de rotulagem atribui um rótulo baseado nas previsões do modelo LinearSVC.\n\n    Args:\n    - x: Um objeto de entrada que deve ter atributos 'pred_label' e 'score' adicionados pelo pré-processador 'custom_model_predict'.\n\n    Retorna:\n    - (1) se a pontuação for maior ou igual a 0.65 e o rótulo previsto for '1'.\n    - (0) se a pontuação for maior ou igual a 0.65 e o rótulo previsto for '0'.\n    - (-1) caso contrário.\n    \"\"\"\n\n    if x.score &gt;= 0.7:\n        if x.pred_label == 1:\n            return 1\n        elif x.pred_label == 0:\n            return 0\n    else:\n        return -1\n\n# Remova qualquer função de rotulagem duplicada\nlfs.append(lf_train_model)\n\n\napplier = PandasLFApplier(lfs=lfs)\n\nL_train = applier.apply(df=df_train)\n\nLFAnalysis(L_train, lfs).lf_summary()\n\nL_dev = applier.apply(df=df_develop)\n\nprint(f'\\nTotal Coverge: {LFAnalysis(L=L_dev, lfs=lfs).label_coverage()}.')\n\nLFAnalysis(L=L_dev, lfs=lfs).lf_summary(Y=df_develop.toxic.values)\n\n100%|██████████| 16030/16030 [02:59&lt;00:00, 89.25it/s] \n100%|██████████| 385/385 [00:04&lt;00:00, 94.22it/s] \n\n\n\nTotal Coverge: 0.8805194805194805.\n\n\n\n\n\n\n\n\n\n\n\n\nj\nPolarity\nCoverage\nOverlaps\nConflicts\nCorrect\nIncorrect\nEmp. Acc.\n\n\n\n\nlf_r_e_amor\n0\n[0]\n0.031169\n0.031169\n0.018182\n9\n3\n0.750000\n\n\nlf_r_o_boiola\n1\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_e_bom dia\n2\n[0]\n0.005195\n0.005195\n0.002597\n2\n0\n1.000000\n\n\nlf_r_e_boa tarde\n3\n[0]\n0.005195\n0.005195\n0.002597\n1\n1\n0.500000\n\n\nlf_r_o_buceta\n4\n[1]\n0.010390\n0.010390\n0.000000\n4\n0\n1.000000\n\n\nlf_r_o_bunda\n5\n[1]\n0.007792\n0.007792\n0.000000\n3\n0\n1.000000\n\n\nlf_r_e_carai\n6\n[0]\n0.010390\n0.002597\n0.002597\n3\n1\n0.750000\n\n\nlf_r_o_caralho\n7\n[1]\n0.103896\n0.044156\n0.010390\n27\n13\n0.675000\n\n\nlf_r_o_cu\n8\n[1]\n0.083117\n0.072727\n0.028571\n21\n11\n0.656250\n\n\nlf_r_e_feliz\n9\n[0]\n0.018182\n0.018182\n0.005195\n5\n2\n0.714286\n\n\nlf_r_o_f[uo]de\n10\n[1]\n0.033766\n0.028571\n0.002597\n10\n3\n0.769231\n\n\nlf_r_e_incrivel\n11\n[0]\n0.005195\n0.002597\n0.002597\n1\n1\n0.500000\n\n\nlf_r_e_lind[oa]\n12\n[0]\n0.018182\n0.018182\n0.007792\n4\n3\n0.571429\n\n\nlf_r_e_maravilhos[oa]\n13\n[0]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_merd\n14\n[1]\n0.023377\n0.020779\n0.005195\n8\n1\n0.888889\n\n\nlf_r_e_mulherao\n15\n[0]\n0.002597\n0.002597\n0.002597\n1\n0\n1.000000\n\n\nlf_r_o_pal\n16\n[1]\n0.010390\n0.010390\n0.005195\n3\n1\n0.750000\n\n\nlf_r_o_pica\n17\n[1]\n0.002597\n0.000000\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_pinto\n18\n[1]\n0.007792\n0.007792\n0.007792\n2\n1\n0.666667\n\n\nlf_r_o_piranha\n19\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_podre\n20\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_porra\n21\n[1]\n0.106494\n0.054545\n0.025974\n29\n12\n0.707317\n\n\nlf_r_o_pqp\n22\n[1]\n0.090909\n0.025974\n0.012987\n21\n14\n0.600000\n\n\nlf_r_o_put\n23\n[1]\n0.088312\n0.064935\n0.010390\n25\n9\n0.735294\n\n\nlf_r_o_putinh\n24\n[1]\n0.007792\n0.007792\n0.000000\n3\n0\n1.000000\n\n\nlf_r_o_ridicul\n25\n[1]\n0.005195\n0.005195\n0.005195\n2\n0\n1.000000\n\n\nlf_r_o_sapatao\n26\n[1]\n0.002597\n0.002597\n0.000000\n1\n0\n1.000000\n\n\nlf_r_o_tnc\n27\n[1]\n0.015584\n0.010390\n0.002597\n4\n2\n0.666667\n\n\nlf_r_o_vaca\n28\n[1]\n0.002597\n0.002597\n0.002597\n1\n0\n1.000000\n\n\nlf_r_o_vagabund\n29\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_r_o_viado\n30\n[1]\n0.005195\n0.002597\n0.000000\n2\n0\n1.000000\n\n\nlf_r_e_vsf\n31\n[0]\n0.007792\n0.005195\n0.002597\n2\n1\n0.666667\n\n\nlf_r_o_vtnc\n32\n[1]\n0.005195\n0.005195\n0.000000\n2\n0\n1.000000\n\n\nlf_hf_toxic\n33\n[0, 1]\n0.550649\n0.324675\n0.080519\n195\n17\n0.919811\n\n\nlf_train_model\n34\n[0, 1]\n0.122078\n0.106494\n0.015584\n47\n0\n1.000000\n\n\n\n\n\n\n\nO modelo treinado aumentou a cobertura de rotulagem de aproximadamente 86% para 88%. Não é um aumentou substancial, porém se mostrou útil.\nRecapitulando conceitos a partir de modelo de rotulagem final (Palavras + Transformer + Modelo de Aprendizado de Máquinas) obtido:\n\nCobertura das Funções de Rotulagem:\n\nAs funções de rotulagem podem cobrir diferentes partes do conjunto de dados, ou seja, algumas funções podem rotular mais exemplos que outras. Isso é o que significa “cobertura”.\n\nSobreposição e Conflito:\n\nAs funções de rotulagem podem se sobrepor, o que significa que mais de uma função pode rotular o mesmo exemplo. Elas também podem entrar em conflito, o que significa que diferentes funções podem atribuir rótulos diferentes ao mesmo exemplo.\n\nPrecisões:\n\nCada função de rotulagem pode ter uma precisão diferente, ou seja, algumas podem ser mais precisas na atribuição de rótulos corretos do que outras.\n\nHistograma:\n\nPara entender melhor a cobertura total das funções de rotulagem, podemos visualizar um histograma que mostra quantos rótulos foram atribuídos pelos LFs aos pontos de dados no conjunto de treinamento.\nEsse histograma ajuda a dar uma ideia de quantos exemplos foram rotulados por nenhuma, uma ou várias funções de rotulagem, o que é essencial para avaliar a qualidade e a cobertura do processo de rotulagem automática.\n\n\n\n%matplotlib inline\n\ndef plot_label_distribution(label_matrix: np.ndarray) -&gt; None:\n    \"\"\"\n    Plota a distribuição do número de rótulos por exemplo no conjunto de dados.\n\n    Args:\n        label_matrix (np.ndarray): Um array 2D numpy onde cada linha representa um exemplo e cada coluna representa um rótulo.\n                                   O valor é -1 se o rótulo não for atribuído ao exemplo, e um valor diferente caso contrário.\n\n    Returns:\n        None\n    \"\"\"\n    # Calcular o número de rótulos para cada exemplo\n    num_labels_per_example = (label_matrix != -1).sum(axis=1)\n    \n    # Plotar um histograma do número de rótulos por exemplo\n    # plt.hist(num_labels_per_example, density=True, bins=range(label_matrix.shape[1])) # Para o caso de querer deixar o eixo x com o número total de rótulos\n    plt.hist(num_labels_per_example, density=True, bins=range(8)) # Delimitar manualmente o número de rótulos no eixo x\n\n    # Definir o rótulo do eixo x\n    plt.xlabel(\"Número de rótulos\")\n    \n    # Definir o rótulo do eixo y\n    plt.ylabel(\"Fração do conjunto de dados\")\n    \n    # Mostrar o gráfico\n    plt.show()\n\n# Plotar a frequência de rótulos no conjunto de treinamento\nplot_label_distribution(L_train)\n\n\n\n\n\n\n\n\nO gráfico mostra que a maioria das instâncias foram rotuladas por entre uma e duas funções de rotulagem. Isso se deve ao Transformer aliado ao modelo de AM que possuíram altos Overlaps, cobrindo assim as funções de rotulagens das palavras.\n\n\nComparando treinamento modelo de AM para DataFrame com Rótulos Reais, e Rótulos de Funções de Rotulagem\nOBS: RETIRANDO ABSTENÇÕES DO DF COM RÓTULOS ORIGINAIS E GERADOS\nAtribuindo Rótulos obtidos a partir de Funções das Rotulagem no DataFrame para Treino Final\n\nfrom snorkel.labeling.model import LabelModel\n\n# Criando cópia do DF de treino\ndf_train_final_total = df_train_final.copy()\n\n# Treinar um modelo de rótulo para combinar os rótulos das LFs\nlabel_model = LabelModel(cardinality=2, verbose=False)\nlabel_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=314)\n\n# Obter os rótulos finais\ndf_train_final_total['toxic_LF'] = label_model.predict(L=L_train)\n\n# Excluindo rótulos inconclusivos (-1)\ndf_train_final_total = df_train_final_total[df_train_final_total['toxic_LF'] != -1]\n\n# Analisando acurácia dos rótulos das LFs que foram rotuladas com sucesso (sem o valor de -1)\nprint(f'\\nF1      : {f1_score(df_train_final_total['toxic'], df_train_final_total['toxic_LF'], average='micro'):.4f}')\nprint(f'Accuracy: {accuracy_score(df_train_final_total['toxic'], df_train_final_total['toxic_LF']):.4f}\\n')\n\ndf_train_final_total.head()\n\n100%|██████████| 500/500 [00:01&lt;00:00, 361.28epoch/s]\n\n\n\nF1      : 0.7826\nAccuracy: 0.7826\n\n\n\n\n\n\n\n\n\n\ntext\ntoxic\ntoxic_LF\n\n\n\n\n2422\nrt user ai so porque tirou ele cai https co kr...\n0\n0\n\n\n15371\npior duvida\n0\n0\n\n\n7312\nuser giovana vai se fuder mano fica pffff\n1\n1\n\n\n1524\nharry caralho nao que tu bom mesmo https co nt...\n1\n1\n\n\n13716\nrt user eu ia falar contigo mas tu tava com ca...\n0\n0\n\n\n\n\n\n\n\nComo pode ser visto, os rótulos gerados a partir das LFs previstos com sucesso (diferentes de -1) obtiveram uma acurácia de 78.26% comparando diretamente com rotulos reais.\n\n# Ajustar o vetor TF-IDF no conjunto de treinamento\ntfidf_vec_ssl.fit(df_train_final_total.text)\n\n# Transformar o conjunto de treinamento e de teste com o vetor TF-IDF ajustado\nX_train = tfidf_vec_ssl.transform(df_train_final_total.text)\nX_train = X_train.toarray()\n\nX_test = tfidf_vec_ssl.transform(df_test.text)\nX_test = X_test.toarray()\n\n# Converter os rótulos para arrays\ny_train = df_train_final_total.toxic.values\ny_train_LF = df_train_final_total.toxic_LF.values\ny_test = df_test.toxic.values\n\nprint('Shape Train:', X_train.shape)\nprint('Shape Test:', X_test.shape)\n\nShape Train: (14105, 2000)\nShape Test: (385, 2000)\n\n\nApenas para fins de verificação de qual modelo treinado (o com rotulagens reais e o com rotulagens geradas) do DataFrame de treino melhor irá predizer dados novos, que serão os ainda intocados do DataFrame de teste. Para isso será utilizado o modelo de treinamento já usado anteriormente (LSVC). Ele foi escolhido visando diminuir o custo computacional do teste, dado as limitações da máquina utilizada.\n\n# Treinando modelo com rótulos reais\nmodel_ssl_train = CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'), cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=314))\nmodel_ssl_train = model_ssl_train.fit(X_train, y_train)\n\n# Treinando modelo com rótulos gerados\nmodel_ssl_train_LF = CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'), cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=314))\nmodel_ssl_train_LF = model_ssl_train_LF.fit(X_train, y_train_LF)\n\n\nf1_orig = f1_score(y_test, model_ssl_train.predict(X_test), average='micro')\nf1_gen = f1_score(y_test, model_ssl_train_LF.predict(X_test), average='micro')\n\nmcc_orig = matthews_corrcoef(y_test, model_ssl_train.predict(X_test))\nmcc_gen = matthews_corrcoef(y_test, model_ssl_train_LF.predict(X_test))\n\nacc_orig = accuracy_score(y_test, model_ssl_train.predict(X_test))\nacc_gen = accuracy_score(y_test, model_ssl_train_LF.predict(X_test))\n\nbac_orig = balanced_accuracy_score(y_test, model_ssl_train.predict(X_test))\nbac_gen = balanced_accuracy_score(y_test, model_ssl_train_LF.predict(X_test))\n\npd.DataFrame(data = {\n    'Métrica': ['F1S', 'MCC', 'ACC', 'BAC'],\n    'Rótulos Originais': [f1_orig, mcc_orig, acc_orig, bac_orig],\n    'Rótulos Gerados': [f1_gen, mcc_gen, acc_gen, bac_gen]\n})\n\n\n\n\n\n\n\n\nMétrica\nRótulos Originais\nRótulos Gerados\n\n\n\n\n0\nF1S\n0.750649\n0.766234\n\n\n1\nMCC\n0.483009\n0.522897\n\n\n2\nACC\n0.750649\n0.766234\n\n\n3\nBAC\n0.732424\n0.762242\n\n\n\n\n\n\n\nPor incrível que pareça, o dataframe com rótulos gerados desempenhou melhor que o com rótulos originais na predição do DataFrame de teste. É um resultado curioso, e um pouco assustador. Porém, é importante notar que as linhas com textos com os resultados inconclusivos (-1) para os rótulos gerados, foram retirados tanto deles como dos rótulos originais. Se faz necessário verificar se caso eles estivessem no treinamento do dataframe dos rótulos originais, ele acabaria gerando predições melhores que a dos rótulos gerados por causa da maior cobertura de casos.\n\n\nComparando treinamento modelo de AM para DataFrame com Rótulos Reais, e Rótulos de Funções de Rotulagem\nOBS: RETIRANDO ABSTENÇÕES APENAS DO DF COM GERADOS\n\nfrom snorkel.labeling.model import LabelModel\n\n# Criando cópia do DF de treino\ndf_train_final_com_abstencao = df_train_final.copy()\n\n# Criando DF de predição\ndf_train_final_sem_abstencao = pd.DataFrame()\ndf_train_final_sem_abstencao['text'] = df_train_final_com_abstencao['text']\n\n# Treinar um modelo de rótulo para combinar os rótulos das LFs\nlabel_model = LabelModel(cardinality=2, verbose=False)\nlabel_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=314)\n\n# Obter os rótulos finais\ndf_train_final_sem_abstencao['toxic'] = label_model.predict(L=L_train)\n\n# Excluindo rótulos inconclusivos (-1)\ndf_train_final_sem_abstencao = df_train_final_sem_abstencao[df_train_final_sem_abstencao['toxic'] != -1]\n\n# Observando diferença de quantidade de dados dos DFs com \n# rótulos originais e gerados\nprint('')\nprint(df_train_final_com_abstencao.info(), '\\n')\nprint(df_train_final_sem_abstencao.info())\n\n100%|██████████| 500/500 [00:00&lt;00:00, 598.64epoch/s]\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 16030 entries, 2422 to 235\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    16030 non-null  object\n 1   toxic   16030 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 375.7+ KB\nNone \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 14105 entries, 2422 to 12172\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    14105 non-null  object\n 1   toxic   14105 non-null  int32 \ndtypes: int32(1), object(1)\nmemory usage: 275.5+ KB\nNone\n\n\n\n# Ajustar o vetor TF-IDF no conjunto de treinamento\ntfidf_vec_ssl.fit(df_train_final_com_abstencao.text)\ntfidf_vec_ssl.fit(df_train_final_sem_abstencao.text)\n\n# Transformar o conjunto de treinamento e de teste com o vetor TF-IDF ajustado\nX_train_com_abstencao = tfidf_vec_ssl.transform(df_train_final_com_abstencao.text)\nX_train_com_abstencao = X_train_com_abstencao.toarray()\n\nX_train_sem_abstencao = tfidf_vec_ssl.transform(df_train_final_sem_abstencao.text)\nX_train_sem_abstencao = X_train_sem_abstencao.toarray()\n\nX_test = tfidf_vec_ssl.transform(df_test.text)\nX_test = X_test.toarray()\n\n# Converter os rótulos para arrays\ny_train_com_abstencao = df_train_final_com_abstencao.toxic.values\ny_train_sem_abstencao = df_train_final_sem_abstencao.toxic.values\ny_test = df_test.toxic.values\n\nprint('Shape de Treino com Abstenções:', X_train_com_abstencao.shape)\nprint('Shape de Treino sem Abstenções:', X_train_sem_abstencao.shape)\nprint('Shape de Teste:', X_test.shape)\n\nShape de Treino com Abstenções: (16030, 2000)\nShape de Treino sem Abstenções: (14105, 2000)\nShape de Teste: (385, 2000)\n\n\n\n# Treinando modelo com rótulos reais\nmodel_ssl_train_com_abstencao = CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'), cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=314))\nmodel_ssl_train_com_abstencao = model_ssl_train_com_abstencao.fit(X_train_com_abstencao, y_train_com_abstencao)\n\n# Treinando modelo com rótulos gerados\nmodel_ssl_train_sem_abstencao = CalibratedClassifierCV(LinearSVC(random_state=314, class_weight='balanced', dual='auto'), cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=314))\nmodel_ssl_train_sem_abstencao = model_ssl_train_sem_abstencao.fit(X_train_sem_abstencao, y_train_sem_abstencao)\n\n\nf1_orig = f1_score(y_test, model_ssl_train_com_abstencao.predict(X_test), average='micro')\nf1_gen = f1_score(y_test, model_ssl_train_sem_abstencao.predict(X_test), average='micro')\n\nmcc_orig = matthews_corrcoef(y_test, model_ssl_train_com_abstencao.predict(X_test))\nmcc_gen = matthews_corrcoef(y_test, model_ssl_train_sem_abstencao.predict(X_test))\n\nacc_orig = accuracy_score(y_test, model_ssl_train_com_abstencao.predict(X_test))\nacc_gen = accuracy_score(y_test, model_ssl_train_sem_abstencao.predict(X_test))\n\nbac_orig = balanced_accuracy_score(y_test, model_ssl_train_com_abstencao.predict(X_test))\nbac_gen = balanced_accuracy_score(y_test, model_ssl_train_sem_abstencao.predict(X_test))\n\npd.DataFrame(data = {\n    'Métrica': ['F1S', 'MCC', 'ACC', 'BAC'],\n    'Rótulos Originais': [f1_orig, mcc_orig, acc_orig, bac_orig],\n    'Rótulos Gerados': [f1_gen, mcc_gen, acc_gen, bac_gen]\n    })\n\n\n\n\n\n\n\n\nMétrica\nRótulos Originais\nRótulos Gerados\n\n\n\n\n0\nF1S\n0.745455\n0.766234\n\n\n1\nMCC\n0.471954\n0.522897\n\n\n2\nACC\n0.745455\n0.766234\n\n\n3\nBAC\n0.727104\n0.762242\n\n\n\n\n\n\n\nNovamente os rótulos originais obtiveram uma acurácia menor que a dos rótulos gerados. É curioso, mas essas são algumas possíveis explicações analisadas.\n\nDataFrame de Teste: O DataFrame de teste foi um recorte do dataset não usado durante todo o aprendizado de máquinas, porém, pode ser que esse recorte em particular seja melhor representado pelo modelo treinado com o DF com rótulos gerados, do que pelo modelo treinado com o DF com rótulos originais.\nExclusão de Dados Ambiguos: Ao extrair os comentários que as LFs não conseguiram rotular, retira-se com isso comentários que podem ser contraditórios para a máquina. Isso pode ser constatado ao notar que o DF com rótulos originais que tiveram as abstenções das LFs excluídas geraram um modelo melhor do que o DF que não excluiu. Aliado a outros fatores, isso pode ter agravado o motivo que levou a uma menor acurácia.\nInfluência do Transformer: Por mais que o transformer tenha apenas integrado parte das LF’s que realizaram a rotulagem, ele se trata de um modelo para o mesmo dataset do trabalho, logo, a influência dele junto de outros fatores pode ter sido preponderante para que o DataFrame com rótulos gerados tenha gerado predições melhores que o DataFrame com rótulos originais.\nModelo de Aprendizado de Máquinas: A primeira é que como o modelo treinado foi o mesmo usado para os dois, sem ajustes de acordo com cada um dos DF’s, talvez ele por acaso seja mais ajustado ao DF dos rótulos gerados, contudo, caso fosse ajustado um modelo para cada um dos dois DF’s, visando elevar de forma particular para cada suas respectivas acurácias, os rótulos originais obtivessem resultados melhores que os rótulos gerados.\nParticularidades do Dataset: O dataset usado visando extrair comentários tóxicos dos usuários do twitter por si só é rotulado de forma subjetiva. Isto porque comentários que usam palavras de baixo calão não necessariamente são tóxicos, e a interpretação pode variar bastante dependendo de quem o analisa. Logo, pode ser que os rótulos atribuídos pelas LB’s demonstrem menos subjetividade, e por mais que os dados usados para testar a acurácia possua rótulos originais não usados no treinamento, de alguma forma essa diminuição da subjetividade tenha ajudado no treinamento do modelo de AM.\nTamanho do Dataset: O tamanho do dataset também pode ter sido um fator preponderante. Isto por que talvez a quantidade de dados usados no treinamento tenham limitado mais a capacidade de predição do DF com rótulos originais, do que a do DF com rótulos gerados.\n\nDe qualquer forma, para este caso, com este Dataset, os rótulos gerados para o Dataset se mostrou mais eficaz que os rótulos originais no treinamento de modelos de aprendizado de máquinas."
  },
  {
    "objectID": "notebooks/studies/obitos_por_fa/obitos_por_fa.html",
    "href": "notebooks/studies/obitos_por_fa/obitos_por_fa.html",
    "title": "Introdução ao Caderno",
    "section": "",
    "text": "Esse caderno trata da predição de óbitos humanos por febre amarela no Brasil por aprendizado de máquinas supervisionado, a partir de uma base de dados pública disponibilizada pelo Mistério da Saúde do Brasil.",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.3 Óbitos por Febre Amarela no Brasil"
    ]
  },
  {
    "objectID": "notebooks/studies/obitos_por_fa/obitos_por_fa.html#k-means",
    "href": "notebooks/studies/obitos_por_fa/obitos_por_fa.html#k-means",
    "title": "Introdução ao Caderno",
    "section": "K-Means",
    "text": "K-Means\n\nfrom sklearn.cluster import KMeans\n\ndef c_kmeans(k, df):\n\n    km = KMeans(n_clusters=k,\n            init='k-means++',\n            max_iter=300,\n            n_init=10,\n            random_state=0)\n    \n    km.fit(df)\n\n    return km",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.3 Óbitos por Febre Amarela no Brasil"
    ]
  },
  {
    "objectID": "notebooks/studies/obitos_por_fa/obitos_por_fa.html#mes_is",
    "href": "notebooks/studies/obitos_por_fa/obitos_por_fa.html#mes_is",
    "title": "Introdução ao Caderno",
    "section": "MES_IS",
    "text": "MES_IS\nA coluna ‘MES_IS’ é numérica, e o único problema que ela apresenta são seus valores nulos.\n\nprint(f\"Existem {len(df['MES_IS'].value_counts())} variáveis diferentes e {df['MES_IS'].isnull().sum()} valores nulos.\")\n\nExistem 12 variáveis diferentes e 11 valores nulos.\n\n\n\n# Ocorrências de meses\nt_ocorrencia(df['MES_IS'])\n\n- A variável 1.0 representa 46.60% do total.\n- A variável 2.0 representa 22.35% do total.\n- A variável 3.0 representa 14.27% do total.\n- A variável 4.0 representa 6.63% do total.\n- A variável 12.0 representa 5.93% do total.\n- A variável 5.0 representa 1.78% do total.\n- A variável 6.0 representa 0.91% do total.\n- A variável 7.0 representa 0.40% do total.\n- A variável 11.0 representa 0.36% do total.\n- A variável 9.0 representa 0.33% do total.\n[...]\n\n\nO mês 1 aparece com uma frequência muito alta, logo, faz mais sentido atribuir os valores nulos a ele.\n\n# Transformando valores nulos em 1, que representa o mês de janeiro\ndf.loc[df['MES_IS'].isna(), 'MES_IS'] = 1.0\ndf_final.loc[df_final['MES_IS'].isna(), 'MES_IS'] = 1.0\n\nprint('Valores nulos:', sum(df['MES_IS'].isnull()))\n\nValores nulos: 0\n\n\n\n‘ANO_IS’ e ‘MONITORAMENTO_IS’\nAs colunas ‘ANO_IS’ e ‘MONITORAMENTO_IS’ representam objetivamente a mesma coisa, logo, por a coluna ‘ANO_IS’ já ser numérica, a coluna ‘MONITORAMENTO_IS’ será excluída.\n\ndf = df.drop('MONITORAMENTO_IS', axis=1)\ndf_final = df_final.drop('MONITORAMENTO_IS', axis=1)\n\ndf.head(3)\n\n\n\n\n\n\n\n\nUF_LPI\nMUN_LPI\nIDADE\nSE_IS\nMES_IS\nANO_IS\nOBITO\nMACRORREG_LPI_SE\nSEXO_M\n\n\n\n\n0\n0.147933\n0.002175\n42.61543\n48.0\n11.0\n1994\nSIM\n0\n1\n\n\n1\n0.147933\n0.000363\n19.00000\n8.0\n2.0\n1995\nNÃO\n0\n1\n\n\n2\n0.147933\n0.000725\n32.00000\n13.0\n4.0\n1995\nIGN\n0\n1\n\n\n\n\n\n\n\n\n\n‘OBITO’\nEsta é a coluna que se pretende prever, por isso, ela será dividida em duas, as que possuem uma resposta, e as que as respostas foram ignoradas. Por fim, as que possuíram respostas serão usadas para prever com base nas características das outras colunas se o indivíduo morreu ou não.\n\n# DataFrame para treino\ndf_treino = df[df['OBITO'] != 'IGN']\ndf_treino = coluna_letras_para_binario(df_treino, 'OBITO')\n\n# DataFrame que se pretende prever\ndf_predicao = df[df['OBITO'] == 'IGN']\ndf_predicao = coluna_letras_para_binario(df_predicao, 'OBITO')\n\n\n\nNormalizando colunas\nCom os dois DataFrames definidos, agora basta normalizar as colunas para que eles fiquem na mesma escala.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Inicializa o scaler\nscaler = MinMaxScaler()\n\n# Normalizando as colunas do DataFrame\nfor coluna in df_treino.columns:\n    df_treino[coluna] = scaler.fit_transform(df_treino[[coluna]])\n\n    if coluna != 'OBITO_SIM':\n        df_predicao[coluna] = scaler.fit_transform(df_predicao[[coluna]])",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.3 Óbitos por Febre Amarela no Brasil"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendendo com Dados e Máquinas",
    "section": "",
    "text": "Introdução\nO livro foi desenvolvido com o intuito de compilar bons algoritmos de tratamento de dados e aprendizado de máquinas. A teoria por trás dos algoritmos é introduzida de forma resumida e concisa, de forma que o maior infoque seja na sua implementação com conteúdo mais voltado para uso prático.\nA construção do livro se dá a partir de cadernos (notebooks) desenvolvidos com a linguagem de programação Python. As bibliotecas usadas sempre são especificadas nos cadernos, e cada um deles trata de um tema que pode ser lido de maneira independente. Contudo, recomenda-se para um maior entendimento a leitura linear do livro.\n\n\nCadernos\nPara acessar os cadernos de forma a baixá-los e executá-los, basta seguir seus respectivos caminhos do repositório.\n\nPré-processamento\n\n1.1 Separação de Dados para Treino, Verificação e Teste\n1.2 Detecção e Exclusão de Dados Duplicados\n1.3 Tratamento de Valores Ausentes com Algoritmos de Imputação\n1.4 Substituição de Outliers por Medidas Centrais\n1.5 Numeralização de Variáveis Categóricas\n1.6 Normalização de Dados Quantitativos\n1.7 Geração de Atributos Sintéticos (Em manutenção)\n1.8 Verificação de Normalidade dos Dados (Em manutenção)\n1.9 Detecção de Colinearidade e Multicolinearidade (Em manutenção)\n1.10 Seleção de Atributos com Métodos Baseados em Importância (Em manutenção)\n1.11 Análise de Componentes Principais (PCA) (Em manutenção)\n1.12 Uso de Auto Machine Learning para Pré-processamento (Em manutenção)\n\nAprendizado Supervisionado\n\n2.1 Validação Cruzada para Modelos Supervisionados (Em manutenção)\n2.2 Métricas para Modelos de Regressão e Classificação (Em manutenção)\n2.3 Técnicas de Balanceamento de Classes (Em manutenção)\n2.4 Treinamento de Modelos de Regressão (Em manutenção)\n2.5 Treinamento de Modelos de Classificação (Em manutenção)\n\nAprendizado Não Supervisionado\n\n3.1 Agrupamento de Dados Quantitativos\n3.2 Análises de Agrupamentos\n3.3 Algoritmos de Agrupamento Hierárquico (Em manutenção)\n3.4 Avaliação de Qualidade de Agrupamento (Em manutenção)\n3.5 Detecção de Anomalias em Clusters (Em manutenção)\n\nProcessamento de Linguagem Natural\n\n4.1 Limpeza de Texto\n4.2 Vetorização de Texto\n4.3 Frequência e Nuvem de Palavras (Em manutenção)\n4.4 Lematização e Stemming (Em manutenção)\n4.5 Funções de Rotulagem (Em manutenção)\n4.6 Criação de Modelos de Tópicos com LDA e NMF (Em manutenção)\n4.7 Classificação de Sentimentos (Em manutenção)\n\nVisão Computacional\n\n5.1 Manipulação e Visualização de Imagem (Em manutenção)\n5.2 Detecção de Bordas em Imagens (Em manutenção)\n5.3 Segmentação de Imagens (Em manutenção)\n5.4 Realce de Regiões de Interesse (Em manutenção)\n5.5 Detecção de Espectro de Cores (Em manutenção)\n5.6 Classificação de Objetos em Imagens (Em manutenção)\n\nEstudos de Caso e Aplicação Prática\n\n6.1 Titanic\n6.2 Dont Overfit II\n6.3 Óbitos por Febre Amarela no Brasil\n6.4 Digit Recognizer\n6.5 Toxic Tweets\n6.6 Detecção e Quantificação de Áreas Verdes\n\n\n\n\nSobre\nLivro desenvolvido por Alexandre Estrela de Lacerda Nobrega.\nPara mais informações sobre o autor acesse: https://alexandre11aa.github.io/.\nPara mais informações sobre o livro acesse: https://github.com/alexandre11aa/datas_and_machines.",
    "crumbs": [
      "Introdução"
    ]
  },
  {
    "objectID": "notebooks/studies/digit_recognizer/digit_recognizer.html",
    "href": "notebooks/studies/digit_recognizer/digit_recognizer.html",
    "title": "Aprendendo com Dados e Máquinas",
    "section": "",
    "text": "import pandas as pd\n\ndf_train = pd.read_csv('datas/dr_train_10pcent.csv')\ndf_test = pd.read_csv('datas/dr_test_10pcent.csv')\n\n#df_train = df_train.sample(frac=0.1, random_state=42)\n#df_test = df_test.sample(frac=0.1, random_state=42)\n\ndf_train\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4195\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4196\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4197\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4198\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4199\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n4200 rows × 785 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\ndef letras(data, labels):\n    fig = plt.figure(figsize = (10, 3))\n\n    rows, cols = 2, 8\n\n    import random\n\n    for i in range(1, rows*cols+1):\n        random_idx = random.randint(0, len(data)-1)\n\n        image = data[random_idx].reshape(28,28)\n\n        label = labels[random_idx]\n\n        ax = fig.add_subplot(rows, cols, i) # Adiciona o subplot\n        ax.imshow(image.squeeze(), cmap = \"gray\") # cmap = \"gray\" para gerar a imagem em escala de cinza\n        ax.set_title(label)\n        ax.axis(False) # Desliga os eixos\n\n    plt.tight_layout() # Ajusta o layout para evitar sobreposição\n    plt.show()\n\n\nimport numpy as np\n\ntrain_data = df_train.drop(columns=['label'], axis=1).to_numpy()\n\nlabels = df_train['label'].to_numpy()\n\nlabels_unique = np.unique(df_train['label'].to_numpy())\n\nprint(f'Dados de treino:\\n\\n{train_data}\\n\\nNúmeros:\\n\\n{labels}')\n\nDados de treino:\n\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\nNúmeros:\n\n[8 1 9 ... 7 7 5]\n\n\n\nletras(train_data, labels)\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Dados de treino\nX_train, X_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.2, random_state=42)\n\nimport tensorflow as tf\n\n# Remodelando os dados de entrada\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28, 1)\n\n\n\n\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Input\n\n# Construindo o modelo\nmodel = tf.keras.models.Sequential([\n\n    # Camada de entrada especificando a forma da imagem\n    Input(shape=(28, 28, 1)),\n    \n    # Camada de convolução com 32 filtros de tamanho 3x3 e ativação ReLU\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n\n    # Camada de pooling para reduzir a dimensionalidade da saída da convolução\n    tf.keras.layers.MaxPooling2D((2, 2)),\n\n    # Camada para transformar os dados em um vetor unidimensional para entrada na camada densa\n    tf.keras.layers.Flatten(),\n\n    # Camada densa (totalmente conectada) com 128 neurônios e ativação ReLU\n    tf.keras.layers.Dense(128, activation='relu'),\n\n    # Camada de saída com 10 neurônios (um para cada classe) e ativação softmax para classificação multiclasse\n    tf.keras.layers.Dense(10, activation='softmax')])\n\n# Compilando o modelo\nmodel.compile(\n    \n    # Otimizador Adam para ajustar os pesos da rede durante o treinamento\n    optimizer='adam',\n    \n    # Função de perda para calcular o erro durante o treinamento\n    loss='sparse_categorical_crossentropy',\n\n    # Métrica para avaliar a performance do modelo durante o treinamento\n    metrics=['accuracy'])\n\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                 │ (None, 26, 26, 32)     │           320 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (MaxPooling2D)    │ (None, 13, 13, 32)     │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 5408)           │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 128)            │       692,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 10)             │         1,290 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 693,962 (2.65 MB)\n\n\n\n Trainable params: 693,962 (2.65 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nfrom keras.callbacks import History\n\n# Criar um objeto \"Histórico\" para armazenar as métricas\nhistory = History()\n\n# Treinando o modelo\nmodel.fit(X_train, y_train, \n          epochs=10, \n          validation_data=(X_test, y_test), \n          callbacks=[history])\n\n# Avaliando o modelo\nloss, accuracy = model.evaluate(X_test, y_test)\n\nprint(f'\\nLoss: {loss}, Accuracy: {accuracy}')\n\nEpoch 1/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.6286 - loss: 11.4479 - val_accuracy: 0.8619 - val_loss: 0.5730\nEpoch 2/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 13ms/step - accuracy: 0.9029 - loss: 0.3746 - val_accuracy: 0.8679 - val_loss: 0.5470\nEpoch 3/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 13ms/step - accuracy: 0.9434 - loss: 0.1761 - val_accuracy: 0.9345 - val_loss: 0.3723\nEpoch 4/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 14ms/step - accuracy: 0.9786 - loss: 0.0630 - val_accuracy: 0.9310 - val_loss: 0.3328\nEpoch 5/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 12ms/step - accuracy: 0.9904 - loss: 0.0316 - val_accuracy: 0.9310 - val_loss: 0.3787\nEpoch 6/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step - accuracy: 0.9951 - loss: 0.0217 - val_accuracy: 0.9476 - val_loss: 0.3546\nEpoch 7/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step - accuracy: 0.9976 - loss: 0.0120 - val_accuracy: 0.9440 - val_loss: 0.3802\nEpoch 8/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.9985 - loss: 0.0083 - val_accuracy: 0.9524 - val_loss: 0.3485\nEpoch 9/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.9512 - val_loss: 0.3609\nEpoch 10/10\n105/105 ━━━━━━━━━━━━━━━━━━━━ 1s 12ms/step - accuracy: 0.9999 - loss: 6.7206e-04 - val_accuracy: 0.9536 - val_loss: 0.3580\n27/27 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.9509 - loss: 0.2762\n\nLoss: 0.3579649329185486, Accuracy: 0.9535714387893677\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Gráfico 1 - Acurácia em Treinamento\nax1.plot(range(1, len(history.history['accuracy']) + 1), \n         history.history['accuracy'], \n         '--&gt;', \n         label='accuracy')\nax1.plot(range(1, len(history.history['accuracy']) + 1), \n         history.history['val_accuracy'], \n         '--o', \n         label='val_accuracy')\nax1.set_title('Acurácia')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Accuracy')\nax1.legend(loc='lower right')\n\n# Gráfico 2 - Perda em Treinamento\nax2.plot(range(1, len(history.history['loss']) + 1), \n         history.history['loss'], \n         '--&gt;', \n         label='loss')\nax2.plot(range(1, len(history.history['loss']) + 1), \n         history.history['val_loss'], \n         '--o', \n         label='val_loss')\nax2.set_title('Erro')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Accuracy')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.4 Digit Recognizer"
    ]
  },
  {
    "objectID": "notebooks/studies/areas_verdes/final_project.html",
    "href": "notebooks/studies/areas_verdes/final_project.html",
    "title": "Identificação e Mensuração de Áreas Verdes",
    "section": "",
    "text": "Recorte de Imagens para Serem Rotuladas\n\nDe início, para rotular as imagens, implementa-se um algorítmo que detecte cores no espectro verde em imagens, de forma a identificar e mensurar as áreas verdes.\n\n\nimport os\nfrom PIL import Image\n\ndef fragment_and_save_image(image_path, rows, cols, output_dir):\n\n    # Listar arquivos na pasta\n    archives = os.listdir(image_path)\n\n    # Listar arquivos recortados\n    archive_list = []\n\n    # Abrir a pasta\n    for n, archive in enumerate(archives):\n\n        jpg = image_path + '/' + archive\n\n        # Abrir a imagem\n        image = Image.open(jpg)\n        width, height = image.size\n\n        # Calcular o tamanho de cada fragmento\n        fragment_width = width // cols\n        fragment_height = height // rows\n\n        # Criar o diretório de saída se não existir\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        for i in range(rows):\n            for j in range(cols):\n                left = j * fragment_width\n                upper = i * fragment_height\n                right = (j + 1) * fragment_width\n                lower = (i + 1) * fragment_height\n\n                # Definindo número do recorte:\n                n_r = os.listdir(f'{image_path}/')[n].split('.')[0]\n\n                # Recortar o fragmento\n                fragment = image.crop((left, upper, right, lower))\n\n                # Adicionar arquivo recortado à lista\n                archive_list.append(f'fragment_{n_r}_{i}_{j}.png')\n\n                # Salvar o fragmento como arquivo PNG\n                fragment_path = os.path.join(output_dir, f'fragment_{n_r}_{i}_{j}.png')\n                fragment.save(fragment_path)\n\n    return archive_list\n\n\nif len(os.listdir('labels')) &gt; 1000:\n    archives = os.listdir('labels')\n\nelse:\n    archives = fragment_and_save_image('features', 15, 15, 'labels')\n\n\n\nFunções para Visualização e Tratamento de Dados\n\nGráfico de Comparação\n\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef graph_1(resized_image, title_1,\n            green_mask, title_2,\n            green_areas, title_3,\n            size):\n\n    # Visualizar a imagem segmentada e as áreas verdes\n\n    plt.figure(figsize=size)\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n    plt.title(title_1, fontsize=8)\n    plt.axis('off')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(green_mask, cmap='gray')\n    plt.title(title_2, fontsize=8)\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(cv2.cvtColor(green_areas, cv2.COLOR_BGR2RGB))\n    plt.title(title_3, fontsize=8)\n    plt.axis('off')\n\n    plt.show()\n\n\n\nGráficos de Acurácia e Perda\n\ndef graph_2(history, metric):\n\n    # Plot das curvas de treinamento\n    plt.figure(figsize=(10, 4))\n\n    # Acurácia\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history[metric], label='Treinamento')\n    plt.plot(history.history['val_' + metric], label='Validação')\n    plt.xlabel('Épocas')\n    plt.ylabel('Acurácia')\n    plt.legend()\n\n    # Perda\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Treinamento')\n    plt.plot(history.history['val_loss'], label='Validação')\n    plt.xlabel('Épocas')\n    plt.ylabel('Perda')\n    plt.legend()\n\n    plt.show()\n\n\n\nGráfico de Confusão\n\n\nSeparação de Áreas Verdes\n\nimport numpy as np\n\ndef green_area(link):\n\n    # Carregar a imagem\n    image = cv2.imread(link)\n\n    # Redimensionar a imagem para aceleração do processamento\n    scale_percent = 50  # percent of original size\n    width = int(image.shape[1] * scale_percent / 100)\n    height = int(image.shape[0] * scale_percent / 100)\n    dim = (width, height)\n    resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n\n    # Converter a imagem para o espaço de cor HSV\n    hsv_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n\n    # Definir intervalo de cor verde (na escala HSV)\n    lower_green = np.array([30, 25, 25])\n    upper_green = np.array([100, 255, 255])\n\n    # Criar máscara para os pixels verdes\n    green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n\n    # Aplicar a máscara na imagem original\n    green_areas = cv2.bitwise_and(resized_image, resized_image, mask=green_mask)\n\n    return resized_image, green_mask, green_areas\n\n\n\nPorcentagem das Áreas Verdes\n\ndef perc_green_area(green_areas):\n\n    # Converter a imagem para RGB (apenas para visualização no matplotlib)\n    image_rgb = cv2.cvtColor(green_areas, cv2.COLOR_BGR2RGB)\n\n    # Converter a imagem para escala de cinza\n    gray_image = cv2.cvtColor(green_areas, cv2.COLOR_BGR2GRAY)\n\n    # Aplicar um limiar para detectar pixels pretos\n    # Pixels pretos serão aqueles com valor 0 no limiar binário\n    _, black_and_white_image = cv2.threshold(gray_image, 1, 255, cv2.THRESH_BINARY_INV)\n\n    # Contar o número de pixels pretos\n    num_black_pixels = np.sum(black_and_white_image == 255)\n    total_pixels = black_and_white_image.size\n\n    # Calcular a porcentagem de pixels pretos\n    percentage_black = (num_black_pixels / total_pixels) * 100\n\n    return image_rgb, gray_image, black_and_white_image, percentage_black\n\n\n\nTeste das Funções\n\nfolder = 'labels/'\n\nir, gm, ga = green_area(f'{folder}fragment_areas_mescladas_urbana_5_6_4.png')\n\ngraph_1(ir, 'Imagem Original',\n        gm, 'Máscara das Áreas Verdes',\n        ga, 'Áreas Verdes',\n        (8, 5))\n\n\n\n\n\n\n\n\n\nic, gi, bwi, pb = perc_green_area(ga)\n\nprint(f'\\nO percentual de áreas verdes é de {100 - pb:.2f}%\\n')\n\ngraph_1(ic,  'Imagem Original Recortada',\n        gi,  'Imagem em Escala de Cinza',\n        bwi, 'Imagem Preto e Branco',\n        (8, 5))\n\n\nO percentual de áreas verdes é de 9.55%\n\n\n\n\n\n\n\n\n\n\n\n\n\nRotulando Imagens\n\nAplicando funções criadas em rotulação de imagens, a partir de áreas verdes e suas mensurações.\n\n\nimport pandas as pd\n\ndata = []\n\nfor n, arch in enumerate(archives):\n\n    ir, _, ga = green_area(f'{folder}{arch}')\n\n    ic, _, _, pb = perc_green_area(ga)\n\n    data.append({\n        'Arquivo': arch,\n        'Área total': ir,\n        'Área verde': ic,\n        'Percentual de verde': (100 - pb)\n    })\n\n# Criar o DataFrame\ndf = pd.DataFrame(data)\n\n# Exportando CSV do DataFrame\ndf.to_csv('test.csv', index=False)\n\n# Visualização de DataFrame\ndf.head()\n\n\n\n\n\n\n\n\nArquivo\nÁrea total\nÁrea verde\nPercentual de verde\n\n\n\n\n0\nfragment_areas_mescladas_rurais_1_0_0.png\n[[[168, 165, 164], [181, 173, 180], [172, 165,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n0.129032\n\n\n1\nfragment_areas_mescladas_rurais_1_0_1.png\n[[[74, 76, 87], [111, 119, 127], [79, 83, 95],...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n0.967742\n\n\n2\nfragment_areas_mescladas_rurais_1_0_10.png\n[[[99, 87, 99], [129, 124, 134], [123, 116, 12...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n0.000000\n\n\n3\nfragment_areas_mescladas_rurais_1_0_11.png\n[[[91, 80, 87], [35, 28, 34], [35, 28, 35], [6...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n0.000000\n\n\n4\nfragment_areas_mescladas_rurais_1_0_12.png\n[[[130, 132, 141], [129, 134, 141], [126, 126,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n0.258065\n\n\n\n\n\n\n\n\n\nRealizando predição de percentual de áreas verdes\n\nConfigurando dados\n\n# Definindo DataFrame para predizer porcentagens\ndf_per = df[['Área total', 'Percentual de verde']]\n\n# Embaralhando linhas\ndf_per = df_per.sample(frac=1, random_state=7).reset_index(drop=True)\n\n# Separando dados de treino\ndf_train = df_per[:5000]\n\n# Separando dados de teste\ndf_test = df_per[5000:5500]\n\n# Visualizando dados de treino\ndf_train.head()\n\n\n\n\n\n\n\n\nÁrea total\nPercentual de verde\n\n\n\n\n0\n[[[152, 158, 164], [130, 135, 140], [130, 136,...\n9.548387\n\n\n1\n[[[85, 71, 30], [86, 73, 32], [85, 71, 30], [8...\n86.645161\n\n\n2\n[[[140, 137, 171], [154, 154, 188], [121, 123,...\n34.451613\n\n\n3\n[[[146, 152, 163], [171, 172, 182], [160, 152,...\n1.612903\n\n\n4\n[[[191, 190, 200], [201, 199, 208], [210, 204,...\n0.967742\n\n\n\n\n\n\n\n\n# Definir a dimensão desejada para as imagens\ndesired_size = (128, 128)  # Exemplo: 128x128 pixels\n\ndef resize_image(image_array, desired_size):\n    image = Image.fromarray(np.uint8(image_array))\n    image = image.resize(desired_size, Image.LANCZOS)\n    return np.array(image)\n\n# Extrair e redimensionar imagens\nimages = np.array([resize_image(np.array(img), desired_size) for img in df_train['Área total']])\nlabels = np.array(df_train['Percentual de verde'].tolist())\n\n# Normalização das imagens (valores de pixels entre 0 e 1)\nimages = images / 255.0\n\n# Normalização dos rótulos (escala entre 0 e 1)\nlabels = labels / 100.0\n\n\n\nArquitetura do modelo e treinamento dos dados\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom sklearn.model_selection import train_test_split\n\n# Dividir os dados em conjuntos de treinamento e validação\nX_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n\n# Construção do modelo CNN\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(desired_size[0], desired_size[1], 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(1)  # Saída para regressão\n])\n\n# Compilação do modelo\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n\n# Treinamento do modelo\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32, verbose=1)\n\n# Avaliação do modelo\nloss = model.evaluate(X_val, y_val)\nprint(f'\\nValidation Loss: {loss}')\n\nc:\\Users\\alexa\\Desktop\\Ícones\\Estudos\\UFRN\\Mestrado\\Códigos\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nEpoch 1/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 49s 344ms/step - loss: 0.8201 - mae: 0.4130 - val_loss: 0.0114 - val_mae: 0.0761\nEpoch 2/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 42s 338ms/step - loss: 0.0083 - mae: 0.0611 - val_loss: 0.0029 - val_mae: 0.0333\nEpoch 3/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 42s 336ms/step - loss: 0.0032 - mae: 0.0364 - val_loss: 0.0037 - val_mae: 0.0336\nEpoch 4/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 42s 335ms/step - loss: 0.0043 - mae: 0.0414 - val_loss: 0.0010 - val_mae: 0.0209\nEpoch 5/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 43s 341ms/step - loss: 0.0014 - mae: 0.0240 - val_loss: 0.0012 - val_mae: 0.0209\nEpoch 6/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 41s 326ms/step - loss: 0.0035 - mae: 0.0354 - val_loss: 0.0024 - val_mae: 0.0313\nEpoch 7/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 40s 318ms/step - loss: 0.0047 - mae: 0.0436 - val_loss: 0.0038 - val_mae: 0.0394\nEpoch 8/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 41s 324ms/step - loss: 0.0019 - mae: 0.0273 - val_loss: 7.9409e-04 - val_mae: 0.0169\nEpoch 9/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 40s 318ms/step - loss: 0.0020 - mae: 0.0256 - val_loss: 0.0039 - val_mae: 0.0395\nEpoch 10/10\n125/125 ━━━━━━━━━━━━━━━━━━━━ 41s 331ms/step - loss: 0.0018 - mae: 0.0261 - val_loss: 0.0020 - val_mae: 0.0316\n32/32 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 0.0020 - mae: 0.0321\n\nValidation Loss: [0.002033500000834465, 0.031632546335458755]\n\n\n\n\nPredição dos dados de teste\n\n# Extraindo imagens teste e fazendo normalização\nimages_test = np.array([resize_image(np.array(img), desired_size) for img in df_test['Área total']])\nimages_test = images_test / 255.0\n\n# Extraindo percentuais teste\nlabels_test = np.array(df_test['Percentual de verde'].tolist())\n\n# Predizendo e revertendo a normalização das porcentagens\npredictions = model.predict(images_test)\npredictions = predictions * 100\n\n16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\n\n\n\n\nAvaliação de acurácia do modelo\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Calcular o erro absoluto médio (Mean Absolute Error - MAE)\nmae = mean_absolute_error(labels_test, predictions)\nprint(f'MAE : {mae:.4f}')\n\n# Calcular o erro quadrático médio (Mean Squared Error - MSE)\nmse = mean_squared_error(labels_test, predictions)\nprint(f'MSE : {mse:.4f}')\n\n# Calcular a raiz do erro quadrático médio (Root Mean Squared Error - RMSE)\nrmse = np.sqrt(mse)\nprint(f'RMSE: {rmse:.4f}')\n\n# Calcular o coeficiente de determinação (R²)\nr2 = r2_score(labels_test, predictions)\nprint(f'R²  : {r2:.4f}\\n')\n\n# Gráfico de treinamento\ngraph_2(history, 'mae')\n\nMAE : 3.0945\nMSE : 19.5039\nRMSE: 4.4163\nR²  : 0.9864\n\n\n\n\n\n\n\n\n\n\n\n\nResultado final do percentual real e predito\n\n# Comparando com o DataSet\npd.DataFrame({'Real': labels_test,\n              'Predição': predictions.flatten()}).head()\n\n\n\n\n\n\n\n\nReal\nPredição\n\n\n\n\n0\n0.000000\n-0.461279\n\n\n1\n100.000000\n97.196976\n\n\n2\n26.903226\n15.845325\n\n\n3\n51.548387\n44.579552\n\n\n4\n0.000000\n-0.436076\n\n\n\n\n\n\n\n\n\n\nRealizando predição de áreas verdes\n\nConfigurando dados\n\n# Definindo DataFrame para predizer porcentagens\ndf_av = df[['Área total', 'Área verde']]\n\n# Definir a cor verde específica (RGB)\ngreen_color = [0, 255, 0]\n\n# Função para alterar os pixels não pretos para a cor verde\ndef apply_green_color(image_array, green_color):\n\n    # Criar uma máscara onde os pixels não são pretos (0, 0, 0)\n    mask = (image_array != [0, 0, 0]).any(axis=-1)\n\n    # Aplicar a cor verde aos pixels que não são pretos\n    image_array[mask] = green_color\n\n    return image_array\n\n# Aplicar a função a cada imagem na coluna \"Área verde\". A função para alterar os pixels\n# não pretos de forma que se tornem um único tipo de verde servirá para ajudar ao modelo\n# na predição dos locais considerados áreas verdes, visto que ele só precisará destingui\n# r entre duas cores, e não todo o espectro de cores que engloba as áreas verdes.\ndf_av.loc[:, 'Área verde'] = df_av['Área verde'].apply(lambda x: apply_green_color(np.array(x), green_color))\n\n\n# Embaralhando linhas\ndf_av = df_av.sample(frac=1, random_state=7).reset_index(drop=True)\n\n# Separando dados de treino\ndf_train_av = df_av[:2000]\n\n# Separando dados de teste\ndf_test_av = df_av[2000:2500]\n\n# Visualizando dados de treino\ndf_train_av.head()\n\n\n\n\n\n\n\n\nÁrea total\nÁrea verde\n\n\n\n\n0\n[[[152, 158, 164], [130, 135, 140], [130, 136,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n\n\n1\n[[[85, 71, 30], [86, 73, 32], [85, 71, 30], [8...\n[[[0, 255, 0], [0, 255, 0], [0, 255, 0], [0, 2...\n\n\n2\n[[[140, 137, 171], [154, 154, 188], [121, 123,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n\n\n3\n[[[146, 152, 163], [171, 172, 182], [160, 152,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n\n\n4\n[[[191, 190, 200], [201, 199, 208], [210, 204,...\n[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...\n\n\n\n\n\n\n\n\n\nArquitetura do modelo, treinamento e predição dos dados\n\n# Extrair e redimensionar imagens\nimages_train = np.array([resize_image(np.array(img), desired_size) for img in df_train_av['Área total']])\nlabels_train = np.array([resize_image(np.array(img), desired_size) for img in df_train_av['Área verde']])\n\nimages_test = np.array([resize_image(np.array(img), desired_size) for img in df_test_av['Área total']])\nlabels_test = np.array([resize_image(np.array(img), desired_size) for img in df_test_av['Área verde']])\n\n# Normalização das imagens (valores de pixels entre 0 e 1)\nimages_train = images_train / 255.0\nlabels_train = labels_train / 255.0\n\nimages_test = images_test / 255.0\nlabels_test = labels_test / 255.0\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense\nfrom sklearn.model_selection import train_test_split\n\n# Dividir os dados em conjuntos de treinamento e validação\nX_train, X_val, y_train, y_val = train_test_split(images_train, labels_train, test_size=0.2, random_state=42)\n\n# Construção do modelo CNN\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n    MaxPooling2D((2, 2), padding='same'),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2), padding='same'),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2), padding='same'),\n    UpSampling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    UpSampling2D((2, 2)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    UpSampling2D((2, 2)),\n    Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n])\n\n# Compilação do modelo\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae'])\n\n# Treinamento do modelo\nhistory = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), batch_size=128, shuffle=True, verbose=1)\n\n# Fazer previsões com o modelo\npredictions = model.predict(images_test)\n\nc:\\Users\\alexa\\Desktop\\Ícones\\Estudos\\UFRN\\Mestrado\\Códigos\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nEpoch 1/5\n13/13 ━━━━━━━━━━━━━━━━━━━━ 36s 2s/step - loss: 0.5088 - mae: 0.3634 - val_loss: 0.1955 - val_mae: 0.1262\nEpoch 2/5\n13/13 ━━━━━━━━━━━━━━━━━━━━ 34s 3s/step - loss: 0.2095 - mae: 0.1329 - val_loss: 0.1926 - val_mae: 0.1140\nEpoch 3/5\n13/13 ━━━━━━━━━━━━━━━━━━━━ 34s 3s/step - loss: 0.1793 - mae: 0.1156 - val_loss: 0.1415 - val_mae: 0.0983\nEpoch 4/5\n13/13 ━━━━━━━━━━━━━━━━━━━━ 34s 3s/step - loss: 0.1256 - mae: 0.0828 - val_loss: 0.1065 - val_mae: 0.0561\nEpoch 5/5\n13/13 ━━━━━━━━━━━━━━━━━━━━ 32s 2s/step - loss: 0.0881 - mae: 0.0501 - val_loss: 0.0767 - val_mae: 0.0416\n16/16 ━━━━━━━━━━━━━━━━━━━━ 3s 172ms/step\n\n\n\n\nAvaliação de acurácia do modelo\n\n# Gráfico de treinamento\ngraph_2(history, 'mae')\n\n\n\n\n\n\n\n\n\nfor i in range(30, 40):\n        # Gráfico para comparação\n        graph_1((images_test[i] * 255).astype(np.uint8), 'Área total',\n                (labels_test[i] * 255).astype(np.uint8), 'Área verde',\n                (predictions[i] * 255).astype(np.uint8), 'Área verde predita',\n                (8, 5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsiderações Finais\n\nOs dois modelos usados para treinar são parecidos, porém a principal diferença é no formato da saída, onde o modelo de percentual de áreas verdes produz números, e o modelo de predição produz imagens. Os resultados se mostraram promissores, contudo sofrem de underfitting, devido a pouca quantidade de imagens e de épocas para treinamento. O modelo de predição de imagens em especial possui alguns outros problemas relacionados à rotulação, e podem ser contornados com as mudanças nos rótulos descritas a seguir.\n\nÁreas totalmente verdes de florestas:\n\nImagens com 100% de cobertura verde.\n\nÁreas totalmente nao verdes de rios e mares:\n\nImagens com 0% de cobertura verde.\n\nÁreas totalmente nao verdes urbanas:\n\nImagens com 0% de cobertura verde.\n\nÁreas mescladas urbanas:\n\nImagens com x% de cobertura verde.\n\nÁreas mescladas rurais:\n\nImagens com x% de cobertura verde.",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.6 Detecção e Quantificação de Áreas Verdes"
    ]
  },
  {
    "objectID": "notebooks/studies/titanic/titanic.html",
    "href": "notebooks/studies/titanic/titanic.html",
    "title": "Configuração de dados",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntest = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')\ngender_submission = pd.read_csv('gender_submission.csv')\n\n\n# Função para promover agilidade em analises.\n\ndef analise(coluna):\n    \n    # Transformando lista de constantes únicas e suas quantidades em string.\n    \n    constantes_unicas = ''\n    \n    for i in range(len(list(coluna.value_counts()))):          \n        if   (i == 0):\n            constantes_unicas += str(list(coluna.value_counts())[i]) + ' \"' + str(list(coluna.value_counts().index)[i]) + '\",'\n            \n        elif (i == len(coluna.unique()) - 1):\n            constantes_unicas += ' ' + str(list(coluna.value_counts())[i]) + ' \"' + str(list(coluna.value_counts().index)[i]) + '\"'\n            \n        else:\n            constantes_unicas += ' ' + str(list(coluna.value_counts())[i]) + ' \"' + str(list(coluna.value_counts().index)[i]) + '\",'\n    \n    # Análise dos dados\n    \n    print(f'Existem {coluna.nunique()} constantes únicas que são {constantes_unicas} e {coluna.isna().sum()} dados faltantes.')\n\n\n# Fazendo diferenciação e união de planilhas\n\ntest['Modify'] = 'test'\ntrain['Modify'] = 'train'\n\ndf_final = pd.DataFrame()\n\ndf_create = pd.concat([test, train])\n\ndf_create.head(5)\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nModify\nSurvived\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\ntest\nNaN\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\ntest\nNaN\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\ntest\nNaN\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\ntest\nNaN\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\ntest\nNaN\n\n\n\n\n\n\n\n\nPassengerld\nA coluna “Passengerld” apenas escala de forma crescente, com o intervalo de 1 unidade. Logo, fica claro que a lógica dela não têm nenhuma interferência no comportamento de outras colunas, e conseguentemente também não influenciará na probabilidade do passageiro sobreviver ou não.\n\n\nPclass\n\nanalise(df_create['Pclass'])\n\nExistem 3 constantes únicas que são 709 \"3\", 323 \"1\", 277 \"2\" e 0 dados faltantes.\n\n\n\n'''\nAnalisando a coluna, é nítido e notório que ela não precisa de nenhum tratamento estatístico \nde dados visto que existem poucas variáveis, e não nenhum dado faltantes.\n'''\n\n# Transformação das variáveis de \"Pclass\" em multiplas colunas que representam de      forma \n# binária os dados de cada uma delas.\n\nPclass = pd.get_dummies(df_create['Pclass'], prefix='Pclass')\n\ndf_final = Pclass\n\ndf_final.head(5)\n\n\n\n\n\n\n\n\nPclass_1\nPclass_2\nPclass_3\n\n\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n2\n0\n1\n0\n\n\n3\n0\n0\n1\n\n\n4\n0\n0\n1\n\n\n\n\n\n\n\n\n\nName\n\n'''\nCada passageiro possui um nome diferente, logo, não faz sentido fazer uma análise geral   da \ncoluna da forma que está. Porém, ao observar o nome dos passageiros nota-se que cada      um \npossui um pronome de tratamento, e estes tendem a se repetir. Então, faz mais        sentido \nclassificar o nome dos passageiros com base neles, e aí sim fazer qualquer análise.\n'''\n\n# Separando pronomes de tratamento e transformando em coluna\n\nName = []\n\nfor i in df_create['Name']:\n    Name.append(i.split(', ')[1].split('.')[0])\n    \ndf_create['Pronoun'] = Name\n\nanalise(df_create['Pronoun'])\n\nExistem 18 constantes únicas que são 757 \"Mr\", 260 \"Miss\", 197 \"Mrs\", 61 \"Master\", 8 \"Rev\", 8 \"Dr\", 4 \"Col\", 2 \"Ms\", 2 \"Major\", 2 \"Mlle\", 1 \"Sir\", 1 \"the Countess\", 1 \"Capt\", 1 \"Don\", 1 \"Lady\", 1 \"Mme\", 1 \"Dona\", 1 \"Jonkheer\" e 0 dados faltantes.\n\n\n\n'''\nAnalisando os números, quatro pronomes de tratamento possuem a esmagadora maioria         de\nparticipação nos dados em comparação aos restantes. Essa afirmação pode ser           melhor\nvisualizada em um gráfico de barras.\n'''\n# Plotando gráfico de barras de pronomes de tratamento\n\ncolunas_de_pronomes = sns.set(font_scale=1)\ncolunas_de_pronomes = sns.countplot(data=df_create, x='Pronoun')\ncolunas_de_pronomes.set_xticklabels(colunas_de_pronomes.get_xticklabels(), rotation=90)\n\nfor i in colunas_de_pronomes.patches:\n    colunas_de_pronomes.annotate(f'{i.get_height()}', \n                     (i.get_x() + i.get_width() / 2., i.get_height()), \n                     ha='center', va='center', \n                     fontsize=8, color='black', xytext=(0, 5),\n                     textcoords='offset points')\n\ncolunas_de_pronomes\n\n\n\n\n\n\n\n\n\n'''\nA depender do comportamento desses títulos que possuem aparições menores ou igual a    oito,\nfaz mais sentido os unir em apenas uma categória, algo como \"outros\".\n'''\n\nquantidades_de_p = df_create['Pronoun'].value_counts()\n\ndf_create[df_create['Pronoun'].isin(quantidades_de_p[quantidades_de_p &lt;= 8].index)][['Pronoun', 'Survived']].value_counts(dropna=False)\n\nPronoun       Survived\nRev           0.0         6\nDr            0.0         4\n              1.0         3\nCol           NaN         2\nRev           NaN         2\nMlle          1.0         2\nCapt          0.0         1\nMajor         1.0         1\nSir           1.0         1\nMs            NaN         1\n              1.0         1\nMme           1.0         1\nLady          1.0         1\nMajor         0.0         1\nCol           0.0         1\nJonkheer      0.0         1\nDr            NaN         1\nDona          NaN         1\nDon           0.0         1\nCol           1.0         1\nthe Countess  1.0         1\ndtype: int64\n\n\n\n'''\nAnalisando os dados, percebe-se que existem pronomes de tratamento citados que        apenas \nmorreram, apenas viveram, e existem aqueles que as quantidades são divididas. Seria     mais \nlógico então resumir esses pronomes em apenas três, os com a taxa de mortalidade alta, baixa\ne dividida.\n'''\n\n# Separando manualmente os pronomes de tratamento\n\ndf_create['Pronoun'] = df_create['Pronoun'].replace(['Capt', 'Don', 'Jonkheer', 'Rev'], 'Pronoun_0')\ndf_create['Pronoun'] = df_create['Pronoun'].replace(['Lady', 'Mme', 'Mlle', 'Sir', 'the Countess'], 'Pronoun_1')\ndf_create['Pronoun'] = df_create['Pronoun'].replace(['Col', 'Dona', 'Dr', 'Major', 'Ms'], 'Pronoun_0_1')\n\n# Nova análise de nomes\n\nanalise(df_create['Pronoun'])\n\nExistem 7 constantes únicas que são 757 \"Mr\", 260 \"Miss\", 197 \"Mrs\", 61 \"Master\", 17 \"Pronoun_0_1\", 11 \"Pronoun_0\", 6 \"Pronoun_1\" e 0 dados faltantes.\n\n\n\n# Transformação das variáveis de \"Pronoun\" em multiplas colunas que representam de forma binária os dados\n# da coluna \"Name\".\n\nb_Name = pd.get_dummies(df_create['Pronoun'], prefix='Name')\n\ndf_final = pd.concat([df_final, b_Name], axis=1)\n\n\n\nSex\n\nanalise(df_create['Sex'])\n\nExistem 2 constantes únicas que são 843 \"male\", 466 \"female\" e 0 dados faltantes.\n\n\n\n'''\nNota-se que existem apenas duas constantes sem nenhum dado faltante, logo, fica claro    que\nnão há necessidade de nenhum tratamento de dados, a não ser transforma-los em binário.\n'''\n\n# Transformando constante da coluna \"Sex\" em binário\n\ndf_create['Sex'] = df_create['Sex'].replace(['male', 'female'], [0, 1])\n\ndf_final = pd.concat([df_final, df_create['Sex']], axis=1)\n\n\n\nAge\n\nanalise(df_create['Age'])\n\nExistem 98 constantes únicas que são 47 \"24.0\", 43 \"22.0\", 41 \"21.0\", 40 \"30.0\", 39 \"18.0\", 34 \"25.0\", 32 \"28.0\", 31 \"36.0\", 30 \"26.0\", 30 \"29.0\", 30 \"27.0\", 29 \"19.0\", 26 \"23.0\", 24 \"32.0\", 23 \"35.0\", 23 \"20.0\", 23 \"31.0\", 21 \"33.0\", 21 \"45.0\", 20 \"17.0\", 20 \"39.0\", 19 \"16.0\", 18 \"42.0\", 18 \"40.0\", 16 \"34.0\", 15 \"50.0\", 14 \"38.0\", 14 \"48.0\", 14 \"47.0\", 12 \"2.0\", 11 \"41.0\", 10 \"1.0\", 10 \"9.0\", 10 \"44.0\", 10 \"4.0\", 10 \"54.0\", 9 \"37.0\", 9 \"43.0\", 9 \"49.0\", 8 \"55.0\", 8 \"14.0\", 8 \"51.0\", 7 \"3.0\", 7 \"60.0\", 6 \"6.0\", 6 \"8.0\", 6 \"52.0\", 6 \"15.0\", 6 \"58.0\", 6 \"46.0\", 5 \"64.0\", 5 \"57.0\", 5 \"13.0\", 5 \"61.0\", 5 \"5.0\", 5 \"62.0\", 4 \"7.0\", 4 \"53.0\", 4 \"56.0\", 4 \"11.0\", 4 \"32.5\", 4 \"10.0\", 4 \"63.0\", 3 \"0.75\", 3 \"0.83\", 3 \"28.5\", 3 \"59.0\", 3 \"40.5\", 3 \"65.0\", 3 \"12.0\", 3 \"18.5\", 2 \"71.0\", 2 \"34.5\", 2 \"45.5\", 2 \"14.5\", 2 \"36.5\", 2 \"0.92\", 2 \"70.0\", 2 \"30.5\", 1 \"23.5\", 1 \"80.0\", 1 \"20.5\", 1 \"24.5\", 1 \"0.42\", 1 \"0.67\", 1 \"26.5\", 1 \"55.5\", 1 \"70.5\", 1 \"66.0\", 1 \"38.5\", 1 \"0.17\", 1 \"0.33\", 1 \"11.5\", 1 \"60.5\", 1 \"76.0\", 1 \"67.0\", 1 \"22.5\", 1 \"74.0\", e 263 dados faltantes.\n\n\n\n'''\nExistem muitas idades diferentes, e ao mesmo tempo muitos dados faltantes. O caminho    mais \nlógico a se seguir é tentar estimar as idades faltantes com base na correlação   estatística \ndessa coluna, com as outras disponíveis.\n'''\n\ndf_create.corr()\n\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nSex\nAge\nSibSp\nParch\nFare\nSurvived\n\n\n\n\nPassengerId\n1.000000\n-0.038354\n-0.013406\n0.028814\n-0.055224\n0.008942\n0.031428\n-0.005007\n\n\nPclass\n-0.038354\n1.000000\n-0.124617\n-0.408106\n0.060832\n0.018322\n-0.558629\n-0.338481\n\n\nSex\n-0.013406\n-0.124617\n1.000000\n-0.063645\n0.109609\n0.213125\n0.185523\n0.543351\n\n\nAge\n0.028814\n-0.408106\n-0.063645\n1.000000\n-0.243699\n-0.150917\n0.178740\n-0.077221\n\n\nSibSp\n-0.055224\n0.060832\n0.109609\n-0.243699\n1.000000\n0.373587\n0.160238\n-0.035322\n\n\nParch\n0.008942\n0.018322\n0.213125\n-0.150917\n0.373587\n1.000000\n0.221539\n0.081629\n\n\nFare\n0.031428\n-0.558629\n0.185523\n0.178740\n0.160238\n0.221539\n1.000000\n0.257307\n\n\nSurvived\n-0.005007\n-0.338481\n0.543351\n-0.077221\n-0.035322\n0.081629\n0.257307\n1.000000\n\n\n\n\n\n\n\n\n'''\nAs colunas que mais se correlacionam com \"Age\" são a \"Pclass\", \"SibSp\", \"Parch\" e    \"Fare\". \nPara preencher os dados faltantes, a melhor lógica a se seguir é tentar fazer médias, e para\naumentar a precisão dessas médias, elas serão calculadas com base em filtros das colunas que\nse correlacionam. As colunas com excessão da \"Fare\" possuem poucas constantes únicas,  então \npara que seja viável utilizar essa última como filtro, faz-se necessária a classificação dos \nvalores através de suas taxas.\n'''\n\nsns.displot(df_create['Fare'], kde=True)\n\n\n\n\n\n\n\n\n\n'''\nAo observar o gráfico, fica nítido que a grande maioria dos passageiros estão apenas  dentro \nde uma pequena faixa de valores, enquanto a minoria possui uma maior variação desses.   Para \nfacilitar a análise é possível dividir esses valores em três categorias de acordo com      o \nnúmero de passageiros:\n\nA baixa, que pode ser compreendida como a média das taxas que atingem aproximadamente     50 \npassageiros;\n\nA razoável, que pode ser compreendida como a média das taxas que atingem números  superiores \na 50 passageiros, e inferiores a aproximadamente 150 passageiros;\n\nA alta, que pode ser compreendida como a média das taxas que atingem números superiores    a \n150 passageiros.\n\nPara a partir de que número pegar de forma mais precisa, é importante observar a  progressão\ndos dados.\n'''\n\n# Crescimento da quantidade de passageiros conforme o valor aumenta\n\nc = 0\n\nfor i in range(101):\n    if i &lt; 10:\n        print(f'P(x &lt;= {i}) = {df_create[(df_create[\"Fare\"] &lt;= i)][\"Fare\"].value_counts().sum()}', end='\\t\\t')\n        \n    else:\n        print(f'P(x &lt;= {i}) = {df_create[(df_create[\"Fare\"] &lt;= i)][\"Fare\"].value_counts().sum()}', end='\\t')\n       \n    c += 1\n    \n    if c == 4:\n        c = 0\n        print()\n\n# Gráfico desse crescimento        \n\nsns.displot(df_create[\"Fare\"].value_counts(), kde=True)\n\nP(x &lt;= 0) = 17      P(x &lt;= 1) = 17      P(x &lt;= 2) = 17      P(x &lt;= 3) = 17      \nP(x &lt;= 4) = 18      P(x &lt;= 5) = 20      P(x &lt;= 6) = 20      P(x &lt;= 7) = 37      \nP(x &lt;= 8) = 360     P(x &lt;= 9) = 461     P(x &lt;= 10) = 491    P(x &lt;= 11) = 531    \nP(x &lt;= 12) = 543    P(x &lt;= 13) = 621    P(x &lt;= 14) = 642    P(x &lt;= 15) = 673    \nP(x &lt;= 16) = 716    P(x &lt;= 17) = 731    P(x &lt;= 18) = 739    P(x &lt;= 19) = 744    \nP(x &lt;= 20) = 752    P(x &lt;= 21) = 779    P(x &lt;= 22) = 787    P(x &lt;= 23) = 803    \nP(x &lt;= 24) = 812    P(x &lt;= 25) = 821    P(x &lt;= 26) = 882    P(x &lt;= 27) = 917    \nP(x &lt;= 28) = 940    P(x &lt;= 29) = 947    P(x &lt;= 30) = 965    P(x &lt;= 31) = 978    \nP(x &lt;= 32) = 997    P(x &lt;= 33) = 1004   P(x &lt;= 34) = 1005   P(x &lt;= 35) = 1013   \nP(x &lt;= 36) = 1017   P(x &lt;= 37) = 1021   P(x &lt;= 38) = 1024   P(x &lt;= 39) = 1032   \nP(x &lt;= 40) = 1044   P(x &lt;= 41) = 1045   P(x &lt;= 42) = 1049   P(x &lt;= 43) = 1052   \nP(x &lt;= 44) = 1052   P(x &lt;= 45) = 1052   P(x &lt;= 46) = 1053   P(x &lt;= 47) = 1061   \nP(x &lt;= 48) = 1063   P(x &lt;= 49) = 1063   P(x &lt;= 50) = 1068   P(x &lt;= 51) = 1070   \nP(x &lt;= 52) = 1084   P(x &lt;= 53) = 1088   P(x &lt;= 54) = 1094   P(x &lt;= 55) = 1096   \nP(x &lt;= 56) = 1102   P(x &lt;= 57) = 1114   P(x &lt;= 58) = 1118   P(x &lt;= 59) = 1118   \nP(x &lt;= 60) = 1124   P(x &lt;= 61) = 1124   P(x &lt;= 62) = 1130   P(x &lt;= 63) = 1130   \nP(x &lt;= 64) = 1132   P(x &lt;= 65) = 1137   P(x &lt;= 66) = 1137   P(x &lt;= 67) = 1139   \nP(x &lt;= 68) = 1139   P(x &lt;= 69) = 1139   P(x &lt;= 70) = 1152   P(x &lt;= 71) = 1154   \nP(x &lt;= 72) = 1156   P(x &lt;= 73) = 1156   P(x &lt;= 74) = 1163   P(x &lt;= 75) = 1163   \nP(x &lt;= 76) = 1167   P(x &lt;= 77) = 1172   P(x &lt;= 78) = 1177   P(x &lt;= 79) = 1182   \nP(x &lt;= 80) = 1193   P(x &lt;= 81) = 1193   P(x &lt;= 82) = 1196   P(x &lt;= 83) = 1200   \nP(x &lt;= 84) = 1208   P(x &lt;= 85) = 1208   P(x &lt;= 86) = 1208   P(x &lt;= 87) = 1211   \nP(x &lt;= 88) = 1211   P(x &lt;= 89) = 1211   P(x &lt;= 90) = 1218   P(x &lt;= 91) = 1218   \nP(x &lt;= 92) = 1220   P(x &lt;= 93) = 1220   P(x &lt;= 94) = 1224   P(x &lt;= 95) = 1224   \nP(x &lt;= 96) = 1224   P(x &lt;= 97) = 1224   P(x &lt;= 98) = 1224   P(x &lt;= 99) = 1224   \nP(x &lt;= 100) = 1224  \n\n\n\n\n\n\n\n\n\n\n'''\nAnalisando a forma como os dados se comportam, é possível aferir a partir de que momento  se \niniciam as três classes porpostas:\n\n- Baixa: Os passageiros com a taxa maior ou igual a 0 e menor ou igual a 6, e maior ou igual\na 32;\n                                                                                            \n- Razoável: Os passageiros com a taxa maior que 9 e menor que 32;                           \n                                                                                            \n- Alta: Os passageiros com a taxa maior que 6 e menor ou igual a 9.                         \n                                                                                            \nAgora basta calcular as médiasdas idades, de acordo com os filtros.\n'''\n\nPclass_u = list(df_create[\"Pclass\"].unique())\nSibSp_u = list(df_create[\"SibSp\"].unique())\nParch_u = list(df_create[\"Parch\"].unique())\n\n# Iterações para \"Pclass\"\nfor i in range(len(df_create[\"Pclass\"].unique())):\n    \n    # Iterações para \"SibSp\"\n    for j in range(len(df_create[\"SibSp\"].unique())):\n    \n        # Iterações para \"Parch\"    \n        for k in range(len(df_create[\"Parch\"].unique())):\n                    \n            # Iterações para \"Fare\"\n            \n            filtro_nan = ((df_create[\"Pclass\"] == Pclass_u[i]) & \n                          (df_create[\"SibSp\"] == SibSp_u[j]) & \n                          (df_create[\"Parch\"] == Parch_u[k]) &\n                          (df_create['Age'].isna()))\n            \n            filtro_mean = ((df_create[\"Pclass\"] == Pclass_u[i]) & \n                           (df_create[\"SibSp\"] == SibSp_u[j]) & \n                           (df_create[\"Parch\"] == Parch_u[k]))\n            \n            ##  Alto\n            \n            alto = (df_create[\"Fare\"] &gt; 6) & (df_create[\"Fare\"] &lt;= 9)\n\n            df_create.loc[filtro_nan + alto, 'Age'] = df_create.loc[filtro_nan + alto, 'Age'].fillna(df_create.loc[filtro_mean + alto, 'Age'].mean())\n            \n            ## Razoável\n            \n            razoavel = (df_create[\"Fare\"] &gt; 9) & (df_create[\"Fare\"] &lt; 32)\n\n            df_create.loc[filtro_nan + razoavel, 'Age'] = df_create.loc[filtro_nan + razoavel, 'Age'].fillna(df_create.loc[filtro_mean + razoavel, 'Age'].mean())\n\n            ## Baixo\n            \n            baixo_06 = (df_create[\"Fare\"] &gt;= 0) & (df_create[\"Fare\"] &lt;= 6)\n            \n            df_create.loc[filtro_nan + baixo_06, 'Age'] = df_create.loc[filtro_nan + baixo_06, 'Age'].fillna(df_create.loc[filtro_mean + baixo_06, 'Age'].mean())\n\n            baixo_32 = (df_create[\"Fare\"] &gt;= 32)\n            \n            df_create.loc[filtro_nan + baixo_32, 'Age'] = df_create.loc[filtro_nan + baixo_32, 'Age'].fillna(df_create.loc[filtro_mean + baixo_32, 'Age'].mean())\n           \ndf_final = pd.concat([df_final, df_create['Age']], axis=1)\n\n\n\nSibSp\n\nanalise(df_create['SibSp'])\n\nExistem 7 constantes únicas que são 891 \"0\", 319 \"1\", 42 \"2\", 22 \"4\", 20 \"3\", 9 \"8\", 6 \"5\" e 0 dados faltantes.\n\n\n\n'''\nAs variáveis são todas numéricas, e não há nenhum dado faltante. Logo, fica claro que      a\ncoluna em questão não necessita de nenhum tratamento estatístico.\n'''\n\ndf_final = pd.concat([df_final, df_create['SibSp']], axis=1)\n\n\n\nParch\n\nanalise(df_create['Parch'])\n\nExistem 8 constantes únicas que são 1002 \"0\", 170 \"1\", 113 \"2\", 8 \"3\", 6 \"4\", 6 \"5\", 2 \"6\", 2 \"9\" e 0 dados faltantes.\n\n\n\n'''\nAs variáveis são todas numéricas, e não há nenhum dado faltante. Logo, fica claro que      a\ncoluna em questão não necessita de nenhum tratamento estatístico.\n'''\n\ndf_final = pd.concat([df_final, df_create['Parch']], axis=1)\n\n\n\nFare\n\nanalise(df_create['Fare'])\n\nExistem 281 constantes únicas que são 60 \"8.05\", 59 \"13.0\", 55 \"7.75\", 50 \"26.0\", 49 \"7.8958\", 35 \"10.5\", 26 \"7.775\", 24 \"7.2292\", 23 \"7.925\", 22 \"26.55\", 21 \"7.225\", 21 \"7.8542\", 21 \"8.6625\", 18 \"7.25\", 17 \"0.0\", 14 \"21.0\", 12 \"9.5\", 12 \"16.1\", 11 \"69.55\", 11 \"14.5\", 11 \"27.7208\", 10 \"7.8792\", 10 \"7.7958\", 10 \"15.5\", 10 \"14.4542\", 9 \"7.05\", 9 \"24.15\", 9 \"15.2458\", 8 \"7.55\", 8 \"52.0\", 8 \"46.9\", 8 \"56.4958\", 7 \"31.275\", 7 \"13.5\", 7 \"39.0\", 7 \"23.0\", 7 \"30.0\", 7 \"39.6875\", 7 \"262.375\", 7 \"7.7333\", 7 \"31.3875\", 7 \"73.5\", 6 \"11.5\", 6 \"29.125\", 6 \"79.2\", 6 \"151.55\", 6 \"7.65\", 6 \"83.1583\", 6 \"27.9\", 6 \"30.5\", 6 \"263.0\", 6 \"26.25\", 6 \"53.1\", 5 \"227.525\", 5 \"134.5\", 5 \"27.75\", 5 \"65.0\", 5 \"21.075\", 5 \"12.35\", 5 \"90.0\", 5 \"34.375\", 5 \"211.5\", 5 \"25.4667\", 5 \"29.7\", 4 \"110.8833\", 4 \"120.0\", 4 \"41.5792\", 4 \"93.5\", 4 \"19.2583\", 4 \"135.6333\", 4 \"164.8667\", 4 \"18.0\", 4 \"59.4\", 4 \"51.8625\", 4 \"211.3375\", 4 \"35.5\", 4 \"512.3292\", 4 \"14.4583\", 4 \"7.125\", 4 \"20.575\", 4 \"23.45\", 4 \"221.7792\", 4 \"36.75\", 4 \"52.5542\", 4 \"15.85\", 4 \"12.475\", 4 \"55.4417\", 3 \"13.8583\", 3 \"6.4958\", 3 \"106.425\", 3 \"20.2125\", 3 \"7.8292\", 3 \"39.6\", 3 \"113.275\", 3 \"33.0\", 3 \"7.4958\", 3 \"26.2875\", 3 \"18.75\", 3 \"77.9583\", 3 \"153.4625\", 3 \"79.65\", 3 \"86.5\", 3 \"31.0\", 3 \"81.8583\", 3 \"20.525\", 3 \"76.7292\", 3 \"11.1333\", 3 \"108.9\", 3 \"20.25\", 3 \"37.0042\", 3 \"15.7417\", 3 \"146.5208\", 3 \"22.3583\", 3 \"9.35\", 3 \"78.85\", 3 \"22.025\", 3 \"22.525\", 3 \"21.6792\", 3 \"15.0458\", 3 \"6.4375\", 3 \"16.7\", 3 \"29.0\", 3 \"15.9\", 3 \"23.25\", 3 \"13.775\", 3 \"9.225\", 3 \"31.5\", 3 \"14.4\", 3 \"32.5\", 3 \"247.5208\", 2 \"11.2417\", 2 \"80.0\", 2 \"7.6292\", 2 \"10.4625\", 2 \"17.8\", 2 \"61.9792\", 2 \"49.5042\", 2 \"83.475\", 2 \"30.6958\", 2 \"61.3792\", 2 \"55.9\", 2 \"7.725\", 2 \"30.0708\", 2 \"42.4\", 2 \"7.0542\", 2 \"82.1708\", 2 \"18.7875\", 2 \"60.0\", 2 \"77.2875\", 2 \"78.2667\", 2 \"6.975\", 2 \"9.5875\", 2 \"9.825\", 2 \"66.6\", 2 \"12.2875\", 2 \"27.0\", 2 \"55.0\", 2 \"57.75\", 2 \"76.2917\", 2 \"91.0792\", 2 \"9.0\", 2 \"71.0\", 2 \"13.4167\", 2 \"19.5\", 2 \"6.75\", 2 \"7.0\", 2 \"13.8625\", 2 \"15.75\", 2 \"6.95\", 2 \"28.5\", 2 \"12.1833\", 2 \"17.4\", 2 \"57.9792\", 2 \"136.7792\", 2 \"7.8875\", 2 \"50.4958\", 2 \"133.65\", 2 \"75.25\", 2 \"71.2833\", 2 \"16.0\", 2 \"15.1\", 2 \"82.2667\", 2 \"8.1125\", 2 \"69.3\", 2 \"25.9292\", 2 \"19.9667\", 2 \"75.2417\", 2 \"14.1083\", 2 \"47.1\", 2 \"13.9\", 2 \"39.4\", 2 \"89.1042\", 2 \"12.875\", 2 \"50.0\", 2 \"8.5167\", 2 \"61.175\", 2 \"8.7125\", 2 \"15.55\", 2 \"51.4792\", 2 \"63.3583\", 2 \"57.0\", 2 \"24.0\", 2 \"56.9292\", 2 \"7.7375\", 1 \"12.65\", 1 \"7.875\", 1 \"6.8583\", 1 \"9.6875\", 1 \"38.5\", 1 \"13.7917\", 1 \"9.8458\", 1 \"5.0\", 1 \"8.3\", 1 \"6.45\", 1 \"8.6833\", 1 \"8.3625\", 1 \"9.4833\", 1 \"7.7417\", 1 \"10.1708\", 1 \"8.1375\", 1 \"9.8417\", 1 \"25.5875\", 1 \"8.4333\", 1 \"32.3208\", 1 \"7.7292\", 1 \"15.0\", 1 \"40.125\", 1 \"26.3875\", 1 \"49.5\", 1 \"34.0208\", 1 \"7.5208\", 1 \"7.0458\", 1 \"9.8375\", 1 \"12.0\", 1 \"25.7417\", 1 \"4.0125\", 1 \"7.5792\", 1 \"9.475\", 1 \"15.5792\", 1 \"8.0292\", 1 \"8.4583\", 1 \"28.5375\", 1 \"25.7\", 1 \"10.7083\", 1 \"7.7208\", 1 \"7.8208\", 1 \"7.7792\", 1 \"35.0\", 1 \"31.6792\", 1 \"7.2833\", 1 \"7.575\", 1 \"45.5\", 1 \"12.7375\", 1 \"9.325\", 1 \"8.9625\", 1 \"27.4458\", 1 \"42.5\", 1 \"7.8\", 1 \"15.0333\", 1 \"8.1583\", 1 \"7.7875\", 1 \"6.2375\", 1 \"14.0\", 1 \"8.85\", 1 \"3.1708\", 1 \"31.6833\", 1 \"12.275\", 1 \"8.4042\", 1 \"15.05\", 1 \"28.7125\", 1 \"33.5\", 1 \"25.925\", 1 \"7.85\", 1 \"7.3125\", 1 \"12.525\", 1 \"9.2167\", 1 \"26.2833\", 1 \"7.1417\", 1 \"8.6542\", 1 \"34.6542\", 1 \"10.5167\", e 1 dados faltantes.\n\n\n\n'''\nAs variáveis são todas numéricas, e existe apenas um dado faltante. A princípio uma    busca \npor colunas com dados semelhantes e maior correlação pode solucionar o problema           de \npreenchimento deste dado.\n'''\n\ndf_create[df_create['Fare'].isna()]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nModify\nSurvived\nPronoun\n\n\n\n\n152\n1044\n3\nStorey, Mr. Thomas\n0\n60.5\n0\n0\n3701\nNaN\nNaN\nS\ntest\nNaN\nMr\n\n\n\n\n\n\n\n\n# Preenchendo dado faltante com média de dados após filtro de colunas que mais            se\n# correlacionam\n\nfiltro = (df_create['Pclass'] == 3) & (df_create['Sex'] == 0) & (df_create['Parch'] == 0)\n\ndf_create.loc[filtro + df_create['Fare'].isna(), 'Fare'] = df_create.loc[filtro + df_create['Fare'].isna(), 'Fare'].fillna(df_create.loc[filtro, 'Fare'].mean())\n\n# Adicionando dados na planilha principal\n\ndf_final = pd.concat([df_final, df_create['Fare']], axis=1)\n\n\n\nEmbarked\n\nanalise(df_create['Embarked'])\n\nExistem 3 constantes únicas que são 914 \"S\", 270 \"C\", 123 \"Q\", e 2 dados faltantes.\n\n\n\n'''\nExistem poucas variáveis nesta coluna, e apenas dois dados faltantes. Por as variáveis serem\nletras não é possível fazer a média como aconteceu na coluna \"Fare\", porém é possível  obter \ninformações em linhas que possuem variáveis semelhantes. Uma boa lógica a se usar é a de  se \nbasear no portão de embarque escolhido com base na cabine do passageiro, além de     algumas \noutras colunas usadas como suporte.\n'''\n\ndf_create[df_create['Embarked'].isna()]\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nModify\nSurvived\nPronoun\n\n\n\n\n61\n62\n1\nIcard, Miss. Amelie\n1\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\ntrain\n1.0\nMiss\n\n\n829\n830\n1\nStone, Mrs. George Nelson (Martha Evelyn)\n1\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\ntrain\n1.0\nMrs\n\n\n\n\n\n\n\n\n'''\nÉ importante notar que além das outras semelhanças, ambos os passageiros que não possuem  os \ndados de embarque, possuem o mesmo valor de \"Ticket\" e de \"Fare\". É razoável constatar então\nque eles embarcaram pelo mesmo portão. Seguinte a filtragem com base na cabine, o     filtro\nocorrerá com base nas vigésimas cabines \"B\".\n'''\n\nfiltro = df_create[(df_create['Pclass'] == 1) & (df_create['Cabin'].str.startswith('B2')) & (df_create['Embarked'].isna())]\n\nfiltro\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nModify\nSurvived\nPronoun\n\n\n\n\n61\n62\n1\nIcard, Miss. Amelie\n1\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\ntrain\n1.0\nMiss\n\n\n829\n830\n1\nStone, Mrs. George Nelson (Martha Evelyn)\n1\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\ntrain\n1.0\nMrs\n\n\n\n\n\n\n\n\n'''\nÉ importante notar que além das outras semelhanças, ambos os passageiros que não possuem  os \nSurpreendentemente todos os passageiros dessas cabines embarcaram em \"S\", logo é    possível\nsupor que os passageiros sem dados também embarcaram por lá.\n'''\n\ndf_create['Embarked'] = df_create['Embarked'].fillna('S')\n\n\n# Transformando números em sistema binário\n\nb_Embarked = pd.get_dummies(df_create['Embarked'], prefix='Name')\n\ndf_final = pd.concat([df_final, b_Embarked], axis=1)\n\n\n\nDefinição Dados para Aprendizado\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nmodelo_arvore = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\nmodelo_logistico = LogisticRegression()\n\ndf_final['Modify'] = df_create['Modify']\n\ndf_train_x = df_final[df_final['Modify'] == 'train'][df_final.columns.tolist()[:-1]]\ndf_train_y = df_create[df_create['Modify'] == 'train'][['Survived']]\n\ndf_test_x = df_final[df_final['Modify'] == 'test'][df_final.columns.tolist()[:-1]]\ndf_test_y = df_create[df_create['Modify'] == 'test'][['Survived']]\n\n\n\nTeste de Acurácia para Treinamentos\n\n# Separando dados para teste de acurácia\n\nnp.random.seed(0)\n\nx_train, x_valid, y_train, y_valid = train_test_split(df_train_x, df_train_y, test_size=0.5)\n\n# Treinamento pela Random Forest\n\nmodelo_arvore.fit(x_train, y_train)\n\nprint('Random Forest')\n\nRandom Forest\n\n\n\n\n\n\n# Treinamento pelo Logistic Regression\n\nmodelo_logistico.fit(x_train, y_train)\n\nprint('Logistic Regression')\n\nLogistic Regression\n\n\n\n\n\n\n# Pedindo para máquina predizer Survived dos dados x_valid pela Random Forest\n\np_acuracia = modelo_arvore.predict(x_valid)\n\n# Média de igualdade entre o teste e os dados separados pela Random Forest\n\nnp.mean(y_valid['Survived'] == p_acuracia)\n\n0.7959641255605381\n\n\n\n# Pedindo para máquina predizer Survived dos dados x_valid pelo Logistic Regression\n\np_acuracia = modelo_logistico.predict(x_valid)\n\n# Média de igualdade entre o teste e os dados separados pelo Logistic Regression\n\nnp.mean(y_valid['Survived'] == p_acuracia)\n\n0.8161434977578476\n\n\n\n\nTreinamento de Máquina\n\n# Treinando pelo Logistic Regression\n\nmodelo_logistico.fit(df_train_x, df_train_y)\n\n# Pedindo para máquina predizer Survived dos dados x_valid\n\np_test = modelo_logistico.predict(df_test_x)\n\nnp.mean(gender_submission['Survived'] == p_test)\n\n\n\n\n0.930622009569378\n\n\n\n\nExportação de Resultados\n\n# Desenvolvendo série para csv\n\nresult = pd.Series(p_test, index=test['PassengerId'], name='Survived')\n\nresult = result.astype(int)\n\n\n# Exportando para csv\n\nresult.to_csv('titanic.csv', header=True)",
    "crumbs": [
      "6. Estudos de Caso e Aplicação Prática",
      "6.1 Titanic"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/separacao_de_dados_treinamento_validacao_teste.html",
    "href": "notebooks/preprocessing/separacao_de_dados_treinamento_validacao_teste.html",
    "title": "Separação de Dados: Treinamento, Validação e Teste",
    "section": "",
    "text": "Na ciência de dados e no aprendizado de máquina, a separação dos dados em conjuntos de treinamento, validação e teste é uma prática essencial para garantir a generalização e a precisão dos modelos. Vamos detalhar cada um desses conjuntos:\n\nConjunto de Treinamento (Training Set)\n O conjunto de treinamento é usado para ajustar os parâmetros do modelo. O modelo “aprende” a partir desses dados, identificando padrões e ajustando seus parâmetros para minimizar o erro.\n\nFinalidade: Treinar o modelo.\nProcesso: Os dados são alimentados no algoritmo de aprendizado de máquina, que ajusta seus parâmetros para se adequar a esses dados.  \n\nConjunto de Validação (Validation Set)\n O conjunto de validação é usado para ajustar os hiperparâmetros do modelo e para a seleção do modelo. Este conjunto ajuda a evitar o overfitting, que ocorre quando o modelo se ajusta muito bem aos dados de treinamento, mas não se generaliza bem para novos dados. \n\nFinalidade: Validar e ajustar hiperparâmetros do modelo.\nProcesso: Após o treinamento, o modelo é avaliado no conjunto de validação. Os hiperparâmetros são ajustados para melhorar o desempenho do modelo neste conjunto.  \n\nConjunto de Teste (Test Set)\n O conjunto de teste é usado para avaliar o desempenho final do modelo. Este conjunto nunca é usado durante o treinamento ou a validação do modelo e serve como uma medida objetiva da performance do modelo em dados não vistos. \n\nFinalidade: Avaliar a performance final do modelo.\nProcesso: Após o modelo ser treinado e validado, ele é testado no conjunto de teste para obter uma estimativa realista de seu desempenho em dados novos.  \n\n\nDurante a separação dos dados, é importante entender que o tipo de problema influencia na forma como eles serão particionados, gerando assim diferentes tipos de separação, onde algumas delas serão tratadas a seguir.\n\nDados com Divisão Aleatória\nA divisão dos dados é feita aleatoriamente para garantir a representatividade de toda a distribuição dos dados nos conjuntos de treino, validação e teste. É o tipo de divisão mais comúm, e é essencial para que o modelo possa aprender de forma eficaz e generalizar bem para novos dados.\n\nGerando dados aleatórios para exemplo\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Número de amostras\nn_samples = 1000\nn_features = 5  # Número de características\n\n# Configurações de gerador aleatório\nrng = np.random.default_rng(seed=0)\n\n# Gerar características aleatórias\nX = rng.normal(loc=0, scale=1, size=(n_samples, n_features))\n\n# Gerar rótulos binários aleatórios\ny = rng.integers(low=0, high=2, size=n_samples)\n\n# Criar DataFrame com dados\ndata = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(n_features)])\ndata['Label'] = y\n\n# Plotar os dados\nplt.figure(figsize=(10, 5))\n\n# Plotar dados de treino\nplt.scatter(data['Feature_0'], data['Feature_1'], label='Treino', alpha=0.6)\n\nplt.title('Dados Gerados', fontsize=12)\nplt.xlabel('Feature 0', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Feature 1', fontsize=10)\nplt.yticks(fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAplicando divisão de dados: Treinamento, Validação e Teste\n\nfrom sklearn.model_selection import train_test_split\n\n# Dividir os dados em treino, validação e teste\ntrain_size = 0.7\nvalidation_size = 0.15\ntest_size = 0.15\n\n# Dividir dados em treino e restante (validação + teste)\nX_train, X_temp, y_train, y_temp = train_test_split(data.drop('Label', axis=1), data['Label'], train_size=train_size, random_state=0)\n\n# Dividir o restante em validação e teste\nX_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)\n\n# Opcional: salvar em novos DataFrames para uso posterior\ntrain_data = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\nvalidation_data = pd.concat([X_validation, y_validation.reset_index(drop=True)], axis=1)\ntest_data = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n\n# Plotar os dados\nplt.figure(figsize=(10, 5))\n\n# Plotar dados de treino\nplt.scatter(train_data['Feature_0'], train_data['Feature_1'], label='Treino', alpha=0.6)\n\n# Plotar dados de validação\nplt.scatter(validation_data['Feature_0'], validation_data['Feature_1'], label='Validação', alpha=0.6)\n\n# Plotar dados de teste\nplt.scatter(test_data['Feature_0'], test_data['Feature_1'], label='Teste', alpha=0.6)\n\nplt.title('Divisão de Dados em Treino, Validação e Teste', fontsize=12)\nplt.xlabel('Feature 0', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Feature 1', fontsize=10)\nplt.yticks(fontsize=8)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDados Temporais (Time Series)\nPara dados que possuem uma dimensão temporal, como séries financeiras ou dados climáticos, é importante respeitar a ordem cronológica. Normalmente, você seleciona os dados mais antigos para treinamento e os mais recentes para teste, para simular como o modelo se comportaria em previsões futuras.\n\nDados de série temporal para exemplo: Treinamento, Validação e Teste\n\n# Definindo o período de tempo\ndate_range = pd.date_range(start='2022-01-01', \n                           end='2022-04-10', \n                           freq='D')\n\n# Número de dias da série temporal\ndias = len(date_range)\n\n# Gerando valores aleatórios para série temporal com distribuição normal\nvalues = np.random.randn(dias) \n\n# Criando o DataFrame\ntime_series = pd.DataFrame({'Date': date_range, 'Value': values})\n\n# Definindo a coluna 'Date' como índice\ntime_series.set_index('Date', inplace=True)\n\n# Plotando a série temporal\nplt.figure(figsize=(8, 3))\nplt.plot(time_series.index, time_series['Value'], label='Série Temporal')\nplt.title('Gráfico de Série Temporal', fontsize=12)\nplt.xlabel('Data', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Valor', fontsize=10)\nplt.yticks(fontsize=8)\nplt.legend(fontsize=8)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAplicando divisão de dados\n\n# Dividindo os dados para 100 dias\ntrain_data = time_series[:int(dias * 0.7)] # 70% para treinamento\nvalidation_data = time_series[int(dias * 0.7):int(dias * 0.85)] # 15% para validação\ntest_data = time_series[int(dias * 0.85):] # 15% para teste\n\n# Plotando os dados\nplt.figure(figsize=(8, 3))\n\n# Plotando dados de treino\nplt.plot(train_data.index, train_data['Value'], label='Treino')\n\n# Plotando dados de validação\nplt.plot(validation_data.index, validation_data['Value'], label='Validação')\n\n# Plotando dados de teste\nplt.plot(test_data.index, test_data['Value'], label='Teste')\n\nplt.title('Divisão da Série Temporal em Treino, Validação e Teste', fontsize=12)\nplt.xlabel('Data', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Valor', fontsize=10)\nplt.yticks(fontsize=8)\nplt.legend(fontsize=8)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nDados Geográficos ou Espaciais\nDados que contêm informação geográfica podem apresentar variações regionais. Uma divisão geográfica pode ser necessária, onde regiões diferentes são usadas para treino e teste para avaliar a capacidade do modelo de generalizar para novas localizações. É importante entender as características do que será predito para não cometer o erro de tornar aleatória a divisão dos dados, quando na verdade uma divisão mais especializada deveria ser feita.\n\nDados espaciais para exemplo: Treinamento, Validação e Teste\n\n# Número de amostras\nn_samples = 1000\n\n# Configurações de gerador aleatório\nrng = np.random.default_rng(seed=0)\n\n# Criar DataFrame com dados geográficos\ndata = pd.DataFrame({'Latitude': rng.uniform(low=-90, high=90, size=n_samples),\n                     'Longitude': rng.uniform(low=-180, high=180, size=n_samples),\n                     'Value': rng.normal(loc=0, scale=1, size=n_samples)})  # Valores associados às coordenadas\n\nlatitude_cutoff = 0  # Ponto de corte para dividir as regiões\n\n# Dados do hemisfério norte e sul\nnorth_data = data[data['Latitude'] &gt;= latitude_cutoff]\nsouth_data = data[data['Latitude'] &lt; latitude_cutoff]\n\n# Plotar os dados\nplt.figure(figsize=(8, 5))\n\n# Plotar dados do hemisfério norte\nplt.scatter(north_data['Longitude'], north_data['Latitude'], color='black', label='Hemisfério Norte')\n\n# Plotar dados do hemisfério sul\nplt.scatter(south_data['Longitude'], south_data['Latitude'], color='grey', label='Hemisfério Sul')\n\nplt.title('Dados Geográficos', fontsize=12)\nplt.xlabel('Longitude', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Latitude', fontsize=10)\nplt.yticks(fontsize=8)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAplicando divisão de dados\n\n# Dividir os dados com base na latitude em três regiões\n# 50% dos dados do sul para treino\nlatitude_cutoff1 = 0  # 25% dos dados norte para validação\nlatitude_cutoff2 = 45  # 25% dos dados do norte para teste\n\n# Dados de treino, validação e teste\ntrain_data = data[data['Latitude'] &lt; latitude_cutoff1]\nvalidation_data = data[(data['Latitude'] &gt;= latitude_cutoff1) & (data['Latitude'] &lt; latitude_cutoff2)]\ntest_data = data[data['Latitude'] &gt;= latitude_cutoff2]\n\n# Plotar os dados\nplt.figure(figsize=(8, 5))\n\n# Plotar dados de treino\nplt.scatter(train_data['Longitude'], train_data['Latitude'], color='red', label='Treino')\n\n# Plotar dados de validação\nplt.scatter(validation_data['Longitude'], validation_data['Latitude'], color='blue', label='Validação')\n\n# Plotar dados de teste\nplt.scatter(test_data['Longitude'], test_data['Latitude'], color='green', label='Teste')\n\nplt.title('Dados Geográficos', fontsize=12)\nplt.xlabel('Longitude', fontsize=10)\nplt.xticks(fontsize=8)\nplt.ylabel('Latitude', fontsize=10)\nplt.yticks(fontsize=8)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOutros tipos de divisão de dados\n\nDados de Usuários ou Pacientes:\nEm estudos onde cada ponto de dados está associado a um usuário ou paciente, é importante garantir que dados de um mesmo usuário ou paciente não estejam presentes nos conjuntos de treino e teste. Isso é conhecido como divisão por identificador único.\nDados de Imagens ou Texto com Contexto:\nEm tarefas onde o contexto é importante, como análise de sentimentos em textos ou reconhecimento de objetos em imagens, é necessário garantir que o contexto seja representado em ambos os conjuntos de dados. Por exemplo, não colocar todos os dados de um determinado assunto apenas no conjunto de teste.\nDados com Distribuição Desbalanceada:\nQuando há uma distribuição desigual entre as classes, é essencial manter a mesma proporção de classes em todos os conjuntos de dados para que o modelo seja treinado e testado de maneira consistente.\nDados com Relações Causais:\nEm cenários onde há uma relação causal entre variáveis, é importante considerar essa relação ao dividir os dados. Por exemplo, em dados de saúde, onde a intervenção médica pode afetar o resultado, é preciso garantir que a intervenção não vaze do conjunto de treino para o de teste.\n\nExistem outros inúmeros tipos de dados que necessitam de outras divisões para predição diferente do que se é mais comúm ou usual. Por isso é importante sempre entender de que tipo de dado se está ligando, para que a predição não seja comprometida.",
    "crumbs": [
      "1. Pré-processamento",
      "1.1 Separação de Dados para Treino, Verificação e Teste"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html",
    "href": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html",
    "title": "Numeralização de Variáveis Categóricas",
    "section": "",
    "text": "Variáveis categóricas, em geral, representam grupos ou categorias, não possuindo na maioria das vezes uma ordem natural entre si e não podendo ser ordenadas de forma significativa. Alguns exemplos dessas variáveis são as cores (vermelho, azul, verde), os tipos de gênero (másculino e feminino), os tipos de animais (cachorro, gato, pássaro) ou os níveis de educação (fundamental, médio, superior).\nPara aplicação dessas variáveis em métodos de aprendizado de máquinas como o agrupamento (cluster) ou a predição, se faz necessário a numeralização delas. Neste notebook serão apresentados dois tipos, a binarização e a percentualização de variáveis.",
    "crumbs": [
      "1. Pré-processamento",
      "1.5 Numeralização de Variáveis Categóricas"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#configuração-dos-dados",
    "href": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#configuração-dos-dados",
    "title": "Numeralização de Variáveis Categóricas",
    "section": "Configuração dos dados",
    "text": "Configuração dos dados\n\n# Importando a biblioteca random\nimport random\n\n# Importando a biblioteca pandas\nimport pandas as pd\n\n# Definindo lista de estados brasileiros\nestados_nordeste = ['alagoas', 'bahia', 'ceará', \n                    'maranhão', 'paraíba', 'pernambuco', \n                    'piauí', 'rio grande do norte', 'sergipe']\n\n# Definindo lista de gêneros\ngeneros = ['masculino', 'feminino']\n\n# Definindo lista de níveis de educação\neducacao = ['fundamental', 'médio', 'superior']\n\n# Definindo lista de dados vazia\ndados = []\n\n# Laço de 101 repetições\nfor i in range(101):\n\n    # Incluindo na lista de dados, biblioteca com gênero e educação aleatórios com base nas listas dos mesmos\n    dados.append({'Estado': random.choice(estados_nordeste), 'Gênero': random.choice(generos), 'Educação': random.choice(educacao)})\n\n# Criando planilha de dados pandas a partir de lista de dados\ndf = pd.DataFrame(dados)\n\n# Exibindo 5 primeiras linhas da planilha de dados\ndf.head(5)\n\n\n\n\n\n\n\n\nEstado\nGênero\nEducação\n\n\n\n\n0\npernambuco\nmasculino\nsuperior\n\n\n1\nparaíba\nmasculino\nfundamental\n\n\n2\npernambuco\nmasculino\nmédio\n\n\n3\nparaíba\nmasculino\nsuperior\n\n\n4\npiauí\nfeminino\nfundamental",
    "crumbs": [
      "1. Pré-processamento",
      "1.5 Numeralização de Variáveis Categóricas"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#binarização-de-variáveis",
    "href": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#binarização-de-variáveis",
    "title": "Numeralização de Variáveis Categóricas",
    "section": "Binarização de variáveis",
    "text": "Binarização de variáveis\nA binarização das variáveis, ocorre quando elas são transformadas em 0 e 1. Quando uma coluna possui apenas duas variáveis, essa binarização ocorre de maneira que o 0 ocupa a posição de uma das variáveis, e o 1 da outra. Já quando a coluna possui mais que duas variáveis, se faz necessária a criação de uma coluna para cada variáveil, onde o 0 assumirá a posição de ocorrência da variável, e o 1 da não ocorrência, ou vice-versa.\n\nColuna com duas variáveis\n\n# Clonando DataFrame para teste\ndf_test = df.copy()\n\n# Binarizando coluna Gênero\ndf_test['Gênero'] = df_test['Gênero'].map({'masculino': 0, 'feminino': 1})\n\n# Exibindo 5 primeiras linhas da planilha de dados\ndf_test.head(5)\n\n\n\n\n\n\n\n\nEstado\nGênero\nEducação\n\n\n\n\n0\npernambuco\n0\nsuperior\n\n\n1\nparaíba\n0\nfundamental\n\n\n2\npernambuco\n0\nmédio\n\n\n3\nparaíba\n0\nsuperior\n\n\n4\npiauí\n1\nfundamental\n\n\n\n\n\n\n\n\n\nColuna com mais de duas variáveis\n\n# Clonando DataFrame para teste\ndf_test = df.copy()\n\n# Binarizando coluna Gênero\ndf_test = pd.get_dummies(df_test, columns=['Educação'], dtype=int)\n\n# Exibindo 5 primeiras linhas da planilha de dados\ndf_test.head(5)\n\n\n\n\n\n\n\n\nEstado\nGênero\nEducação_fundamental\nEducação_médio\nEducação_superior\n\n\n\n\n0\npernambuco\nmasculino\n0\n0\n1\n\n\n1\nparaíba\nmasculino\n1\n0\n0\n\n\n2\npernambuco\nmasculino\n0\n1\n0\n\n\n3\nparaíba\nmasculino\n0\n0\n1\n\n\n4\npiauí\nfeminino\n1\n0\n0",
    "crumbs": [
      "1. Pré-processamento",
      "1.5 Numeralização de Variáveis Categóricas"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#percentualização-de-variáveis",
    "href": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#percentualização-de-variáveis",
    "title": "Numeralização de Variáveis Categóricas",
    "section": "Percentualização de variáveis",
    "text": "Percentualização de variáveis\nA percentualização das variáveis, ocorre quando elas são transformadas no percentual das vezes em que aparecem na coluna. O método possui um bom uso para colunas que possuem multiplas variáveis, visto que apenas uma coluna é criada, e só é recomendado seu uso quando a coluna não possui variáveis com o mesmo número de repetições entre si.\n\n# Clonando DataFrame para demonstração\ndf_test = df.copy()\n\n# Calculando a frequência relativa das variáveis\nfrequencia_relativa = df_test['Estado'].value_counts(normalize=True).to_dict()\n\n# Substituindo os valores categóricos pelos valores numéricos correspondentes\ndf_test['Estado'] = df_test['Estado'].map(frequencia_relativa)\n\n# Exibindo 5 primeiras linhas da planilha de dados\ndf_test.head(5)\n\n\n\n\n\n\n\n\nEstado\nGênero\nEducação\n\n\n\n\n0\n0.099010\nmasculino\nsuperior\n\n\n1\n0.138614\nmasculino\nfundamental\n\n\n2\n0.099010\nmasculino\nmédio\n\n\n3\n0.138614\nmasculino\nsuperior\n\n\n4\n0.108911\nfeminino\nfundamental",
    "crumbs": [
      "1. Pré-processamento",
      "1.5 Numeralização de Variáveis Categóricas"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#considerações-finais",
    "href": "notebooks/preprocessing/numeralizacao_de_variaveis_categoricas.html#considerações-finais",
    "title": "Numeralização de Variáveis Categóricas",
    "section": "Considerações finais",
    "text": "Considerações finais\nO método a ser utilizado dependerá da disposição dos dados, e o indicativo de qual utilizar sempre partirá da interpretação do analista, e da precisão que os resultados do uso apresentarão.",
    "crumbs": [
      "1. Pré-processamento",
      "1.5 Numeralização de Variáveis Categóricas"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/normalizacao_de_dados.html",
    "href": "notebooks/preprocessing/normalizacao_de_dados.html",
    "title": "Normalização de dados quantitativos",
    "section": "",
    "text": "A normalização de dados é um processo comum em análise de dados que visa colocar variáveis em uma escala comum. O objetivo principal é evitar que variáveis com diferentes unidades ou escalas dominem os resultados, especialmente em algoritmos de aprendizado de máquina que são sensíveis à escala das variáveis.\nExistem várias técnicas de normalização de dados, sendo as mais comuns a normalização min-max e a padronização (z-score).",
    "crumbs": [
      "1. Pré-processamento",
      "1.6 Normalização de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/normalizacao_de_dados.html#configurando-dados",
    "href": "notebooks/preprocessing/normalizacao_de_dados.html#configurando-dados",
    "title": "Normalização de dados quantitativos",
    "section": "Configurando dados",
    "text": "Configurando dados\n\n# Importando biblioteca pandas\nimport pandas as pd\n\n# Importando dados\ndf = pd.read_csv('datas/diabetes.csv')\n\nfor coluna in df.columns:\n    \n    # Calculando mediana\n    mediana_coluna = df[coluna].median()\n    \n    # Substitui os valores nulos pela mediana da coluna usando loc\n    df.loc[df[coluna].isnull(), coluna] = mediana_coluna\n\n# Visualizando dados\ndf.head(5)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n0\n6\n148\n72.0\n35.0\n0.0\n33.6\n627.0\n50.0\n\n\n1\n1\n85\n66.0\n29.0\n0.0\n26.6\n351.0\n31.0\n\n\n2\n8\n183\n64.0\n23.0\n0.0\n23.3\n672.0\n32.0\n\n\n3\n1\n89\n66.0\n23.0\n94.0\n28.1\n167.0\n21.0\n\n\n4\n0\n137\n40.0\n35.0\n168.0\n43.1\n2288.0\n29.0",
    "crumbs": [
      "1. Pré-processamento",
      "1.6 Normalização de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/normalizacao_de_dados.html#min-max",
    "href": "notebooks/preprocessing/normalizacao_de_dados.html#min-max",
    "title": "Normalização de dados quantitativos",
    "section": "Min-Max",
    "text": "Min-Max\nA fórmula do Min-Max pode ser descrita como Xnorm = (Xmax − Xmin) / (X − Xmin). A escala os dados para um intervalo específico (geralmente [0, 1]), mantendo a distribuição relativa dos dados. É importante saber que o Min-Max é sensível aos outliers, pois a presença de valores extremos pode distorcer a escala dos dados. Sua utilização é especialmente útil quando os dados têm distribuição uniforme ou quando a distribuição não é gaussiana.\n\n# Importando a classe KMeans do módulo cluster da biblioteca scikit-learn\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Normalização por Min-Max\nscaler_minmax = MinMaxScaler()\n\n# Aplicando normalização no DataFrame\ndf_minmax = pd.DataFrame(scaler_minmax.fit_transform(df), columns=df.columns)\n\n# Visualizando dados\ndf_minmax.head(5)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n0\n0.352941\n0.743719\n0.590164\n0.353535\n0.000000\n0.500745\n0.269183\n0.483333\n\n\n1\n0.058824\n0.427136\n0.540984\n0.292929\n0.000000\n0.396423\n0.150672\n0.166667\n\n\n2\n0.470588\n0.919598\n0.524590\n0.232323\n0.000000\n0.347243\n0.288505\n0.183333\n\n\n3\n0.058824\n0.447236\n0.540984\n0.232323\n0.111111\n0.418778\n0.071665\n0.000000\n\n\n4\n0.000000\n0.688442\n0.327869\n0.353535\n0.198582\n0.642325\n0.982395\n0.133333",
    "crumbs": [
      "1. Pré-processamento",
      "1.6 Normalização de Dados Quantitativos"
    ]
  },
  {
    "objectID": "notebooks/preprocessing/normalizacao_de_dados.html#z-score",
    "href": "notebooks/preprocessing/normalizacao_de_dados.html#z-score",
    "title": "Normalização de dados quantitativos",
    "section": "Z-score",
    "text": "Z-score\nA fórmula do Z-Score pode ser descrita como Z = (X - μ) / σ, onde μ é a média dos dados e σ é o desvio padrão. O Z-Score normaliza os dados para que tenham média zero e desvio padrão um, o que significa que os dados estão centrados em torno de zero e têm uma dispersão padrão. O Z-Score é útil para identificar valores extremos, pois valores muito altos ou baixos terão um Z-Score significativamente diferente de zero. Ele é adequado para dados que seguem uma distribuição normal ou aproximadamente normal.\n\n# Importando a classe KMeans do módulo cluster da biblioteca scikit-learn\nfrom sklearn.preprocessing import StandardScaler\n\n# Normalização por Min-Max\nscaler_zscore = StandardScaler()\n\n# Aplicando normalização no DataFrame\ndf_zscore = pd.DataFrame(scaler_zscore.fit_transform(df), columns=df.columns)\n\n# Visualizando dados\ndf_zscore.head(5)\n\n\n\n\n\n\n\n\npreg\nplas\npres\nskin\ninsu\nmass\npedi\nage\n\n\n\n\n0\n0.639947\n0.848324\n0.142335\n0.905450\n-0.693391\n0.204013\n0.584149\n1.426218\n\n\n1\n-0.844885\n-1.123396\n-0.173144\n0.528274\n-0.693391\n-0.684422\n-0.226986\n-0.191582\n\n\n2\n1.233880\n1.943724\n-0.278304\n0.151099\n-0.693391\n-1.103255\n0.716400\n-0.106434\n\n\n3\n-0.844885\n-0.998208\n-0.173144\n0.151099\n0.122965\n-0.494043\n-0.767743\n-1.043055\n\n\n4\n-1.141852\n0.504055\n-1.540222\n0.905450\n0.765628\n1.409746\n5.465654\n-0.361876",
    "crumbs": [
      "1. Pré-processamento",
      "1.6 Normalização de Dados Quantitativos"
    ]
  }
]